<!DOCTYPE html>
<html lang="en-US">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-06-24T11:24:14-06:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math"><article class="hiddenproof"><p>Any proof of linear independence should start by defining our set of vectors, and assuming that a linear combination of these vectors is equal to the zero vector, with the goal of showing that the scalars have to be zero.</p>
<p>Set up the equation (say, <span class="process-math">\(c_1\vv_1+\cdots c_n\vv_n=\zer\)</span>), with the assumption that your set of vectors is orthogonal. What happens if you take the dot product of both sides with one of these vectors?</p></article></body>
</html>
