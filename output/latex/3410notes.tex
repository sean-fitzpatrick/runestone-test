%********************************************%
%*       Generated from PreTeXt source      *%
%*       on 2022-03-22T14:24:15-06:00       *%
%*   A recent stable commit (2020-08-09):   *%
%* 98f21740783f166a773df4dc83cab5293ab63a4a *%
%*                                          *%
%*         https://pretextbook.org          *%
%*                                          *%
%********************************************%
%% We elect to always write snapshot output into <job>.dep file
\RequirePackage{snapshot}
\documentclass[oneside,10pt,]{book}
%% Custom Preamble Entries, early (use latex.preamble.early)
 \usepackage{xcoffins}
 \NewCoffin\Framex
 \NewCoffin\Theox   
%% Default LaTeX packages
%%   1.  always employed (or nearly so) for some purpose, or
%%   2.  a stylewriter may assume their presence
\usepackage{geometry}
%% Some aspects of the preamble are conditional,
%% the LaTeX engine is one such determinant
\usepackage{ifthen}
%% etoolbox has a variety of modern conveniences
\usepackage{etoolbox}
\usepackage{ifxetex,ifluatex}
%% Raster graphics inclusion
\usepackage{graphicx}
%% Color support, xcolor package
%% Always loaded, for: add/delete text, author tools
%% Here, since tcolorbox loads tikz, and tikz loads xcolor
\PassOptionsToPackage{usenames,dvipsnames,svgnames,table}{xcolor}
\usepackage{xcolor}
%% begin: defined colors, via xcolor package, for styling
%% end: defined colors, via xcolor package, for styling
%% Colored boxes, and much more, though mostly styling
%% skins library provides "enhanced" skin, employing tikzpicture
%% boxes may be configured as "breakable" or "unbreakable"
%% "raster" controls grids of boxes, aka side-by-side
\usepackage{tcolorbox}
\tcbuselibrary{skins}
\tcbuselibrary{breakable}
\tcbuselibrary{raster}
%% We load some "stock" tcolorbox styles that we use a lot
%% Placement here is provisional, there will be some color work also
%% First, black on white, no border, transparent, but no assumption about titles
\tcbset{ bwminimalstyle/.style={size=minimal, boxrule=-0.3pt, frame empty,
colback=white, colbacktitle=white, coltitle=black, opacityfill=0.0} }
%% Second, bold title, run-in to text/paragraph/heading
%% Space afterwards will be controlled by environment,
%% independent of constructions of the tcb title
%% Places \blocktitlefont onto many block titles
\tcbset{ runintitlestyle/.style={fonttitle=\blocktitlefont\upshape\bfseries, attach title to upper} }
%% Spacing prior to each exercise, anywhere
\tcbset{ exercisespacingstyle/.style={before skip={1.5ex plus 0.5ex}} }
%% Spacing prior to each block
\tcbset{ blockspacingstyle/.style={before skip={2.0ex plus 0.5ex}} }
%% xparse allows the construction of more robust commands,
%% this is a necessity for isolating styling and behavior
%% The tcolorbox library of the same name loads the base library
\tcbuselibrary{xparse}
%% The tcolorbox library loads TikZ, its calc package is generally useful,
%% and is necessary for some smaller documents that use partial tcolor boxes
%% See:  https://github.com/PreTeXtBook/pretext/issues/1624
\usetikzlibrary{calc}
%% Hyperref should be here, but likes to be loaded late
%%
%% Inline math delimiters, \(, \), need to be robust
%% 2016-01-31:  latexrelease.sty  supersedes  fixltx2e.sty
%% If  latexrelease.sty  exists, bugfix is in kernel
%% If not, bugfix is in  fixltx2e.sty
%% See:  https://tug.org/TUGboat/tb36-3/tb114ltnews22.pdf
%% and read "Fewer fragile commands" in distribution's  latexchanges.pdf
\IfFileExists{latexrelease.sty}{}{\usepackage{fixltx2e}}
%% Footnote counters and part/chapter counters are manipulated
%% April 2018:  chngcntr  commands now integrated into the kernel,
%% but circa 2018/2019 the package would still try to redefine them,
%% so we need to do the work of loading conditionally for old kernels.
%% From version 1.1a,  chngcntr  should detect defintions made by LaTeX kernel.
\ifdefined\counterwithin
\else
    \usepackage{chngcntr}
\fi
%% Text height identically 9 inches, text width varies on point size
%% See Bringhurst 2.1.1 on measure for recommendations
%% 75 characters per line (count spaces, punctuation) is target
%% which is the upper limit of Bringhurst's recommendations
\geometry{letterpaper,total={340pt,9.0in}}
%% Custom Page Layout Adjustments (use latex.geometry)
\geometry{inner=1in,textheight=9in,textwidth=340pt,marginparwidth=140pt,marginparsep=20pt,bottom=1in,footskip=29pt}
%% This LaTeX file may be compiled with pdflatex, xelatex, or lualatex executables
%% LuaTeX is not explicitly supported, but we do accept additions from knowledgeable users
%% The conditional below provides  pdflatex  specific configuration last
%% begin: engine-specific capabilities
\ifthenelse{\boolean{xetex} \or \boolean{luatex}}{%
%% begin: xelatex and lualatex-specific default configuration
\ifxetex\usepackage{xltxtra}\fi
%% realscripts is the only part of xltxtra relevant to lualatex 
\ifluatex\usepackage{realscripts}\fi
%% end:   xelatex and lualatex-specific default configuration
}{
%% begin: pdflatex-specific default configuration
%% We assume a PreTeXt XML source file may have Unicode characters
%% and so we ask LaTeX to parse a UTF-8 encoded file
%% This may work well for accented characters in Western language,
%% but not with Greek, Asian languages, etc.
%% When this is not good enough, switch to the  xelatex  engine
%% where Unicode is better supported (encouraged, even)
\usepackage[utf8]{inputenc}
%% end: pdflatex-specific default configuration
}
%% end:   engine-specific capabilities
%%
%% Fonts.  Conditional on LaTex engine employed.
%% Default Text Font: The Latin Modern fonts are
%% "enhanced versions of the [original TeX] Computer Modern fonts."
%% We use them as the default text font for PreTeXt output.
%% Default Monospace font: Inconsolata (aka zi4)
%% Sponsored by TUG: http://levien.com/type/myfonts/inconsolata.html
%% Loaded for documents with intentional objects requiring monospace
%% See package documentation for excellent instructions
%% fontspec will work universally if we use filename to locate OTF files
%% Loads the "upquote" package as needed, so we don't have to
%% Upright quotes might come from the  textcomp  package, which we also use
%% We employ the shapely \ell to match Google Font version
%% pdflatex: "varl" package option produces shapely \ell
%% pdflatex: "var0" package option produces plain zero (not used)
%% pdflatex: "varqu" package option produces best upright quotes
%% xelatex,lualatex: add OTF StylisticSet 1 for shapely \ell
%% xelatex,lualatex: add OTF StylisticSet 2 for plain zero (not used)
%% xelatex,lualatex: add OTF StylisticSet 3 for upright quotes
%%
%% Automatic Font Control
%% Portions of a document, are, or may, be affected by defined commands
%% These are perhaps more flexible when using  xelatex  rather than  pdflatex
%% The following definitions are meant to be re-defined in a style, using \renewcommand
%% They are scoped when employed (in a TeX group), and so should not be defined with an argument
\newcommand{\divisionfont}{\relax}
\newcommand{\blocktitlefont}{\relax}
\newcommand{\contentsfont}{\relax}
\newcommand{\pagefont}{\relax}
\newcommand{\tabularfont}{\relax}
\newcommand{\xreffont}{\relax}
\newcommand{\titlepagefont}{\relax}
%%
\ifthenelse{\boolean{xetex} \or \boolean{luatex}}{%
%% begin: font setup and configuration for use with xelatex
%% Generally, xelatex is necessary for non-Western fonts
%% fontspec package provides extensive control of system fonts,
%% meaning *.otf (OpenType), and apparently *.ttf (TrueType)
%% that live *outside* your TeX/MF tree, and are controlled by your *system*
%% (it is possible that a TeX distribution will place fonts in a system location)
%%
%% The fontspec package is the best vehicle for using different fonts in  xelatex
%% So we load it always, no matter what a publisher or style might want
%%
\usepackage{fontspec}
%%
%% begin: xelatex main font ("font-xelatex-main" template)
%% XeLaTeX font configuration from PreTeXt Guide style
%% We rely on a font installed at the system level,
%% so that we can exercise specific font features
%%
\IfFontExistsTF{Latte-Regular}{}{\GenericError{}{The font "Latte-Regular" requested by PreTeXt output is not available.  Either a file cannot be located in default locations via a filename, or a font is not known by its name as part of your system.}{Consult the PreTeXt Guide for help with LaTeX fonts.}{}}
\setmainfont{Latte-Regular}
%% end:   xelatex main font ("font-xelatex-main" template)
%% begin: xelatex mono font ("font-xelatex-mono" template)
%% (conditional on non-trivial uses being present in source)
\IfFontExistsTF{Inconsolatazi4-Regular.otf}{}{\GenericError{}{The font "Inconsolatazi4-Regular.otf" requested by PreTeXt output is not available.  Either a file cannot be located in default locations via a filename, or a font is not known by its name as part of your system.}{Consult the PreTeXt Guide for help with LaTeX fonts.}{}}
\IfFontExistsTF{Inconsolatazi4-Bold.otf}{}{\GenericError{}{The font "Inconsolatazi4-Bold.otf" requested by PreTeXt output is not available.  Either a file cannot be located in default locations via a filename, or a font is not known by its name as part of your system.}{Consult the PreTeXt Guide for help with LaTeX fonts.}{}}
\usepackage{zi4}
\setmonofont[BoldFont=Inconsolatazi4-Bold.otf,StylisticSet={1,3}]{Inconsolatazi4-Regular.otf}
%% end:   xelatex mono font ("font-xelatex-mono" template)
%% begin: xelatex font adjustments ("font-xelatex-style" template)
%% end:   xelatex font adjustments ("font-xelatex-style" template)
%%
%% Extensive support for other languages
\usepackage{polyglossia}
%% Set main/default language based on pretext/@xml:lang value
%% document language code is "en-US", US English
%% usmax variant has extra hypenation
\setmainlanguage[variant=usmax]{english}
%% Enable secondary languages based on discovery of @xml:lang values
%% Enable fonts/scripts based on discovery of @xml:lang values
%% Western languages should be ably covered by Latin Modern Roman
%% end:   font setup and configuration for use with xelatex
}{%
%% begin: font setup and configuration for use with pdflatex
%% begin: pdflatex main font ("font-pdflatex-main" template)
\usepackage{lmodern}
\usepackage[T1]{fontenc}
%% end:   pdflatex main font ("font-pdflatex-main" template)
%% begin: pdflatex mono font ("font-pdflatex-mono" template)
%% (conditional on non-trivial uses being present in source)
\usepackage[varqu,varl]{inconsolata}
%% end:   pdflatex mono font ("font-pdflatex-mono" template)
%% begin: pdflatex font adjustments ("font-pdflatex-style" template)
%% end:   pdflatex font adjustments ("font-pdflatex-style" template)
%% end:   font setup and configuration for use with pdflatex
}
%% Micromanage spacing, etc.  The named "microtype-options"
%% template may be employed to fine-tune package behavior
\usepackage{microtype}
%% Symbols, align environment, commutative diagrams, bracket-matrix
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
%% allow page breaks within display mathematics anywhere
%% level 4 is maximally permissive
%% this is exactly the opposite of AMSmath package philosophy
%% there are per-display, and per-equation options to control this
%% split, aligned, gathered, and alignedat are not affected
\allowdisplaybreaks[4]
%% allow more columns to a matrix
%% can make this even bigger by overriding with  latex.preamble.late  processing option
\setcounter{MaxMatrixCols}{30}
%%
%%
%% Division Titles, and Page Headers/Footers
%% titlesec package, loading "titleps" package cooperatively
%% See code comments about the necessity and purpose of "explicit" option.
%% The "newparttoc" option causes a consistent entry for parts in the ToC 
%% file, but it is only effective if there is a \titleformat for \part.
%% "pagestyles" loads the  titleps  package cooperatively.
\usepackage[explicit, newparttoc, pagestyles]{titlesec}
%% The companion titletoc package for the ToC.
\usepackage{titletoc}
%% Fixes a bug with transition from chapters to appendices in a "book"
%% See generating XSL code for more details about necessity
\newtitlemark{\chaptertitlename}
%% begin: customizations of page styles via the modal "titleps-style" template
%% Designed to use commands from the LaTeX "titleps" package
%% Plain pages should have the same font for page numbers
\renewpagestyle{plain}{%
\setfoot{}{\pagefont\thepage}{}%
}%
%% Single pages as in default LaTeX
\renewpagestyle{headings}{%
\sethead{\pagefont\slshape\MakeUppercase{\ifthechapter{\chaptertitlename\space\thechapter.\space}{}\chaptertitle}}{}{\pagefont\thepage}%
}%
\pagestyle{headings}
%% end: customizations of page styles via the modal "titleps-style" template
%%
%% Create globally-available macros to be provided for style writers
%% These are redefined for each occurence of each division
\newcommand{\divisionnameptx}{\relax}%
\newcommand{\titleptx}{\relax}%
\newcommand{\subtitleptx}{\relax}%
\newcommand{\shortitleptx}{\relax}%
\newcommand{\authorsptx}{\relax}%
\newcommand{\epigraphptx}{\relax}%
%% Create environments for possible occurences of each division
%% Environment for a PTX "preface" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{preface}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Preface}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter*{#1}%
\addcontentsline{toc}{chapter}{#3}
\label{#6}%
}{}%
%% Environment for a PTX "chapter" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{chapterptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Chapter}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "section" at the level of a LaTeX "section"
\NewDocumentEnvironment{sectionptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Section}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\section[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "worksheet" at the level of a LaTeX "section"
\NewDocumentEnvironment{worksheet-section}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Worksheet}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\section[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "worksheet" at the level of a LaTeX "section"
\NewDocumentEnvironment{worksheet-section-numberless}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Worksheet}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\section*{#1}%
\addcontentsline{toc}{section}{#3}
\label{#6}%
}{}%
%% Environment for a PTX "subsection" at the level of a LaTeX "subsection"
\NewDocumentEnvironment{subsectionptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Subsection}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\subsection[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "subsubsection" at the level of a LaTeX "subsubsection"
\NewDocumentEnvironment{subsubsectionptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Subsubsection}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\subsubsection[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "appendix" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{appendixptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Appendix}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "solutions" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{solutions-chapter}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Appendix}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "solutions" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{solutions-chapter-numberless}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Appendix}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter*{#1}%
\addcontentsline{toc}{chapter}{#3}
\label{#6}%
}{}%
%%
%% Styles for six traditional LaTeX divisions
\titleformat{\part}[display]
{\divisionfont\Huge\bfseries\centering}{\divisionnameptx\space\thepart}{30pt}{\Huge#1}
[{\Large\centering\authorsptx}]
\titleformat{\chapter}[display]
{\divisionfont\huge\bfseries}{\divisionnameptx\space\thechapter}{20pt}{\Huge#1}
[{\Large\authorsptx}]
\titleformat{name=\chapter,numberless}[display]
{\divisionfont\huge\bfseries}{}{0pt}{#1}
[{\Large\authorsptx}]
\titlespacing*{\chapter}{0pt}{50pt}{40pt}
\titleformat{\section}[hang]
{\divisionfont\Large\bfseries}{\thesection}{1ex}{#1}
[{\large\authorsptx}]
\titleformat{name=\section,numberless}[block]
{\divisionfont\Large\bfseries}{}{0pt}{#1}
[{\large\authorsptx}]
\titlespacing*{\section}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\titleformat{\subsection}[hang]
{\divisionfont\large\bfseries}{\thesubsection}{1ex}{#1}
[{\normalsize\authorsptx}]
\titleformat{name=\subsection,numberless}[block]
{\divisionfont\large\bfseries}{}{0pt}{#1}
[{\normalsize\authorsptx}]
\titlespacing*{\subsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titleformat{\subsubsection}[hang]
{\divisionfont\normalsize\bfseries}{\thesubsubsection}{1em}{#1}
[{\small\authorsptx}]
\titleformat{name=\subsubsection,numberless}[block]
{\divisionfont\normalsize\bfseries}{}{0pt}{#1}
[{\normalsize\authorsptx}]
\titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titleformat{\paragraph}[hang]
{\divisionfont\normalsize\bfseries}{\theparagraph}{1em}{#1}
[{\small\authorsptx}]
\titleformat{name=\paragraph,numberless}[block]
{\divisionfont\normalsize\bfseries}{}{0pt}{#1}
[{\normalsize\authorsptx}]
\titlespacing*{\paragraph}{0pt}{3.25ex plus 1ex minus .2ex}{1.5em}
%%
%% Styles for five traditional LaTeX divisions
\titlecontents{part}%
[0pt]{\contentsmargin{0em}\addvspace{1pc}\contentsfont\bfseries}%
{\Large\thecontentslabel\enspace}{\Large}%
{}%
[\addvspace{.5pc}]%
\titlecontents{chapter}%
[0pt]{\contentsmargin{0em}\addvspace{1pc}\contentsfont\bfseries}%
{\large\thecontentslabel\enspace}{\large}%
{\hfill\bfseries\thecontentspage}%
[\addvspace{.5pc}]%
\dottedcontents{section}[3.8em]{\contentsfont}{2.3em}{1pc}%
\dottedcontents{subsection}[6.1em]{\contentsfont}{3.2em}{1pc}%
\dottedcontents{subsubsection}[9.3em]{\contentsfont}{4.3em}{1pc}%
%%
%% Begin: Semantic Macros
%% To preserve meaning in a LaTeX file
%%
%% \mono macro for content of "c", "cd", "tag", etc elements
%% Also used automatically in other constructions
%% Simply an alias for \texttt
%% Always defined, even if there is no need, or if a specific tt font is not loaded
\newcommand{\mono}[1]{\texttt{#1}}
%%
%% Following semantic macros are only defined here if their
%% use is required only in this specific document
%%
%% Used to markup initialisms, text or titles
\newcommand{\initialism}[1]{\textsc{\MakeLowercase{#1}}}
\DeclareRobustCommand{\initialismintitle}[1]{\texorpdfstring{#1}{#1}}
%% Used for warnings, typically bold and italic
\newcommand{\alert}[1]{\textbf{\textit{#1}}}
%% Used for inline definitions of terms
\newcommand{\terminology}[1]{\textbf{#1}}
%% End: Semantic Macros
%% begin: environments for duplicates in solutions divisions
%% Solutions to inline exercises, style and environment
\tcbset{ inlinesolutionstyle/.style={bwminimalstyle, runintitlestyle, exercisespacingstyle, after title={\space}, breakable, parbox=false } }
\newtcolorbox{inlinesolution}[3]{inlinesolutionstyle, title={\hyperref[#3]{Exercise~#1}\notblank{#2}{\space#2}{}}}
%% Solutions to division exercises, not in exercise group
\tcbset{ divisionsolutionstyle/.style={bwminimalstyle, runintitlestyle, exercisespacingstyle, after title={\space}, breakable, parbox=false } }
\newtcolorbox{divisionsolution}[3]{divisionsolutionstyle, title={\hyperlink{#3}{#1}.\notblank{#2}{\space#2}{}}}
%% Divisional exercises (and worksheet) as LaTeX environments
%% Third argument is option for extra workspace in worksheets
%% Hanging indent occupies a 5ex width slot prior to left margin
%% Experimentally this seems just barely sufficient for a bold "888."
%% Division exercises, not in exercise group
\tcbset{ divisionexercisestyle/.style={bwminimalstyle, runintitlestyle, exercisespacingstyle, left=5ex, breakable, parbox=false } }
\newtcolorbox{divisionexercise}[4]{divisionexercisestyle, before title={\hspace{-5ex}\makebox[5ex][l]{#1.}}, title={\notblank{#2}{#2\space}{}}, phantom={\hypertarget{#4}{}}, after={\notblank{#3}{\newline\rule{\workspacestrutwidth}{#3}\newline\vfill}{\par}}}
%% Worksheet exercises may have workspaces
\newlength{\workspacestrutwidth}
%% @workspace strut is invisible
\setlength{\workspacestrutwidth}{0pt}
%% Localize LaTeX supplied names (possibly none)
\renewcommand*{\appendixname}{Appendix}
\renewcommand*{\chaptername}{Chapter}
%% Equation Numbering
%% Controlled by  numbering.equations.level  processing parameter
%% No adjustment here implies document-wide numbering
\numberwithin{equation}{section}
%% "tcolorbox" environment for a single image, occupying entire \linewidth
%% arguments are left-margin, width, right-margin, as multiples of
%% \linewidth, and are guaranteed to be positive and sum to 1.0
\tcbset{ imagestyle/.style={bwminimalstyle} }
\NewTColorBox{image}{mmm}{imagestyle,left skip=#1\linewidth,width=#2\linewidth}
%% Footnote Numbering
%% Specified by numbering.footnotes.level
%% Undo counter reset by chapter for a book
\counterwithout{footnote}{chapter}
\counterwithin*{footnote}{section}
%% QR Code Support
%% Videos and other interactives
\usepackage{qrcode}
\newlength{\qrsize}
\newlength{\previewwidth}
%% tcolorbox styles for interactive previews
%% changing size= and/or colback can aid in debugging
\tcbset{ previewstyle/.style={bwminimalstyle, halign=center} }
\tcbset{ qrstyle/.style={bwminimalstyle, hbox} }
\tcbset{ captionstyle/.style={bwminimalstyle, left=1em, width=\linewidth} }
%% Generic red play button (from SVG)
%% tikz package should be loaded by now
\definecolor{playred}{RGB}{230,33,23}
\newcommand{\genericpreview}{
        \begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]
        \path[fill=playred] (94.9800,28.8400) .. controls (94.9800,28.8400) and
        (94.0400,22.2400) .. (91.1700,19.3400) .. controls (87.5300,15.5300) and
        (83.4500,15.5100) .. (81.5800,15.2900) .. controls (68.1800,14.3200) and
        (48.0600,14.4400) .. (48.0600,14.4400) .. controls (48.0600,14.4400) and
        (27.9400,14.3200) .. (14.5400,15.2900) .. controls (12.6700,15.5100) and
        (8.5900,15.5300) .. (4.9500,19.3400) .. controls (2.0800,22.2400) and
        (1.1400,28.8400) .. (1.1400,28.8400) .. controls (1.1400,28.8400) and
        (0.1800,36.5800) .. (0.0000,44.3300) -- (0.0000,51.5900) .. controls
        (0.1800,59.3400) and (1.1400,67.0800) .. (1.1400,67.0800) .. controls
        (1.1400,67.0800) and (2.0700,73.6800) .. (4.9500,76.5800) .. controls
        (8.5900,80.3900) and (13.3800,80.2700) .. (15.5100,80.6700) .. controls
        (23.0400,81.3900) and (47.2100,81.5600) .. (48.0500,81.5700) .. controls
        (48.0600,81.5700) and (68.1900,81.6000) .. (81.5900,80.6300) .. controls
        (83.4600,80.4100) and (87.5400,80.3900) .. (91.1800,76.5800) .. controls
        (94.0500,73.6800) and (94.9900,67.0800) .. (94.9900,67.0800) .. controls
        (94.9900,67.0800) and (95.9500,59.3300) .. (96.0100,51.5900) --
        (96.0100,44.3300) .. controls (95.9400,36.5800) and (94.9800,28.8400) ..
        (94.9800,28.8400) -- cycle(38.2800,61.4100) -- (38.2800,34.4100) --
        (64.0200,47.9100) -- (38.2800,61.4100) -- cycle;
        \end{tikzpicture}
        }
%% Program listing support: for listings, programs, consoles, and Sage code
\ifthenelse{\boolean{xetex} \or \boolean{luatex}}%
  {\tcbuselibrary{listings}}%
  {\tcbuselibrary{listingsutf8}}%
%% We define the listings font style to be the default "ttfamily"
%% To fix hyphens/dashes rendered in PDF as fancy minus signs by listing
%% http://tex.stackexchange.com/questions/33185/listings-package-changes-hyphens-to-minus-signs
\makeatletter
\lst@CCPutMacro\lst@ProcessOther {"2D}{\lst@ttfamily{-{}}{-{}}}
\@empty\z@\@empty
\makeatother
%% We define a null language, free of any formatting or style
%% for use when a language is not supported, or pseudo-code, or consoles
%% Not necessary for Sage code, so in limited cases included unnecessarily
\lstdefinelanguage{none}{identifierstyle=,commentstyle=,stringstyle=,keywordstyle=}
\ifthenelse{\boolean{xetex}}{}{%
%% begin: pdflatex-specific listings configuration
%% translate U+0080 - U+00F0 to their textmode LaTeX equivalents
%% Data originally from https://www.w3.org/Math/characters/unicode.xml, 2016-07-23
%% Lines marked in XSL with "$" were converted from mathmode to textmode
\lstset{extendedchars=true}
\lstset{literate={ }{{~}}{1}{¡}{{\textexclamdown }}{1}{¢}{{\textcent }}{1}{£}{{\textsterling }}{1}{¤}{{\textcurrency }}{1}{¥}{{\textyen }}{1}{¦}{{\textbrokenbar }}{1}{§}{{\textsection }}{1}{¨}{{\textasciidieresis }}{1}{©}{{\textcopyright }}{1}{ª}{{\textordfeminine }}{1}{«}{{\guillemotleft }}{1}{¬}{{\textlnot }}{1}{­}{{\-}}{1}{®}{{\textregistered }}{1}{¯}{{\textasciimacron }}{1}{°}{{\textdegree }}{1}{±}{{\textpm }}{1}{²}{{\texttwosuperior }}{1}{³}{{\textthreesuperior }}{1}{´}{{\textasciiacute }}{1}{µ}{{\textmu }}{1}{¶}{{\textparagraph }}{1}{·}{{\textperiodcentered }}{1}{¸}{{\c{}}}{1}{¹}{{\textonesuperior }}{1}{º}{{\textordmasculine }}{1}{»}{{\guillemotright }}{1}{¼}{{\textonequarter }}{1}{½}{{\textonehalf }}{1}{¾}{{\textthreequarters }}{1}{¿}{{\textquestiondown }}{1}{À}{{\`{A}}}{1}{Á}{{\'{A}}}{1}{Â}{{\^{A}}}{1}{Ã}{{\~{A}}}{1}{Ä}{{\"{A}}}{1}{Å}{{\AA }}{1}{Æ}{{\AE }}{1}{Ç}{{\c{C}}}{1}{È}{{\`{E}}}{1}{É}{{\'{E}}}{1}{Ê}{{\^{E}}}{1}{Ë}{{\"{E}}}{1}{Ì}{{\`{I}}}{1}{Í}{{\'{I}}}{1}{Î}{{\^{I}}}{1}{Ï}{{\"{I}}}{1}{Ð}{{\DH }}{1}{Ñ}{{\~{N}}}{1}{Ò}{{\`{O}}}{1}{Ó}{{\'{O}}}{1}{Ô}{{\^{O}}}{1}{Õ}{{\~{O}}}{1}{Ö}{{\"{O}}}{1}{×}{{\texttimes }}{1}{Ø}{{\O }}{1}{Ù}{{\`{U}}}{1}{Ú}{{\'{U}}}{1}{Û}{{\^{U}}}{1}{Ü}{{\"{U}}}{1}{Ý}{{\'{Y}}}{1}{Þ}{{\TH }}{1}{ß}{{\ss }}{1}{à}{{\`{a}}}{1}{á}{{\'{a}}}{1}{â}{{\^{a}}}{1}{ã}{{\~{a}}}{1}{ä}{{\"{a}}}{1}{å}{{\aa }}{1}{æ}{{\ae }}{1}{ç}{{\c{c}}}{1}{è}{{\`{e}}}{1}{é}{{\'{e}}}{1}{ê}{{\^{e}}}{1}{ë}{{\"{e}}}{1}{ì}{{\`{\i}}}{1}{í}{{\'{\i}}}{1}{î}{{\^{\i}}}{1}{ï}{{\"{\i}}}{1}{ð}{{\dh }}{1}{ñ}{{\~{n}}}{1}{ò}{{\`{o}}}{1}{ó}{{\'{o}}}{1}{ô}{{\^{o}}}{1}{õ}{{\~{o}}}{1}{ö}{{\"{o}}}{1}{÷}{{\textdiv }}{1}{ø}{{\o }}{1}{ù}{{\`{u}}}{1}{ú}{{\'{u}}}{1}{û}{{\^{u}}}{1}{ü}{{\"{u}}}{1}{ý}{{\'{y}}}{1}{þ}{{\th }}{1}{ÿ}{{\"{y}}}{1}}
%% end: pdflatex-specific listings configuration
}
%% End of generic listing adjustments
%% Program listings via new tcblisting environment
%% First a universal color scheme for parts of any language
%% Colors match a subset of Google prettify "Default" style
%% Full colors for "electronic" version
%% http://code.google.com/p/google-code-prettify/source/browse/trunk/src/prettify.css
\definecolor{identifiers}{rgb}{0.375,0,0.375}
\definecolor{comments}{rgb}{0.5,0,0}
\definecolor{strings}{rgb}{0,0.5,0}
\definecolor{keywords}{rgb}{0,0,0.5}
%% Options passed to the listings package via tcolorbox
\lstdefinestyle{programcodestyle}{identifierstyle=\color{identifiers},commentstyle=\color{comments},stringstyle=\color{strings},keywordstyle=\color{keywords}, breaklines=true, breakatwhitespace=true, columns=fixed, extendedchars=true, aboveskip=0pt, belowskip=0pt}
\tcbset{ programboxstyle/.style={left=3ex, right=0pt, top=0ex, bottom=0ex, middle=0pt, toptitle=0pt, bottomtitle=0pt, boxsep=0pt, 
listing only, fontupper=\small\ttfamily,
colback=white, sharp corners, boxrule=-0.3pt, leftrule=0.5pt, toprule at break=-0.3pt, bottomrule at break=-0.3pt,
breakable, parbox=false,
} }
\newtcblisting{program}[4]{programboxstyle, left skip=#2\linewidth, width=#3\linewidth, listing options={language=#1, style=programcodestyle}}
%% The listings package as tcolorbox for Sage code
%% We do as much styling as possible with tcolorbox, not listings
%% Sage's blue is 50%, we go way lighter (blue!05 would also work)
%% Note that we defuse listings' default "aboveskip" and "belowskip"
\definecolor{sageblue}{rgb}{0.95,0.95,1}
\tcbset{ sagestyle/.style={left=0pt, right=0pt, top=0ex, bottom=0ex, middle=0pt, toptitle=0pt, bottomtitle=0pt,
boxsep=4pt, listing only, fontupper=\small\ttfamily,
breakable, parbox=false, 
listing options={language=Python,breaklines=true,breakatwhitespace=true, extendedchars=true, aboveskip=0pt, belowskip=0pt}} }
\newtcblisting{sageinput}{sagestyle, colback=sageblue, sharp corners, boxrule=0.5pt, toprule at break=-0.3pt, bottomrule at break=-0.3pt, }
\newtcblisting{sageoutput}{sagestyle, colback=white, colframe=white, frame empty, before skip=0pt, after skip=0pt, }
%% Fancy Verbatim for consoles, preformatted, code display, literate programming
\usepackage{fancyvrb}
%% code display (cd), by analogy with math display (md)
%% savebox, lrbox, etc to achieve centering
\newsavebox{\codedisplaybox}
\newenvironment{codedisplay}
{\VerbatimEnvironment\begin{center}\begin{lrbox}{\codedisplaybox}\begin{BVerbatim}}
{\end{BVerbatim}\end{lrbox}\usebox{\codedisplaybox}\end{center}}
%% More flexible list management, esp. for references
%% But also for specifying labels (i.e. custom order) on nested lists
\usepackage{enumitem}
%% Description lists as tcolorbox sidebyside
%% "dli" short for "description list item"
\newlength{\dlititlewidth}
\newlength{\dlimaxnarrowtitle}\setlength{\dlimaxnarrowtitle}{11ex}
\newlength{\dlimaxmediumtitle}\setlength{\dlimaxmediumtitle}{18ex}
\tcbset{ dlistyle/.style={sidebyside, sidebyside align=top seam, lower separated=false, bwminimalstyle, bottomtitle=0.75ex, after skip=1.5ex, boxsep=0pt, left=0pt, right=0pt, top=0pt, bottom=0pt} }
\tcbset{ dlinarrowstyle/.style={dlistyle, lefthand width=\dlimaxnarrowtitle, sidebyside gap=1ex, halign=flush left, righttitle=10ex} }
\tcbset{ dlimediumstyle/.style={dlistyle, lefthand width=\dlimaxmediumtitle, sidebyside gap=4ex, halign=flush right} }
\NewDocumentEnvironment{descriptionlist}{}{\par\vspace*{1.5ex}}{\par\vspace*{1.5ex}}%
%% begin enviroment has an if/then to open the tcolorbox
\NewDocumentEnvironment{dlinarrow}{mm}{%
\settowidth{\dlititlewidth}{{\textbf{#1}}}%
\ifthenelse{\dlititlewidth > \dlimaxnarrowtitle}%
{\begin{tcolorbox}[title={\textbf{#1}}, phantom={\hypertarget{#2}{}}, dlinarrowstyle]\tcblower}%
{\begin{tcolorbox}[dlinarrowstyle, phantom={\hypertarget{#2}{}}]\textbf{#1}\tcblower}%
}%
{\end{tcolorbox}}%
%% medium option is simpler
\NewDocumentEnvironment{dlimedium}{mm}%
{\begin{tcolorbox}[dlimediumstyle, phantom={\hypertarget{#2}{}}]\textbf{#1}\tcblower}%
{\end{tcolorbox}}%
%% hyperref driver does not need to be specified, it will be detected
%% Footnote marks in tcolorbox have broken linking under
%% hyperref, so it is necessary to turn off all linking
%% It *must* be given as a package option, not with \hypersetup
\usepackage[hyperfootnotes=false]{hyperref}
%% configure hyperref's  \href{}{}  and  \nolinkurl  to match listings' inline verbatim
\renewcommand\UrlFont{\small\ttfamily}
%% Hyperlinking active in electronic PDFs, all links without surrounding boxes and blue
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,filecolor=blue,urlcolor=blue}
\hypersetup{pdftitle={Lecture Notes for Math 3410, with Computational Examples}}
%% If you manually remove hyperref, leave in this next command
%% This will allow LaTeX compilation, employing this no-op command
\providecommand\phantomsection{}
%% Division Numbering: Chapters, Sections, Subsections, etc
%% Division numbers may be turned off at some level ("depth")
%% A section *always* has depth 1, contrary to us counting from the document root
%% The latex default is 3.  If a larger number is present here, then
%% removing this command may make some cross-references ambiguous
%% The precursor variable $numbering-maxlevel is checked for consistency in the common XSL file
\setcounter{secnumdepth}{3}
%%
%% AMS "proof" environment is no longer used, but we leave previously
%% implemented \qedhere in place, should the LaTeX be recycled
\newcommand{\qedhere}{\relax}
%%
%% A faux tcolorbox whose only purpose is to provide common numbering
%% facilities for most blocks (possibly not projects, 2D displays)
%% Controlled by  numbering.theorems.level  processing parameter
\newtcolorbox[auto counter, number within=section]{block}{}
%%
%% This document is set to number PROJECT-LIKE on a separate numbering scheme
%% So, a faux tcolorbox whose only purpose is to provide this numbering
%% Controlled by  numbering.projects.level  processing parameter
\newtcolorbox[auto counter, number within=section]{project-distinct}{}
%% A faux tcolorbox whose only purpose is to provide common numbering
%% facilities for 2D displays which are subnumbered as part of a "sidebyside"
\makeatletter
\newtcolorbox[auto counter, number within=tcb@cnt@block, number freestyle={\noexpand\thetcb@cnt@block(\noexpand\alph{\tcbcounter})}]{subdisplay}{}
\makeatother
%%
%% tcolorbox, with styles, for THEOREM-LIKE
%%
%% theorem: fairly simple numbered block/structure
\tcbset{ theoremstyle/.style={fonttitle=\normalfont\bfseries, colbacktitle=green!60!black!20, colframe=green!30!black!50, colback=white!95!green, coltitle=black, titlerule=-0.3pt,} }
\newtcolorbox[use counter from=block]{theorem}[3]{title={{Theorem~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, phantomlabel={#3}, breakable, parbox=false, after={\par}, fontupper=\itshape, theoremstyle, }
%% lemma: fairly simple numbered block/structure
\tcbset{ lemmastyle/.style={fonttitle=\normalfont\bfseries, colbacktitle=green!60!black!20, colframe=green!30!black!50, colback=white!95!green, coltitle=black, titlerule=-0.3pt,} }
\newtcolorbox[use counter from=block]{lemma}[3]{title={{Lemma~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, phantomlabel={#3}, breakable, parbox=false, after={\par}, fontupper=\itshape, lemmastyle, }
%%
%% tcolorbox, with styles, for PROOF-LIKE
%%
%% proof: title is a replacement
\tcbset{ proofstyle/.style={bwminimalstyle, fonttitle=\blocktitlefont\itshape, attach title to upper, after title={\space}, after upper={\space\space\hspace*{\stretch{1}}\(\blacksquare\)},
} }
\newtcolorbox{proof}[2]{title={\notblank{#1}{#1}{Proof.}}, phantom={\hypertarget{#2}{}}, breakable, parbox=false, after={\par}, proofstyle }
%%
%% tcolorbox, with styles, for DEFINITION-LIKE
%%
%% definition: fairly simple numbered block/structure
\tcbset{ definitionstyle/.style={fonttitle=\normalfont\bfseries, colbacktitle=yellow!90!black!30, colframe=yellow!95!black!60, colback=white!95!yellow, coltitle=black, titlerule=-0.3pt,} }
\newtcolorbox[use counter from=block]{definition}[2]{title={{Definition~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, definitionstyle, }
%%
%% tcolorbox, with styles, for REMARK-LIKE
%%
%% insight: fairly simple numbered block/structure
\tcbset{ insightstyle/.style={fonttitle=\normalfont\bfseries, colbacktitle=red!60!black!20, colframe=red!30!black!50, colback=white!95!red, coltitle=black, titlerule=-0.3pt,} }
\newtcolorbox[use counter from=block]{insight}[2]{title={{Insight~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, insightstyle, }
%%
%% tcolorbox, with styles, for EXAMPLE-LIKE
%%
%% example: fairly simple numbered block/structure
\tcbset{ examplestyle/.style={fonttitle=\normalfont\bfseries, colback=white, colframe=black, colbacktitle=white, coltitle=black,
      enhanced,
      breakable,
      frame hidden,
      overlay unbroken={
          \draw[thick] ([yshift=-2ex]frame.north west)--([yshift=2ex]frame.south west)--([xshift=2ex,yshift=2ex]frame.south west);
      },
      overlay first={
          \draw[thick] ([yshift=-2ex]frame.north west)--(frame.south west);
      },
      overlay middle={
          \draw[thick] ([yshift=-2ex]frame.north west)--(frame.south west);
      },
      overlay last={
          \draw[thick] ([yshift=-2ex]frame.north west)--([yshift=2ex]frame.south west)--([xshift=2ex,yshift=2ex]frame.south west);
      },
    } }
\newtcolorbox[use counter from=block]{example}[2]{title={{Example~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, examplestyle, }
%%
%% tcolorbox, with styles, for inline exercises
%%
%% inlineexercise: fairly simple numbered block/structure
\tcbset{ inlineexercisestyle/.style={fonttitle=\normalfont\bfseries, colback=white, colframe=black, colbacktitle=white, coltitle=black,
      enhanced,
      breakable,
      frame hidden,
      overlay unbroken={
          \draw[thick] ([yshift=-2ex]frame.north west)--([yshift=2ex]frame.south west)--([xshift=2ex,yshift=2ex]frame.south west);
      },
      overlay first={
          \draw[thick] ([yshift=-2ex]frame.north west)--(frame.south west);
      },
      overlay middle={
          \draw[thick] ([yshift=-2ex]frame.north west)--(frame.south west);
      },
      overlay last={
          \draw[thick] ([yshift=-2ex]frame.north west)--([yshift=2ex]frame.south west)--([xshift=2ex,yshift=2ex]frame.south west);
      },
    } }
\newtcolorbox[use counter from=block]{inlineexercise}[2]{title={{Exercise~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, inlineexercisestyle, }
%%
%% tcolorbox, with styles, for ASIDE-LIKE
%%
%% aside: fairly simple un-numbered block/structure
\tcbset{ asidestyle/.style={enhanced, colback=white, colframe=white,
coltitle=black, fonttitle=\bfseries, attach title to upper, after title={\space},left=1pt,} }
\newtcolorbox{aside}[2]{title={\notblank{#1}{#1}{}}, phantomlabel={#2}, breakable, parbox=false, asidestyle}
%%
%% tcolorbox, with styles, for FIGURE-LIKE
%%
%% figureptx: 2-D display structure
\tcbset{ figureptxstyle/.style={bwminimalstyle, middle=1ex, blockspacingstyle, fontlower=\blocktitlefont} }
\newtcolorbox[use counter from=block]{figureptx}[3]{lower separated=false, before lower={{\textbf{Figure~\thetcbcounter}\space#1}}, phantomlabel={#2}, unbreakable, parbox=false, figureptxstyle, }
%% listingptx: 2-D display structure
\tcbset{ listingptxstyle/.style={bwminimalstyle, middle=1ex, blockspacingstyle, fontlower=\blocktitlefont} }
\newtcolorbox[use counter from=block]{listingptx}[3]{lower separated=false, before lower={{\textbf{Listing~\thetcbcounter}\space#1}}, phantomlabel={#2}, unbreakable, parbox=false, listingptxstyle, }
%%
%% xparse environments for introductions and conclusions of divisions
%%
%% introduction: in a structured division
\NewDocumentEnvironment{introduction}{m}
{\notblank{#1}{\noindent\textbf{#1}\space}{}}{\par\medskip}
%%
%% tcolorbox, with styles, for miscellaneous environments
%%
%% paragraphs: the terminal, pseudo-division
%% We use the lowest LaTeX traditional division
\titleformat{\subparagraph}[runin]{\normalfont\normalsize\bfseries}{\thesubparagraph}{1em}{#1}
\titlespacing*{\subparagraph}{0pt}{3.25ex plus 1ex minus .2ex}{1em}
\NewDocumentEnvironment{paragraphs}{mm}
{\subparagraph*{#1}\hypertarget{#2}{}}{}
%% Graphics Preamble Entries
\usepackage{pgfplots}
\usetikzlibrary{positioning,matrix,arrows,arrows.meta}
%% If tikz has been loaded, replace ampersand with \amp macro
\ifdefined\tikzset
    \tikzset{ampersand replacement = \amp}
\fi
%% extpfeil package for certain extensible arrows,
%% as also provided by MathJax extension of the same name
%% NB: this package loads mtools, which loads calc, which redefines
%%     \setlength, so it can be removed if it seems to be in the 
%%     way and your math does not use:
%%     
%%     \xtwoheadrightarrow, \xtwoheadleftarrow, \xmapsto, \xlongequal, \xtofrom
%%     
%%     we have had to be extra careful with variable thickness
%%     lines in tables, and so also load this package late
\usepackage{extpfeil}
%% Custom Preamble Entries, late (use latex.preamble.late)
 \newlength{\Textw} % save textwidth outside the boxes
 \setlength{\Textw}{\textwidth}
 \newlength{\Hshift}
 \newlength{\Mshift}
 \newcommand*{\marginshift}{%
     \setlength{\Hshift}{5.5mm}
     \setlength{\Mshift}{\marginparsep}
     }
     
 \newcommand{\tcbmarginbox}[2]{%
     \marginshift
     \SetHorizontalCoffin\Framex{} %clear box Framex
     \SetVerticalCoffin\Theox{\marginparwidth}{#1}% fill box \Theox
     \JoinCoffins*\Framex[r,vc]\Theox[l,vc](\dimexpr\Mshift+\textwidth\relax,#2)%join boxes
     \noindent\TypesetCoffin\Framex(\Hshift,0pt)\\[-2\baselineskip] %typset assembly
 }
 
 \newcommand{\parmarginbox}[2]{%
     \marginshift
     \SetHorizontalCoffin\Framex{}
     \SetVerticalCoffin\Theox{\marginparwidth}{#1}
     \JoinCoffins*\Framex[r,vc]\Theox[l,vc](\dimexpr\Mshift+\textwidth\relax,#2)
     \noindent\TypesetCoffin\Framex(0mm,0pt)\\[-2\baselineskip]
 }
%% Begin: Author-provided packages
%% (From  docinfo/latex-preamble/package  elements)
%% End: Author-provided packages
%% Begin: Author-provided macros
%% (From  docinfo/macros  element)
%% Plus three from PTX for XML characters
\newcommand{\spn}{\operatorname{span}}
\newcommand{\bbm}{\begin{bmatrix}}
\newcommand{\ebm}{\end{bmatrix}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\nll}{\operatorname{null}}
\newcommand{\csp}{\operatorname{col}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\dotp}{\!\boldsymbol{\cdot}\!}
\newcommand{\len}[1]{\lVert #1\rVert}
\newcommand{\abs}[1]{\lvert #1\rvert}
\newcommand{\proj}[2]{\operatorname{proj}_{#1}{#2}}
\newcommand{\bz}{\overline{z}}
\newcommand{\zz}{\mathbf{z}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\ww}{\mathbf{w}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\zer}{\mathbf{0}}
\newcommand{\vecq}{\mathbf{q}}
\newcommand{\vecp}{\mathbf{p}}
\newcommand{\vece}{\mathbf{e}}
\newcommand{\basis}[2]{\{\mathbf{#1}_1,\mathbf{#1}_2,\ldots,\mathbf{#1}_{#2}\}}
\newcommand{\lt}{<}
\newcommand{\gt}{>}
\newcommand{\amp}{&}
%% End: Author-provided macros
\begin{document}
%% bottom alignment is explicit, since it normally depends on oneside, twoside
\raggedbottom
\frontmatter
%% begin: half-title
\thispagestyle{empty}
{\titlepagefont\centering
\vspace*{0.28\textheight}
{\Huge Lecture Notes for Math 3410, with Computational Examples}\\}
\clearpage
%% end:   half-title
%% begin: title page
%% Inspired by Peter Wilson's "titleDB" in "titlepages" CTAN package
\thispagestyle{empty}
{\titlepagefont\centering
\vspace*{0.14\textheight}
%% Target for xref to top-level element is ToC
\addtocontents{toc}{\protect\hypertarget{x:book:linear-algebra}{}}
{\Huge Lecture Notes for Math 3410, with Computational Examples}\\[3\baselineskip]
{\Large Sean Fitzpatrick}\\[0.5\baselineskip]
{\Large University of Lethbridge}\\[3\baselineskip]
{\Large March 22, 2022}\\}
\clearpage
%% end:   title page
%% begin: copyright-page
\thispagestyle{empty}
\hypertarget{g:colophon:idp1}{}\vspace*{\stretch{2}}
\noindent{\bfseries Edition}: Version 1.0.0 (just past beta?)\par\medskip
\noindent\textcopyright{}2021\quad{}Sean Fitzpatrick\\[0.5\baselineskip]
Licensed to the public under Creative Commons Attribution-Noncommercial 4.0 International Public License\par\medskip
\vspace*{\stretch{1}}
\null\clearpage
%% end:   copyright-page
%
%
\typeout{************************************************}
\typeout{Preface  Preface}
\typeout{************************************************}
%
\begin{preface}{Preface}{}{Preface}{}{}{g:preface:idp2}
Linear algebra is a mature, rich subject, full of both fascinating theory and useful applications. One of the things you might have taken away from a first course in the subject is that there's a lot of tedious calculation involved. This is true, if you're a human. But the algorithms you learn in a course like Math 1410 are easily implemented on a computer. If we want to be able to discuss any of the interesting applications of linear algebra, we're going to need to learn how to do linear algebra on a computer.%
\par
There are many good mathematical software products that can deal with linear algebra, like Maple, Mathematica, and MatLab. But all of these are proprietary, and expensive. Sage is a popular open source system for mathematics, and students considering further studies in mathematics would do well to learn Sage. Since most people in Math 3410 are probably not considering a career as a mathematician, we'll try to do everything in Python.%
\par
Python is a very popular programming language, partly because of its ease of use. Those of you enrolled in Education may find yourself teaching Python to your students one day. Also, if you do want to use Sage, you're in luck: Sage is an amalgamation of many different software tools, including Python. So any Python code you encounter in this course can also be run on Sage. \emph{You do not have to be a programmer} to run the code in this book. We'll be primarily working with the \terminology{SymPy} Python library, which provides many easy to use functions for operations like determinant and inverse.%
\par
These notes originally began as an attempt to make Python-based worksheets that could be exported from PreTeXt to Jupyter, for use in the classroom. It quickly became apparent that something more was needed, and the worksheets morphed into lecture notes. These are intended to serve as a textbook for Math 3410, but with some work they can also be used in class. The notes are written in PreTeXt, and can be converted to both Jupyter notebooks and reveal.js slides.%
\par
I initially wrote these notes during the Fall 2019 semester, for Math 3410 at the University of Lethbridge. The original textbook for the course was \emph{Linear Algebra with Applications}, by Keith Nicholson. This book is available as an open education resource from \href{https://lyryx.com/linear-algebra-applications/}{Lyryx Learning}\footnote{\nolinkurl{lyryx.com/linear-algebra-applications/}\label{g:fn:idp3}}. Since these notes followed our course, and the course, to some extent, followed Nicholson's book, the notation, terminology, and (to some extent) organization of content aligns with that in Nicholson. (However, in a few places a careful reader might be able to detect the influence of Sheldon Axler's beautiful \emph{Linear Algebra Done Right}.)%
\end{preface}
%% begin: table of contents
%% Adjust Table of Contents
\setcounter{tocdepth}{1}
\renewcommand*\contentsname{Contents}
\tableofcontents
%% end:   table of contents
\mainmatter
%
%
\typeout{************************************************}
\typeout{Chapter 1 Vector spaces}
\typeout{************************************************}
%
\begin{chapterptx}{Vector spaces}{}{Vector spaces}{}{}{x:chapter:ch-vector-space}
\begin{introduction}{}%
In your first course in linear algebra, you likely worked a lot with vectors in two and three dimensions, where they can be visualized geometrically as objects with magnitude and direction (and drawn as arrows). You probably extended your understanding of vectors to include \emph{column vectors}; that is, \(1\times n\) matrices of the form \(\vv=\bbm v_1\\v_2\\\vdots\\v_n\ebm\).%
\par
Using either geometric arguments (in \(\R^2\) or \(\R^3\)) or the properties of matrix arithmetic, you would have learned that these vectors can be added, by adding corresponding components, and multiplied by \emph{scalars} \textemdash{} that is, real numbers \textemdash{} by multiplying each component of the vector by the scalar.%
\par
It's also likely, although you may not have spent too long thinking about it, that you looked at the properties obeyed by the addition and scalar multiplication of vectors (or, for that matter, matrices). For example, you may have made use of the fact that order of addition doesn't matter, or that scalar multiplication distributes over addition.%
\par
It turns out that the algebraic properties satisfied by vector addition and scalar multiplication are not unique to vectors, as vectors were understood in your first course in linear algebra. In fact, many types of mathematical object exhibit similar behaviour. Examples include matrices, polynomials, and even functions.%
\par
Linear algebra, as an abstract mathematical topic, begins with a realization of the importance of these properties. Indeed, these properties, established as theorems for vectors in \(\R^n\), become the \emph{axioms} for the abstract notion of a \emph{vector space}. The advantage of abstracting these ideas is that any proofs we write that depend only on these axioms will automatically be valid for any set of objects satisfying those axioms. That is, a result that is true for vectors in \(\R^2\) is often also true for vectors in \(\R^n\), and for matrices, and polynomials, and so on. Mathematicians like to be efficient, and prefer to establish a result once in an abstract setting, knowing that it will then apply to many concrete settings that fit into the framework of the abstract result.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Section 1.1 Abstract vector spaces}
\typeout{************************************************}
%
\begin{sectionptx}{Abstract vector spaces}{}{Abstract vector spaces}{}{}{x:section:sec-vec-sp}
\begin{definition}{}{x:definition:def-vector-space}%
A \terminology{real vector space} (or vector space over \(\R\)) is a nonempty set \(V\), whose objects are called \terminology{vectors}, equipped with two operations:%
\begin{enumerate}
\item{}\terminology{Addition}, which is a map from \(V\times V\) to \(V\) that associates each ordered pair of vectors \((\vv,\ww)\) to a vector \(\vv+\ww\), called the \terminology{sum} of \(\vv\) and \(\ww\).%
\item{}\terminology{Scalar multiplication}, which is a map from \(\R\times V\) to \(V\) that associates each real number \(c\) and vector \(\vv\) to a vector \(c\vv\).%
\end{enumerate}
%
\par
The operations of addition and scalar multiplication are required to satisfy the following \emph{axioms}:%
\begin{descriptionlist}
\begin{dlimedium}{A1.}{g:li:idp4}%
If \(\uu,\vv\in V\), then \(\uu+\vv\in V\). (Closure under addition)%
\end{dlimedium}%
\begin{dlimedium}{A2.}{g:li:idp5}%
For all \(\uu,\vv\in V\), \(\uu+\vv=\vv+\uu\). (Commutativity of addition)%
\end{dlimedium}%
\begin{dlimedium}{A3.}{g:li:idp6}%
For all \(\uu,\vv,\ww\in V\), \(\uu+(\vv+\ww)=(\uu+\vv)+\ww\). (Associativity of addition)%
\end{dlimedium}%
\begin{dlimedium}{A4.}{g:li:idp7}%
There exists an element \(\zer\in V\) such that \(\vv+\zer=\vv\) for each \(\vv\in V\). (Existence of a zero vector)%
\end{dlimedium}%
\begin{dlimedium}{A5.}{g:li:idp8}%
For each \(\vv\in V\), there exists a vector \(-\vv\in V\) such that \(\vv+(-\vv)=\zer\). (Existence of negatives)%
\end{dlimedium}%
\begin{dlimedium}{S1.}{g:li:idp9}%
If \(\vv\in V\), then \(c\vv\in V\) for all \(c\in\R\). (Closure under scalar multiplication)%
\end{dlimedium}%
\begin{dlimedium}{S2.}{g:li:idp10}%
For all \(c\in \R\) and \(\vv,\ww\in V\), \(c(\vv+\ww)=c\vv+c\ww\). (Distribution over vector addition)%
\end{dlimedium}%
\begin{dlimedium}{S3.}{g:li:idp11}%
For all \(a,b\in\R\) and \(\vv\in V\), \((a+b)\vv=a\vv+b\vv\). (Distribution over scalar addition)%
\end{dlimedium}%
\begin{dlimedium}{S4.}{g:li:idp12}%
For all \(a,b\in \R\) and \(\vv\in V\), \(a(b\vv)=(ab)\vv\). (Associativity of scalar multiplication)%
\end{dlimedium}%
\begin{dlimedium}{S5.}{g:li:idp13}%
For all \(\vv\in V\), \(1\vv=\vv\). (Normalization of scalar multiplication)%
\end{dlimedium}%
\end{descriptionlist}
%
\end{definition}
Note that a zero vector must exist in every vector space. This simple observation is a key component of many proofs and counterexamples in linear algebra. In general, we may define a vector space whose scalars belong to a \emph{field} \(\mathbb{F}\). A field is a set of objects whose algebraic properties are modelled after those of the real numbers. Fields have their \href{https://en.wikipedia.org/wiki/Field_(mathematics)\#Classic_definition}{own set of axioms}\footnote{\nolinkurl{en.wikipedia.org/wiki/Field_(mathematics)\#Classic_definition}\label{g:fn:idp14}}, which we will not list here. While it is possible to study linear algebra over \emph{finite fields} (like the integers modulo a prime number) we will only consider two fields: the real numbers \(\R\), and the complex numbers \(\C\).%
\par
A vector space whose scalars are complex numbers will be called a \emph{complex vector space}. While many students are initially intimidated by the complex numbers, most results in linear algebra work exactly the same over \(\C\) as they do over \(\R\). And where the results differ, things are usually \emph{easier} with complex numbers, owing in part to the fact that all complex polynomials can be completely factored.%
\par
To help us gain familiarity with the abstract nature of \hyperref[x:definition:def-vector-space]{Definition~{\xreffont\ref{x:definition:def-vector-space}}}, let us consider some basic examples.%
\begin{example}{}{x:example:ex-vector-spaces}%
The following are examples of vector spaces. We leave verification of axioms as an exercise.%
\begin{enumerate}
\item{}The set \(\R^n\) of \(n\)-tuples \((x_1,x_2,\ldots, x_n)\) of real numbers, where we define%
\begin{align*}
(x_1,x_2,\ldots, x_n)+(y_1,y_2,\ldots, y_n) \amp = (x_1+y_1,x_2+y_2,\ldots, x_n+y_n) \\
c(x_1,x_2,\ldots, x_n)\amp = (cx_1,cx_2,\ldots, cx_n)\text{.}
\end{align*}
We will also often use \(\R^n\) to refer to the vector space of \(1\times n\) column matrices \(\bbm x_1\\x_2\\\vdots\\x_n\ebm\), where addition and scalar multiplication are defined as for matrices (and the same as the above, with the only difference being the way in which we choose to write our vectors). If the distinction between \(n\)-tuples and column matrices is ever important, it will be made clear.%
\item{}The set \(\mathbf{M}_{mn}\) of \(m\times n\) matrices, equipped with the usual matrix addition and scalar multiplication.%
\item{}The set \(\mathbf{P}_n\) of all polynomials%
\begin{equation*}
p(x) = a_0+a_1x+\cdots + a_nx^n
\end{equation*}
of degree less than or equal to \(n\), where, for%
\begin{align*}
p(x) \amp = a_0+a_1x+\cdots + a_nx^n \\
q(x) \amp = b_0+b_1x+\cdots +b_nx^n
\end{align*}
we define%
\begin{equation*}
p(x)+q(x)=(a_0+b_0)+(a_1+b_1)x+\cdots + (a_n+b_n)x^n
\end{equation*}
and%
\begin{equation*}
cp(x) = ca_0+(ca_1)x+\cdots + (ca_n)x^n\text{.}
\end{equation*}
The zero vector is the polynomial \(0=0+0x+\cdots + 0x^n\).%
\item{}The set \(\mathbf{F}[a,b]\) of all functions \(f:[a,b]\to \R\), where we define \((f+g)(x)=f(x)+g(x)\) and \((cf)(x)=c(f(x))\). The zero function is the function satisfying \(0(x)=0\) for all \(x\in [a,b]\), and the negative of a function \(f\) is given by \((-f)(x)=-f(x)\) for all \(x\in [a,b]\).%
\end{enumerate}
%
\end{example}
Other common examples of vector spaces can be found online; for example, \href{https://en.wikipedia.org/wiki/Examples_of_vector_spaces}{on Wikipedia}\footnote{\nolinkurl{en.wikipedia.org/wiki/Examples_of_vector_spaces}\label{g:fn:idp15}}. It is also interesting to try to think of less common examples. For example, can you think of a way to define a vector space structure on the set \((0,\infty)\) of all positive real numbers?%
\par
There are a number of other algebraic properties that are common to all vector spaces; for example, it is true that \(0\vv = \zer\) for all vectors \(\vv\) in any vector space \(V\). The reason these are not included is that the ten axioms in \hyperref[x:definition:def-vector-space]{Definition~{\xreffont\ref{x:definition:def-vector-space}}} are the ones deemed ``essential'' \textendash{} all other properties can be deduced from the axioms. To demonstrate, we next give the proof that \(0\vv = \zer\).%
\begin{theorem}{}{}{x:theorem:thm-zero-mult}%
In any vector space \(V\), we have \(0\vv = \zer\) for all \(\vv\in V\)%
\end{theorem}
\begin{proof}{}{g:proof:idp16}
Since \(0+0=0\), we have \(0\vv=(0+0)\vv\). Using the distributive axiom S3, this becomes%
\begin{equation*}
0\vv + 0\vv = 0\vv\text{.}
\end{equation*}
By axiom A5, there is an element \(-0\vv\in V\) such that \(0\vv+(-0\vv)=\zer\). Adding this to both sides of the equation above, we get:%
\begin{equation*}
(0\vv+0\vv)+(-0\vv) = 0\vv+(-0\vv)\text{.}
\end{equation*}
Now, apply the associative property (A3) on the left, and A5 on the right, to get%
\begin{equation*}
0\vv + (0\vv+(-0\vv)) = \zer\text{.}
\end{equation*}
Using A5 again on the left, we get \(0\vv+\zer = \zer\). Finally, axiom A4 guarantees \(0\vv = 0\vv+\zer = \zer\).%
\end{proof}
Similar tactics can be used to establish the following results, which we leave as an exercise. Solutions are included, but it will be worth your while in the long run to wrestle with these.%
\begin{inlineexercise}{}{x:exercise:ex-more-props}%
Show that the following properties are valid in any vector space \(V\):%
\begin{enumerate}
\item{}If \(\uu+\vv=\uu+\ww\), then \(\vv=\ww\).%
\item{}For any scalar \(c\), \(c\zer=\zer\).%
\item{}If \(c\vv=\zer\), then either \(c=0\) or \(\vv=\zer\).%
\item{}For any \(\vv\in V\), \((-1)\vv=-\vv\).%
\item{}The zero vector is the unique vector such that \(\vv+\zer=\vv\) for all \(\vv\in V\).%
\item{}The negative \(-\vv\) of any vector \(\vv\) is unique.%
\end{enumerate}
%
\end{inlineexercise}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 1.2 Subspaces}
\typeout{************************************************}
%
\begin{sectionptx}{Subspaces}{}{Subspaces}{}{}{x:section:sec-subspace}
We begin with a motivating example. Let \(\vv\) be a nonzero vector in some vector space \(V\). Consider the set \(S = \{c\vv\,|\, c\in \R\}\). Given \(a\vv,b\vv\in S\), notice that \(a\vv+b\vv=(a+b)\vv\) is also an element of \(S\), since \(a+b\) is again a real number. Moreover, for any real number \(c\), \(c(a\vv)=(ca)\vv\) is an element of \(S\).%
\par
There are two important observations: one is that performing addition or scalar multiplication on elements of \(S\) produces a new element of \(S\). The other is that this addition and multiplication is essentially that of \(\R\). The vector \(\vv\) is just a placeholder. Addition simply involves the real number addition \(a+b\). Scalar multiplication becomes the real number multiplication \(ca\). So we expect that the rules for addition and scalar multiplication in \(S\) follow those in \(\R\), so that \(S\) is like a ``copy'' of \(\R\) inside of \(V\). In particular, addition and scalar multiplication in \(S\) will satisfy all the vector space axioms, so that \(S\) deserves to be considered a vector space in its own right.%
\par
A similar thing happens if we consider a set \(U=\{a\vv+b\ww\,|\, a,b\in\R\}\), where \(\vv,\ww\) are two vectors in a vector space \(V\). Given two elements \(a_1\vv+a_2\ww,b_1\uu+b_2\ww\), we have%
\begin{equation*}
(a_1\vv+a_2\ww)+(b_1\vv+b_2\ww) = (a_1+b_1)\vv+(a_2+b_2)\ww\text{,}
\end{equation*}
which is again an element of \(U\), and the addition rule looks an awful lot like the addition rule \((a_1,a_2)+(b_1,b_2)=(a_1+b_1,a_2+b_2)\) in \(\R^2\). Scalar multiplication follows a similar pattern.%
\par
In general we are often interested in subsets of vectors spaces that behave like ``copies'' of smaller vector spaces contained within the larger space. The technical term for this is \emph{subspace}.%
\begin{definition}{}{x:definition:def-subspace}%
Let \(V\) be a vector space, and let \(U\subseteq V\) be a subset. We say that \(U\) is a \terminology{subspace} of \(V\) if \(U\) is itself a vector space when using the addition and scalar multiplication of \(V\).%
\end{definition}
If we were to follow the definition, then verifying that a subset \(U\) is a subspace would involve checking all ten vector space axioms. Fortunately, this is not necessary. Since the operations are those of the vector space \(V\), most properties follow automatically, being inherited from those of \(V\).%
\begin{theorem}{}{}{x:theorem:thm-subspace-test}%
Let \(V\) be a vector space and let \(U\subseteq V\) be a subset. Then \(U\) is a subspace of \(V\) if and only if the following conditions are satisfied:%
\begin{enumerate}
\item{}\(\zer\in U\), where \(\zer\) is the zero vector of \(V\).%
\item{}\(U\) is closed under addition. That is, for all \(\uu_1,\uu_2\in U\), we have \(\uu_1+\uu_2\in U\).%
\item{}\(U\) is closed under scalar multiplication. That is, for all \(\uu\in U\) and \(c\in\R\), \(c\uu\in U\).%
\end{enumerate}
%
\end{theorem}
\begin{proof}{}{g:proof:idp17}
If \(U\) is a vector space, then clearly the second and third conditions must hold. Since a vector space must be nonempty, there is some \(\uu\in U\), from which it follows that \(\zer=0\uu\in U\).%
\par
Conversely, if all three conditions hold, we have axioms A1, A4, and S1 by assumption. Axioms A2 and A3 hold since any vector in \(U\) is also a vector in \(V\); the same reasoning shows that axioms S2, S3, S4, and S5 hold. Finally, axiom A5 holds because condition 3 ensures that \((-1)\uu\in U\) for any \(\uu\in U\), and we know that \((-1)\uu=-\uu\) by \hyperref[x:exercise:ex-more-props]{Exercise~{\xreffont\ref{x:exercise:ex-more-props}}}.%
\end{proof}
In some texts, the condition that \(\zer\in U\) is replaced by the requirement that \(U\) be nonempty. Existence of \(\zer\) then follows from the fact that \(0\vv=\zer\). However, it is usually easy to check that a set contains the zero vector, so it's the first thing one typically looks for when confirming that a subset is nonempty.%
\begin{example}{}{g:example:idp18}%
For any vector space \(V\), the set \(\{\zer\}\) is a subspace, known as the \emph{trivial subspace}.%
\par
If \(V=\mathbf{P}\) is the vector space of all polynomials, then for any natural number \(n\), the subset \(U\) of all polynomials of degree less than or equal to \(n\) is a subspace of \(V\). Another common type of polynomial subspace is the set of all polynomials with a given root. For example, the set \(U=\{p(x)\in\mathbf{P}\,|\,p(1)=0\}\) is easily confirmed to be a subspace. However, a condition such as \(p(1)=2\) would \emph{not} define a subspace, since this condition is not satisfied by the zero polynomial.%
\par
In \(\R^n\), we can define a subspace using one or more homogeneous linear equations. For example, the set%
\begin{equation*}
\{(x,y,z)\,|\, 2x-3y+4z=0\}
\end{equation*}
is a subspace of \(\R^3\). A non-homogeneous equation won't work, however, since it would exclude the zero vector. Of course, we should expect that any non-linear equation fails to define a subspace, although one is still expected to verify this by confirming the failure of one of the axioms. For example, the set \(S=\{(x,y)\,|\,x=y^2\}\) is not a subspace; although it contains the zero vector (since \(0^2=0\)), we have \((1,1)\in S\), but \(2(1,1)=(2,2)\) does not belong to \(S\).%
\end{example}
In the next section, we'll encounter perhaps the most fruitful source of subspaces: sets of linear combinations (or \emph{spans}).%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 1.3 Span}
\typeout{************************************************}
%
\begin{sectionptx}{Span}{}{Span}{}{}{x:section:sec-span}
Recall that a \terminology{linear combination} of a set of vectors \(\vv_1,\ldots, \vv_k\) is a vector expression of the form%
\begin{equation*}
\ww=c_1\vv_1+c_2\vv_2+\cdots +c_k\vv_k,
\end{equation*}
where \(c_1,\ldots, c_k\) are scalars.%
\par
The \terminology{span} of those same vectors is the set of all possible linear combinations:%
\begin{equation*}
\spn\{\vv_1,\ldots, \vv_k\} = \{c_1\vv_1+ \cdots + c_k\vv_k \,|\, c_1,\ldots, c_k \in \mathbb{F}\}.
\end{equation*}
Therefore, the questions ``Is the vector \(\ww\) in \(\spn\{\vv_1,\ldots, \vv_k\}\)?'' is really asking, ``Can \(\ww\) be written as a linear combination of \(\vv_1,\ldots, \vv_k\)?''%
\par
With the appropriate setup, all such questions become questions about solving systems of equations. Here, we will look at a few such examples.%
\begin{inlineexercise}{}{g:exercise:idp19}%
Determine whether the vector \(\bbm 2\\3\ebm\) is in the span of the vectors \(\bbm 1\\1\ebm,\bbm -1\\2\ebm\).%
\end{inlineexercise}%
Our next example involves polynomials. At first this looks like a different problem, but it's essentially the same once we set it up.%
\begin{inlineexercise}{}{g:exercise:idp20}%
Determine whether \(p(x)=1+x+4x^2\) belongs to \(\spn\{1+2x-x^2,3+5x+2x^2\}\).%
\end{inlineexercise}%
One of the reasons we care about linear combinations and span is that it gives us an easy means of generating subspaces, as the following theorem suggests.%
\begin{theorem}{}{}{x:theorem:thm-span-is-subspace}%
Let \(V\) be a vector space, and let \(\vv_1,\vv_2,\ldots, \vv_k\) be vectors in \(V\). Then:%
\begin{enumerate}
\item{}\(U=\spn\{\vv_1,\vv_2,\ldots, \vv_k\}\) is a subspace of \(V\).%
\item{}\(U\) is the \emph{smallest} subspace of  \(V\) containing \(\vv_1,\ldots, \vv_k\), in the sense that if \(W\subseteq V\) is a subspace and \(\vv_1,\ldots, \vv_k\in W\), then \(U\subseteq W\).%
\end{enumerate}
%
\end{theorem}
\begin{proof}{}{g:proof:idp21}
Let \(U=\spn\{\vv_1,\vv_2,\ldots, \vv_k\}\). Then \(0\in U\), since \(0=0\vv_1+0\vv_2+\cdots + 0\vv_k\). If \(\uu=a_1\vv_1+a_2\vv_2+\cdots +a_k\vv_k\) and \(\ww=b_1\vv_1+b_2\vv_2+\cdots +b_k\vv_k\) are vectors in \(U\), then%
\begin{align*}
\uu+\ww \amp =(a_1\vv_1+a_2\vv_2+\cdots +a_k\vv_k)+(b_1\vv_1+b_2\vv_2+\cdots +b_k\vv_k)\\
\amp = (a_1+b_1)\vv_1+(a_2+b_2)\vv_2+\cdots + (a_k+b_k)\vv_k
\end{align*}
is in \(U\), and%
\begin{align*}
c\uu \amp =c(a_1\vv_1+a_2\vv_2+\cdots +a_k\vv_k)\\
\amp =(ca_1)\vv_1+(ca_2)\vv_2+\cdots + (ca_k)\vv_k
\end{align*}
is in \(U\), so by \hyperref[x:theorem:thm-subspace-test]{Theorem~{\xreffont\ref{x:theorem:thm-subspace-test}}}, \(U\) is a subspace.%
\par
To see that \(U\) is the smallest subspace containing \(\vv_1,\ldots, \vv_k\), we need only note that if \(\vv_1,\ldots, \vv_k\in W\), where \(W\) is a subspace, then since \(W\) is closed under scalar multiplication, we know that \(c_1\vv_1,\ldots, c_k\vv_k\) for any scalars \(c_1,\ldots, c_k\), and since \(W\) is closed under addition, \(c_1\vv_1+\cdots+c_k\vv_k\in W\). Thus, \(W\) contains all linear combinations of \(\vv_1,\ldots, \vv_k\), which is to say that \(W\) contains \(U\).%
\end{proof}
We end this section with a few non-computational, but useful results, which will be left as exercises to be done in class, or by the reader.%
\begin{inlineexercise}{}{g:exercise:idp22}%
Let \(V\) be a vector space, and let \(X,Y\subseteq V\). Show that if \(X\subseteq Y\), then \(\spn X \subseteq \spn Y\).%
\end{inlineexercise}%
\begin{inlineexercise}{}{g:exercise:idp23}%
Can \(\{(1,2,0), (1,1,1)\}\) span \(\{(a,b,0)\,|\, a,b \in\R\}\)?%
\end{inlineexercise}%
\begin{theorem}{}{}{x:theorem:theorem-surplus-span}%
Let \(V\) be a vector space, and let \(\vv_1,\ldots, \vv_k\in V\). If \(\uu\in \spn\{\vv_1,\ldots, \vv_k\}\), then%
\begin{equation*}
\spn\{\uu,\vv_1,\ldots, \vv_k\} = \spn\{\vv_1,\ldots, \vv_k\}\text{.}
\end{equation*}
%
\end{theorem}
The moral of \hyperref[x:theorem:theorem-surplus-span]{Theorem~{\xreffont\ref{x:theorem:theorem-surplus-span}}} is that one vector in a set is a linear combination of the others, we can remove it from the set without affecting the span. This suggests that we might want to look for the most ``efficient'' spanning sets \textendash{} those in which no vector in the set can be written in terms of the others. Such sets are called \terminology{linearly independent}, and they are the subject of \hyperref[x:section:sec-independence]{Section~{\xreffont\ref{x:section:sec-independence}}}.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Worksheet 1.4 Worksheet: understanding span}
\typeout{************************************************}
%
\newgeometry{left=1.25cm, right=1.25cm, top=1.25cm, bottom=1.25cm}
\begin{worksheet-section}{Worksheet: understanding span}{}{Worksheet: understanding span}{}{}{x:worksheet:worksheet-span}
\begin{introduction}{}%
In this worksheet, we will attempt to understand the concept of span. Recall from \hyperref[x:section:sec-span]{Section~{\xreffont\ref{x:section:sec-span}}} that the span of a set of vectors \(\vv_1, \vv_2,\ldots, \vv_k\) in a vector space \(V\) is the set of all linear combinations that can be generated from those vectors.%
\par
Therefore, the question ``Does the vector \(\ww\) belong to the span of \(\vv_1, \vv_2,\ldots, \vv_k\)?'' is equivalent to asking, ``Can I write \(\ww\) as a linear combination of the \(\vv_i\)?'', which, in turn, is equivalent to asking:%
\par
Do there exist scalars \(c_1,c_2,\ldots, c_k\) such that%
\begin{equation*}
\ww=c_1\vv_1+c_2\vv_2+\cdots +c_k\vv_k\text{?}
\end{equation*}
%
\par
In any finite-dimensional vector space, this last question can be turned into a system of equations. If that system has a solution, then yes \textemdash{} your vector is in the span. If the system is inconsistent, then the answer is no.%
\end{introduction}%
\begin{divisionexercise}{1}{}{1.5in}{g:exercise:idp24}%
Determine whether or not the vector \(\ww=\langle 3,-1, 4, 2\rangle\) in \(\R^4\) belongs to the span of the vectors%
\begin{equation*}
\langle 2, 1, 4, -3\rangle, \langle 0, 2, 1, 4\rangle, \langle -1, 1, 0, 2\rangle\text{.}
\end{equation*}
%
\end{divisionexercise}%
To assist with solving this problem, a code cell is provided below. Once you have determined the augmented matrix of your system of equations, see \hyperref[x:section:sec-sympy]{Section~{\xreffont\ref{x:section:sec-sympy}}} for details on how to enter your matrix, and then compute its reduced row-echelon form.%
\begin{sageinput}
from sympy import Matrix, init_printing
init_printing()
\end{sageinput}
\begin{divisionexercise}{2}{}{1.5in}{g:exercise:idp25}%
Determine whether or not the polynomial \(q(x) = 4-6x-11x^2\) belongs to the span of the polynomials%
\begin{equation*}
p_1(x) = x-3x^2, p_2(x)=2-x, p_3(x) = -1+4x+x^2\text{.}
\end{equation*}
%
\end{divisionexercise}%
%
\clearpage
For our next activity, we are going to look at \initialism{RGB} colours. Here, \initialism{RGB} stands for Red, Green, Blue. All colours displayed by your computer monitor can be expressed in terms of these colours.%
\par
First, we load some Python libraries we'll need. These are intended for use in a Jupyter notebook and won't run properly if you are using Sagecell in the \initialism{HTML} textbook.%
\begin{sageinput}
import ipywidgets as wid
import matplotlib.pyplot as plt
\end{sageinput}
Next, we will create a widget that lets us select values for red, green, and blue. The \initialism{RGB} colour system assigns 8-bit values to each colour. Possible values for each range from 0 to 255; this indicates how much of each colour will be blended to create the colour you want. Extensive information on the \initialism{RGB} colour system can be found \href{https://en.wikipedia.org/wiki/RGB_color_model}{on wikipedia}\footnote{\nolinkurl{en.wikipedia.org/wiki/RGB_color_model}\label{g:fn:idp26}}, and there are a number of good online resources about the use of \initialism{RGB} in web design, \href{https://www.w3schools.com/colors/colors_rgb.asp}{such as this one from w3schools}\footnote{\nolinkurl{www.w3schools.com/colors/colors_rgb.asp}\label{g:fn:idp27}}.%
\begin{sageinput}
r=wid.IntSlider(
    value=155,
    min=0,
    max=255,
    step=1,
    description='Red:'
)
g=wid.IntSlider(
    value=155,
    min=0,
    max=255,
    step=1,
    description='Green:'
)
b=wid.IntSlider(
    value=155,
    min=0,
    max=255,
    step=1,
    description='Blue:'
)
display(r,g,b)
\end{sageinput}
By moving the sliders generated above, you can create different colours. To see what colour you've created by moving the sliders, run the code below.%
\begin{sageinput}
plt.imshow([[(r.value/255, g.value/255, b.value/255)]])
\end{sageinput}
\begin{divisionexercise}{3}{}{0.65in}{g:exercise:idp28}%
In what ways can you explain the \initialism{RGB} colour system in terms of span?%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{0.65in}{g:exercise:idp29}%
Why would it nonetheless be inappropriate to describe the set of all \initialism{RGB} colours as a vector space?%
\end{divisionexercise}%
\end{worksheet-section}
\restoregeometry
%
%
\typeout{************************************************}
\typeout{Section 1.5 Linear Independence}
\typeout{************************************************}
%
\begin{sectionptx}{Linear Independence}{}{Linear Independence}{}{}{x:section:sec-independence}
In any vector space \(V\), we say that a set of vectors%
\begin{equation*}
\{\vv_1,\ldots,\vv_k\}
\end{equation*}
is \terminology{linearly independent} if for any scalars \(c_1,\ldots, c_k\)%
\begin{equation*}
c_1\vv_1+\cdots + c_k\vv_k = \mathbf{0} \quad\Rightarrow\quad c_1=\cdots = c_k=0\text{.}
\end{equation*}
%
\par
This means that no vector in the set can be written as a linear combination of the other vectors in that set. We will soon see that when looking for vectors that span a subspace, it is especially useful to find a spanning set that is also linearly independent. The following lemma establishes some basic properties of independent sets.%
\begin{lemma}{}{}{g:lemma:idp30}%
In any vector space \(V\):%
\begin{enumerate}
\item{}If \(\vv\neq\mathbf{0}\), then \(\{\vv\}\) is independent.%
\item{}If \(S\subseteq V\) contains the zero vector, then \(S\) is dependent.%
\end{enumerate}
%
\end{lemma}
The definition of linear independence tells us that if \(\{\vv_1,\ldots, \vv_k\}\) is an independent set of vectors, then there is only one way to write \(\mathbf{0}\) as a linear combination of these vectors; namely,%
\begin{equation*}
\mathbf{0} = 0\vv_1+0\vv_2+\cdots +0\vv_k\text{.}
\end{equation*}
In fact, more is true: \emph{any} vector in the span of a linearly independent set can be written in only one way as a linear combination of those vectors.%
\par
Computationally, questions about linear independence are just questions about homogeneous systems of linear equations. For example, suppose we want to know if the vectors%
\begin{equation*}
\uu=\bbm 1\\-1\\4\ebm, \vv=\bbm 0\\2\\-3\ebm, \ww=\bbm 4\\0\\-3\ebm
\end{equation*}
are linearly independent in \(\mathbb{R}^3\). This question leads to the vector equation%
\begin{equation*}
x\uu+y\vv+z\ww=\mathbf{0}\text{,}
\end{equation*}
which becomes the matrix equation%
\begin{equation*}
\bbm 1\amp0\amp4\\-1\amp2\amp0\\4\amp-3\amp-3\ebm\bbm x\\y\\z\ebm = \bbm 0\\0\\0\ebm\text{.}
\end{equation*}
%
\par
We now apply some basic theory from linear algebra. A unique (and therefore, trivial) solution to this system is guaranteed if the matrix \(A = \bbm 1\amp0\amp4\\-1\amp2\amp0\\4\amp-3\amp-3\ebm\) is invertible, since in that case we have \(\bbm x\\y\\z\ebm = A^{-1}\mathbf{0} = \mathbf{0}\).%
\par
This approach is problematic, however, since it won't work if we have 2 vectors, or 4. Instead, we look at the reduced row-echelon form. A unique solution corresponds to having a leading 1 in each column of \(A\). Let's check this condition.%
\begin{sageinput}
from sympy import Matrix,init_printing
init_printing()
A = Matrix(3,3,[1,0,4,-1,2,0,4,-3,-3])
A.rref()
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp 1\ebm, (0,1,2)\]
\end{sageoutput}
One observation is useful here, and will lead to a better understanding of independence. First, it would be impossible to have 4 or more linearly independent vectors in \(\mathbb{R}^3\). Why? (How many leading ones can you have in a \(3\times 4\) matrix?) Second, having two or fewer vectors makes it more likely that the set is independent.%
\par
The largest set of linearly independent vectors possible in \(\mathbb{R}^3\) contains three vectors. You might have also observed that the smallest number of vectors needed to span \(\mathbb{R}^3\) is 3. Hmm. Seems like there's something interesting going on here. But first, some more computation.%
\begin{inlineexercise}{}{g:exercise:idp31}%
Determine whether the set \(\left\{\bbm 1\\2\\0\ebm, \bbm -1\\0\\3\ebm,\bbm -1\\4\\9\ebm\right\}\) is linearly independent in \(\R^3\).%
\end{inlineexercise}%
\begin{inlineexercise}{}{g:exercise:idp32}%
Which of the following subsets of \(P_2(\mathbb{R})\) are independent?%
\begin{gather*}
\text{(a) } S_1 = \{x^2+1, x+1, x\}\\
\text{(b) } S_2 = \{x^2-x+3, 2x^2+x+5, x^2+5x+1\}
\end{gather*}
%
\end{inlineexercise}%
\begin{inlineexercise}{}{g:exercise:idp33}%
Determine whether or not the set%
\begin{equation*}
\left\{\bbm -1\amp 0\\0\amp -1\ebm, \bbm 1\amp -1\\ -1\amp 1\ebm,
\bbm 1\amp 1\\1\amp 1\ebm, \bbm 0\amp -1\\-1\amp 0\ebm\right\}
\end{equation*}
is linearly independent in \(M_2(\mathbb{R})\).%
\end{inlineexercise}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 1.6 Basis and dimension}
\typeout{************************************************}
%
\begin{sectionptx}{Basis and dimension}{}{Basis and dimension}{}{}{x:section:sec-dimension}
Next, we begin with an important result, sometimes known as the ``Fundamental Theorem'':%
\begin{theorem}{Fundamental Theorem (Steinitz Exchange Lemma).}{}{x:theorem:theorem-steinitz}%
Suppose \(V = \spn\{\vv_1,\ldots, \vv_n\}\). If \(\{\ww_1,\ldots, \ww_m\}\) is a linearly independent set of vectors in \(V\), then \(m\leq n\).%
\end{theorem}
If a set of vectors spans a vector space \(V\), and it is not independent, we observed that it is possible to remove a vector from the set and still span \(V\). This suggests that spanning sets that are also linearly independent are of particular importance, and indeed, they are important enough to have a name.%
\begin{definition}{}{x:definition:def-basis}%
Let \(V\) be a vector space. A set \(\mathcal{B}=\{\mathbf{e}_1,\ldots, \mathbf{e}_n\}\) is called a \terminology{basis} of \(V\) if \(\mathcal{B}\) is linearly independent, and \(\operatorname{span}\mathcal{B} = V\).%
\end{definition}
The importance of a basis is that vector vector \(\vv\in V\) can be written in terms of the basis, and this expression as a linear combination of basis vectors is \emph{unique}. Another important fact is that every basis has the same number of elements.%
\begin{theorem}{Invariance Theorem.}{}{x:theorem:thm-invariance}%
If \(\{\mathbf{e}_1,\ldots, \mathbf{e}_n\}\) and \(\{\mathbf{f}_1,\ldots, \mathbf{f}_m\}\) are both bases of a vector space \(V\), then \(m=n\).%
\end{theorem}
Suppose \(V=\spn\{\vv_1,\ldots,\vv_n\}\). If this set is not linearly independent, \hyperref[x:theorem:theorem-surplus-span]{Theorem~{\xreffont\ref{x:theorem:theorem-surplus-span}}} tells us that we can remove a vector from the set, and still span \(V\). We can repeat this procedure until we have a linearly independent set of vectors, which will then be a basis. These results let us make a definition.%
\begin{definition}{}{x:definition:def-dimension}%
Let \(V\) be a vector space. If \(V\) can be spanned by a finite number of vectors, then we call \(V\) a \terminology{finite-dimensional} vector space. If \(V\) is finite-dimensional (and non-trivial), and \(\{\mathbf{e}_1,\ldots, \mathbf{e}_n\}\) is a basis of \(V\), we say that \(V\) has \terminology{dimension} \(n\), and write%
\begin{equation*}
\dim V = n\text{.}
\end{equation*}
If \(V\) cannot be spanned by finitely many vectors, we say that \(V\) is \terminology{infinite-dimensional}.%
\end{definition}
\begin{inlineexercise}{}{g:exercise:idp34}%
Find a basis for \(U=\{X\in M_{22} \,|\, XA = AX\}\), if \(A = \bbm 1\amp 1\\0\amp 0\ebm\)%
\end{inlineexercise}%
\begin{example}{Standard bases.}{g:example:idp35}%
Most of the vector spaces we work with come equipped with a \emph{standard basis}. The standard basis for a vector space is typically a basis such that the scalars needed to express a vector in terms of that basis are the same scalars used to define the vector in the first place. For example, we write an element of \(\R^3\) as \((x,y,z)\) (or \(\langle x,y,z\rangle\), or \(\begin{bmatrix}x\amp y\amp z\end{bmatrix}\), or \(\begin{bmatrix}x\\y\\z\end{bmatrix}\)\textellipsis{}). We can also write%
\begin{equation*}
(x,y,z)=x(1,0,0)+y(0,1,0)+z(0,0,1)\text{.}
\end{equation*}
The set \(\{(1,0,0),(0,1,0),(0,0,1)\}\) is the standard basis for \(\R^3\). In general, the vector space \(\R^n\) (written this time as column vectors) has standard basis%
\begin{equation*}
\vece_1=\bbm 1\\0\\ \vdots \\0\ebm, \vece_2 = \bbm 0\\1\\ \vdots \\0\ebm, \ldots, \vece_n=\bbm 0\\0\\ \vdots \\1\ebm\text{.}
\end{equation*}
From this, we can conclude (unsurprisingly) that \(\dim \R^n = n\).%
\par
Similarly, a polynomial \(p(x)\in P_n(\R)\) is usually written as%
\begin{equation*}
p(x) = a_0+a_1x+a_2x^2+\cdots + a_nx^n\text{,}
\end{equation*}
suggesting the standard basis \(\{1,x,x^2,\ldots, x^n\}\). As a result, we see that \(\dim P_n(\R)=n+1\).%
\par
For one more example, we note that a \(2\times 2\) matrix \(A\in M_{22}(\R)\) can be written as%
\begin{equation*}
\bbm a\amp b\\c\amp d\ebm = a\bbm 1\amp 0\\0\amp 0\ebm+b\bbm 0\amp 1\\0\amp 0\ebm +c\bbm 0\amp 0\\1\amp 0\ebm + d\bbm 0\amp 0\\0\amp 1\ebm\text{,}
\end{equation*}
which suggests a standard basis for \(M_{22}(\R)\), with similar results for any other matrix space. From this, we can conclude (exercise) that \(\dim M_{mn}(\R)=mn\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp36}{}\quad{}%
\end{example}
\begin{inlineexercise}{}{g:exercise:idp37}%
Show that the following sets are bases of \(\R^3\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item{}\(\{(1,1,0),(1,0,1),(0,1,1)\}\)%
\item{}\(\{(-1,1,1),(1,-1,1),(1,1,-1)\}\)%
\end{enumerate}
\end{inlineexercise}%
The next two exercises are left to the reader to solve. In each case, your goal should be to turn the questions of independence and span into a system of equations, which you can then solve using the computer.%
\begin{inlineexercise}{}{g:exercise:idp38}%
Show that the following is a basis of \(M_{22}\):%
\begin{equation*}
\left\{\bbm 1\amp 0\\0\amp 1\ebm, \bbm 0\amp 1\\1\amp 0\ebm, \bbm 1\amp 1\\0\amp 1\ebm, \bbm 1\amp 0\\0\amp 0\ebm\right\}\text{.}
\end{equation*}
%
\end{inlineexercise}%
\begin{inlineexercise}{}{g:exercise:idp39}%
Show that \(\{1+x,x+x^2,x^2+x^3,x^3\}\) is a basis for \(P_3\).%
\end{inlineexercise}%
\begin{inlineexercise}{}{g:exercise:idp40}%
Find a basis and dimension for the following subspaces of \(P_2\):%
\begin{enumerate}[label=\alph*]
\item{}\(\displaystyle U_1 = \{a(1+x)+b(x+x^2)\,|\, a,b\in\R\}\)%
\item{}\(\displaystyle U_2=\{p(x)\in P_2 \,|\, p(1)=0\}\)%
\item{}\(\displaystyle U_3 = \{p(x)\in P_2 \,|\, p(x)=p(-x)\}\)%
\end{enumerate}
%
\end{inlineexercise}%
We've noted a few times now that if \(\ww\in\spn\{\vv_1,\ldots, \vv_n\}\), then%
\begin{equation*}
\spn\{\ww,\vv_1,\ldots, \vv_n\}=\spn\{\vv_1,\ldots, \vv_n\}
\end{equation*}
If \(\ww\) is not in the span, we can make another useful observation:%
\begin{lemma}{Independent Lemma.}{}{x:lemma:lemma-independent}%
Suppose \(\{\vv_1,\ldots, \vv_n\}\) is a linearly independent set of vectors in a vector space \(V\). If \(\uu\in V\) but \(\uu\notin \spn\{\vv_1,\ldots, \vv_n\}\), then \(\{\uu,\vv_1,\ldots, \vv_n\}\) is independent.%
\end{lemma}
\begin{proof}{}{g:proof:idp41}
Suppose \(S=\{\vv_1,\ldots, \vv_n\}\) is independent, and that \(\uu\notin\spn S\). Suppose we have%
\begin{equation*}
a\uu+c_1\vv_1+c_2\vv_2+\cdots +c_n\mathbf{b}_n=\mathbf{0}
\end{equation*}
for scalars \(a,c_1,\ldots, c_n\). We must have \(a=0\); otherwise, we can multiply by \(\frac1a\) and rearrange to obtain%
\begin{equation*}
\uu = -\frac{c_1}{a}\vv_1-\cdots -\frac{c_n}{a}\vv_n\text{,}
\end{equation*}
but this would mean that \(\uu\in \spn S\), contradicting our assumption.%
\par
With \(a=0\) we're left with%
\begin{equation*}
c_1\vv_1+c_2\vv_2+\cdots +c_n\mathbf{b}_n=\mathbf{0}\text{,}
\end{equation*}
and since we assumed that the set \(S\) is independent, we must have \(c_1=c_2=\cdots=c_n=0\). Since we already showed \(a=0\), this shows that \(\{\uu,\vv_1,\ldots, \vv_n\}\) is independent.%
\end{proof}
This is, in fact, an ``if and only if'' result. If \(\uu\in\spn\{\vv_1,\ldots, \vv_n\}\), then \(\{\uu,\vv_1,\ldots, \vv_n\}\) is not independent. Above, we argued that if \(V\) is finite dimensional, then any spanning set for \(V\) can be reduced to a basis. It probably won't surprise you that the following is also true.%
\begin{lemma}{}{}{x:lemma:lem-enlarge-independent}%
Let \(V\) be a finite-dimensional vector space, and let \(U\) be any subspace of \(V\). Then any independent set of vectors \(\{\uu_1,\ldots, \uu_k\}\) in \(U\) can be enlarged to a basis of \(U\).%
\end{lemma}
\begin{proof}{}{g:proof:idp42}
This follows from \hyperref[x:lemma:lemma-independent]{Lemma~{\xreffont\ref{x:lemma:lemma-independent}}}. If our independent set of vectors spans \(U\), then it's a basis and we're done. If not, we can find some vector not in the span, and add it to our set to obtain a larger set that is still independent. We can continue adding vectors in this fashion until we obtain a spanning set.%
\par
Note that this process \emph{must} terminate: \(V\) is finite-dimensional, so there is a finite spanning set for \(V\), and therefore for \(U\). By the Steinitz Exchange lemma, our independent set cannot get larger than this spanning set.%
\end{proof}
\begin{theorem}{}{}{x:theorem:thm-basis-exist}%
Any finite-dimensional vector space \(V\) has a basis. Moreover:%
\begin{enumerate}
\item{}If \(V\) can be spanned by \(m\) vectors, then \(\dim V\leq m\).%
\item{}Given an independent set \(I\) in \(V\), and a basis \(\mathcal{B}\) of \(V\), we can enlarge \(I\) to a basis of \(V\) by adding elements of \(\mathcal{B}\).%
\end{enumerate}
%
\par
If \(U\) is a subspace of \(V\), then:%
\begin{enumerate}
\item{}\(U\) is finite-dimensional, and \(\dim U\leq \dim V\).%
\item{}If \(\dim U = \dim V\), then \(U=V\).%
\end{enumerate}
%
\end{theorem}
\begin{inlineexercise}{}{g:exercise:idp43}%
Find a basis of \(M_{22}(\R)\) that contains the vectors%
\begin{equation*}
\vv=\bbm 1\amp 1\\0\amp 0\ebm, \ww=\bbm 0\amp 1\\0\amp 1\ebm\text{.}
\end{equation*}
%
\end{inlineexercise}%
\begin{inlineexercise}{}{g:exercise:idp44}%
Extend the set \(\{1+x,x+x^2,x-x^3\}\) to a basis of \(P_3(\R)\).%
\end{inlineexercise}%
\begin{inlineexercise}{}{g:exercise:idp45}%
Give two examples of infinite-dimensional vector spaces. Support your answer.%
\end{inlineexercise}%
Let's recap our results so far:%
\begin{itemize}[label=\textbullet]
\item{}A basis for a vector space \(V\) is an independent set of vectors that spans \(V\).%
\item{}The number of vectors in any basis of \(V\) is a constant, called the dimension of \(V\).%
\item{}The number of vectors in any independent set is always less than or equal to the number of vectors in a spanning set.%
\item{}In a finite-dimensional vector space, any independent set can be enlarged to a basis, and any spanning set can be cut down to a basis by deleting vectors that are in the span of the remaining vectors.%
\end{itemize}
Another important aspect of dimension is that it reduces many problems, such as determining equality of subspaces, to counting problems.%
\begin{theorem}{}{}{x:theorem:thm-subspace-dim}%
Let \(U\) and \(W\) be subspaces of a finite-dimensional vector space \(V\).%
\begin{enumerate}
\item{}If \(U\subseteq W\), then \(\dim U\leq \dim W\).%
\item{}If \(U\subseteq W\) and \(\dim U=\dim W\), then \(U=W\).%
\end{enumerate}
%
\end{theorem}
\begin{proof}{}{g:proof:idp46}
%
\begin{enumerate}
\item{}Suppose \(U\subseteq W\), and let \(B=\{\uu_1,\ldots, \uu_k\}\) be a basis for \(U\). Since \(B\) is a basis, it's independent. And since \(B\subseteq U\) and \(U\subseteq W\), \(B\subseteq W\). Thus, \(B\) is an independent subset of \(W\), and since any basis of \(W\) spans \(W\), we know that \(\dim U = k \leq \dim W\), by \hyperref[x:theorem:theorem-steinitz]{Theorem~{\xreffont\ref{x:theorem:theorem-steinitz}}}.%
\item{}Suppose \(U\subseteq W\) and \(\dim U = \dim W\). Let \(B\) be a basis for \(U\). As above, \(B\) is an independent subset of \(W\). If \(W\neq U\), then there is some \(\ww\in W\) with \(\ww\notin U\). But \(U=\spn B\), so that would mean that \(B\cup \{\ww\}\) is a linearly independent set containing \(\dim U+1\) vectors. This is impossible, since \(\dim W=\dim U\), so no independent set can contain more than \(\dim U\) vectors.%
\end{enumerate}
%
\end{proof}
An even more useful counting result is the following:%
\begin{theorem}{}{}{x:theorem:thm-half-the-work}%
Let \(V\) be an \(n\)-dimensional vector space. If the set \(S\) contains \(n\) vectors, then \(S\) is independent if and only if \(\spn S=V\).%
\end{theorem}
\begin{proof}{}{g:proof:idp47}
If \(S\) is independent, then it can be extended to a basis \(B\) with \(S\subseteq B\). But \(S\) and \(B\) both contain \(n\) vectors (since \(\dim V=n\)), so we must have \(S=B\).%
\par
If \(S\) spans \(V\), then \(S\) must contain a basis \(B\), and as above, since \(S\) and \(B\) contain the same number of vectors, they must be equal.%
\end{proof}
\begin{paragraphs}{New subspaces from old.}{x:paragraphs:pars-subspace-combine}%
On your first assignment, you showed that if \(U\) and \(W\) are subspaces of a vector space \(V\), then the intersection \(U\cap W\) is also a subspace of \(V\). You also showed that the union \(U\cup W\) is generally not a subspace, unless one subspace is contained in the other (in which case the union is just the larger of the two subspaces we already have).%
\par
In class, we discussed the fact that the right way to define a subspace containing both \(U\) and \(W\) is using their \terminology{sum}: we define the sum \(U+W\) of two subspaces by%
\begin{equation*}
U+W = \{\uu+\ww \,|\, \uu\in U \text{ and } \ww\in W\}\text{.}
\end{equation*}
We proved that \(U+W\) is again a subspace of \(V\).%
\par
If \(U\cap W = \{\mathbf{0}\}\), we say that the sum is a \terminology{direct sum}, and write it as \(U\oplus W\). The following theorem might help us understand why direct sums are singled out for special attention:%
\begin{theorem}{}{}{x:theorem:thm-sum-dimension}%
Let \(U\) and \(W\) be subspaces of a finite-dimensional vector space \(V\). Then \(U+W\) is finite-dimensional, and%
\begin{equation*}
\dim(U+W)=\dim U + \dim W - \dim(U\cap W)\text{.}
\end{equation*}
%
\end{theorem}
If the sum is direct, then we have simply \(\dim(U\oplus W) = \dim U + \dim W\). The other reason why direct sums are preferable, is that any \(\vv\in U\oplus W\) can be written \emph{uniquely} as \(\vv=\uu+\ww\) where \(\uu\in U\) and \(\ww\in W\).%
\begin{theorem}{}{}{x:theorem:thm-direct-sum}%
For any subspaces \(U,W\) of a vector space \(V\), \(U\cap W = \{\mathbf{0}\}\) if and only if for every \(\vv\in U+W\) there exist unique \(\uu\in U, \ww\in W\) such that \(\vv=\uu+\ww\).%
\end{theorem}
\begin{proof}{}{g:proof:idp48}
Suppose that \(U\cap W = \{\mathbf{0}\}\), and suppose that we have \(\vv = \uu_1+\ww_1 = \uu_2+\ww_2\), for \(\uu_1,\uu_2\in U,\ww_1,\ww_2\in W\). Then \(\mathbf{0}=(\uu_1-\uu_2)+(\ww_1-\ww_2)\), which implies that%
\begin{equation*}
\ww_1-\ww_2 = -(\uu_1-\uu_2)\text{.}
\end{equation*}
Now, \(\uu=\uu_1-\uu_2\in U\), since \(U\) is a subspace, and similarly, \(\ww=\ww_1-\ww_2\in W\). But we also have \(\ww=-\uu\), which implies that \(\ww\in U\). Therefore, \(\ww\in U\cap W\), which implies that \(\ww=\mathbf{0}\), so \(\ww_1=\ww_2\). But we must also then have \(\uu=\mathbf{0}\), so \(\uu_1=\uu_2\).%
\par
Conversely, suppose that every \(\vv\in U+W\) can be written uniquely as \(\vv=\vec{u}+\ww\), with \(\uu\in U\) and \(\ww\in W\). Suppose that \(\mathbf{a}\in U\cap W\). Then \(\mathbf{a}\in U\) and \(\mathbf{a}\in W\), so we also have \(-\mathbf{a}\in W\), since \(W\) is a subspace. But then \(\mathbf{0}=\mathbf{a}+(-\mathbf{a})\), where \(\mathbf{a}\in U\) and \(-\mathbf{a}\in W\). On the other hand, \(\mathbf{0}=\mathbf{0}+\mathbf{0}\), and \(\mathbf{0}\) belongs to both \(U\) and \(W\). It follows that \(\mathbf{a}=\mathbf{0}\). Since \(\mathbf{a}\) was arbitrary, \(U\cap W = \{\mathbf{0}\}\).%
\end{proof}
We end with one last application of the theory we've developed on the existence of a basis for a finite-dimensional vector space. As we continue on to later topics, we'll find that it is often useful to be able to decompose a vector space into a direct sum of subspaces. Using bases, we can show that this is always possible.%
\begin{theorem}{}{}{x:theorem:thm-construct-complement}%
Let \(V\) be a finite-dimensional vector space, and let \(U\) be any subspace of \(V\). Then there exists a subspace \(W\subseteq V\) such that \(U\oplus W = V\).%
\end{theorem}
\begin{proof}{}{g:proof:idp49}
Let \(\{\uu_1,\ldots, \uu_m\}\) be a basis of \(U\). Since \(U\subseteq V\), the set \(\{\uu_1,\ldots, \uu_m\}\) is a linearly independent subset of \(V\). Since any linearly independent set can be extended to a basis of \(V\), there exist vectors \(\ww_1,\ldots,\ww_n\) such that%
\begin{equation*}
\{\uu_1,\ldots, \uu_m,\ww_1,\ldots, \ww_n\}
\end{equation*}
is a basis of \(V\).%
\par
Now, let \(W = \spn\{\ww_1,\ldots, \ww_n\}\). Then \(W\) is a subspace, and \(\{\ww_1,\ldots, \ww_n\}\) is a basis for \(W\). (It spans, and must be independent since it's a subset of an independent set.)%
\par
Clearly, \(U+W=V\), since \(U+W\) contains the basis for \(V\) we've constructed. To show the sum is direct, it suffices to show that \(U\cap W = \{\mathbf{0}\}\). To that end, suppose that \(\vv\in U\cap W\). Since \(\vv\in U\), we have%
\begin{equation*}
\vv=a_1\uu_1+\cdots +a_m\uu_m
\end{equation*}
for scalars \(a_1,\ldots, a_m\). Since \(\vv\in W\), we can write%
\begin{equation*}
\vv=b_1\ww_1+\cdots + b_n\ww_n
\end{equation*}
for scalars \(b_1,\ldots, b_n\). But then%
\begin{equation*}
\mathbf{0}=\vv-\vv=a_1\uu_1+\cdots a_m\uu_m-b_1\ww_1-\cdots -b_n\ww_n.
\end{equation*}
Since \(\{\uu_1,\ldots, \uu_m,\ww_1,\ldots, \ww_n\}\) is a basis for \(V\), it's independent, and therefore, all of the \(a_i,b_j\) must be zero, and therefore, \(\vv=\mathbf{0}\).%
\end{proof}
The subspace \(W\) constructed in the theorem above is called a \terminology{complement} of \(U\). It is not unique; indeed, it depends on the choice of basis vectors. For example, if \(U\) is a one-dimensional subspace of \(\R^2\); that is, a line, then any other non-parallel line through the origin provides a complement of \(U\). Later we will see that an especially useful choice of complement is the \terminology{orthogonal complement}.%
\end{paragraphs}%
\end{sectionptx}
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 2 Linear Transformations}
\typeout{************************************************}
%
\begin{chapterptx}{Linear Transformations}{}{Linear Transformations}{}{}{x:chapter:ch-linear-trans}
\begin{introduction}{}%
At an elementary level, Linear Algebra is the study of vectors (in \(\R^n\)) and matrices. Of course, much of that study revolves around systems of equations. Recall that if \(\xx\) is a vector in \(\R^n\) (viewed as an \(n\times 1\) column matrix), and \(A\) is an \(m\times n\) matrix, then \(\yy=A\xx\) is a vector in \(\R^m\). Thus, multiplication by \(A\) produces a function from \(\R^n\) to \(\R^m\).%
\par
This example motivates the definition of a \emph{linear transformation}, and as we'll see, provides the archetype for all linear transformations in the finite-dimensional setting. Many areas of mathematics can be viewed at some fundamental level as the study of sets with certain properties, and the functions between them. Linear algebra is no different. The sets in this context are, of course, vector spaces. Since we care about the linear algebraic structure of vector spaces, it should come as no surprise that we're most interested in functions that preserve this structure. That is precisely the idea behind linear transformations.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Section 2.1 Definition and examples}
\typeout{************************************************}
%
\begin{sectionptx}{Definition and examples}{}{Definition and examples}{}{}{x:section:sec-lin-tran-intro}
Let \(V\) and \(W\) be vector spaces. At their most basic, all vector spaces are sets. Given any two sets, we can consider functions from one to the other. The functions of interest in linear algebra are those that respect the vector space structure of the sets.%
\begin{definition}{}{x:definition:def-lin-trans}%
Let \(V\) and \(W\) be vector spaces. A function \(T:V\to W\) is called a \terminology{linear transformation} if:%
\begin{enumerate}
\item{}For all \(\vv_1,\vv_2\in V\), \(T(\vv_1+\vv_2)=T(\vv_1)+T(\vv_2)\).%
\item{}For all \(\vv\in V\) and scalars \(c\), \(T(c\vv)=cT(\vv)\).%
\end{enumerate}
We often use the term \terminology{linear operator} to refer to a linear transformation \(T:V\to V\) from a vector space to itself.%
\end{definition}
Note on notation: it is common usage to drop the usual parentheses of function notation when working with linear transformations, as long as this does not cause confusion. That is, one might write \(T\vv\) instead of \(T(\vv)\), but one should never write \(T\vv+\ww\) in place of \(T(\vv+\ww)\), for the same reason that one should never write \(2x+y\) in place of \(2(x+y)\). Mathematicians often think of linear transformations in terms of matrix multiplication, which probably explains this notation to some extent.%
\par
The properties of a linear transformation tell us that a linear map \(T\) \emph{preserves} the operations of addition and scalar multiplication. (When the domain and codomain are different vector spaces, we might say that \(T\) \emph{intertwines} the operations of the two vector spaces.) In particular, any linear transformation \(T\) must preserve the zero vector, and respect linear combinations.%
\begin{theorem}{}{}{x:theorem:thm-lt-props}%
Let \(T:V\to W\) be a linear transformation. Then%
\begin{enumerate}
\item{}\(T(\mathbf{0}_V) = \mathbf{0}_W\), and%
\item{}For any scalars \(c_1,\ldots, c_n\) and vectors \(\vv_1,\ldots, \vv_n\in V\),%
\begin{equation*}
T(c_1\vv_1+c_2\vv_2+\cdots + c_n\vv_n) = c_1T(\vv_1)+c_2T(\vv_2)+\cdots + c_nT(\vv_n)\text{.}
\end{equation*}
%
\end{enumerate}
%
\end{theorem}
\begin{proof}{}{g:proof:idp50}
%
\begin{enumerate}
\item{}Since \(\mathbf{0}_V+\mathbf{0}_V = \mathbf{0}_V\), we have%
\begin{equation*}
T(\mathbf{0}_V) = T(\mathbf{0}_V+\mathbf{0}_V) = T(\mathbf{0}_V)+T(\mathbf{0}_V)\text{.}
\end{equation*}
Adding \(-T(\mathbf{0}_V)\) to both sides of the above gives us \(\mathbf{0}_W = T(\mathbf{0}_V)\).%
\item{}The addition property of a linear transformation can be extended to sums of three or more vectors using associativity. Therefore, we have%
\begin{align*}
T(c_1\vv_1+\cdots + c_n\vv_n) \amp = T(c_1\vv_1)+ \cdots T(c_n\vv_n)\\
\amp = c_1T(\vv_1)+\cdots +c_nT(\vv_n)\text{,}
\end{align*}
where the second line follows from the scalar multiplication property.%
\end{enumerate}
%
\end{proof}
\begin{example}{}{x:example:ex-matrix-trans}%
Let \(V=\R^n\) and let \(W=\R^m\). For any \(m\times n\) matrix \(A\), the map \(T_A:\R^n\to \R^m\) defined by%
\begin{equation*}
T_A(\xx) = A\xx
\end{equation*}
is a linear transformation. (This follows immediately from properties of matrix multiplication.)%
\par
Let \(B = \{\mathbf{e}_1,\ldots, \mathbf{e}_n\}\) denote the standard basis of \(\R^n\). Recall that \(A\mathbf{e}_i\) is equal to the \(i\)th column of \(A\). Thus, if we know the value of a linear transformation \(T:\R^n\to \R^m\) on each basis vector, we can immediately determine the matrix \(A\) such that \(T=T_A\):%
\begin{equation*}
A = \bbm T(\mathbf{e}_1) \amp T(\mathbf{e}_2) \amp \cdots \amp T(\mathbf{e}_n)\ebm\text{.}
\end{equation*}
This is true because \(T\) and \(T_A\) agree on the standard basis: for each \(i=1,2,\ldots, n\),%
\begin{equation*}
T_A(\mathbf{e}_i) = A\mathbf{e}_i = T(\mathbf{e}_i)\text{.}
\end{equation*}
Moreover, if two linear transformations agree on a basis, they must be equal. Given any \(\xx\in \R^n\), we can write \(\xx\) uniquely as a linear combination%
\begin{equation*}
\xx=c_1\mathbf{e}_1+c_2\mathbf{e}_2+\cdots + c_n\mathbf{e}_n.
\end{equation*}
If \(T(\mathbf{e}_i)=T_A(\mathbf{e}_i)\) for each \(i\), then by \hyperref[x:theorem:thm-lt-props]{Theorem~{\xreffont\ref{x:theorem:thm-lt-props}}} we have%
\begin{align*}
T(\xx) \amp = T(c_1\mathbf{e}_1+c_2\mathbf{e}_2+\cdots + c_n\mathbf{e}_n) \\
\amp = c_1T(\mathbf{e}_1)+c_2T(\mathbf{e}_2)+\cdots + c_nT(\mathbf{e}_n)\\
\amp = c_1T_A(\mathbf{e}_1)+c_2T_A(\mathbf{e}_2)+\cdots + c_nT_A(\mathbf{e}_n)\\
\amp = T_A(c_1\mathbf{e}_1+c_2\mathbf{e}_2+\cdots + c_n\mathbf{e}_n) \\
\amp = T_A(\xx)\text{.}
\end{align*}
%
\end{example}
Let's look at some other examples of linear transformations.%
\begin{itemize}[label=\textbullet]
\item{}For any vector spaces \(V,W\) we can define the \terminology{zero transformation} \(0:V\to W\) by \(0(\vv)=\mathbf{0}\) for all \(\vv\in V\).%
\item{}On any vector space \(V\) we have the \terminology{identity transformation} \(1_V:V\to V\) defined by \(1_V(\vv)=\vv\) for all \(\vv\in V\).%
\item{}Let \(V = F[a,b]\) be the space of all functions \(f:[a,b]\to \R\). For any \(c\in [a,b]\) we have the \terminology{evaluation map} \(E_a: V\to \R\) defined by \(E_a(f) = f(a)\).%
\par
To see that this is linear, note that \(E_a(0)=\mathbf{0}(a)=0\), where \(\mathbf{0}\) denotes the zero function; for any \(f,g\in V\),%
\begin{equation*}
E_a(f+g)=(f+g)(a)=f(a)+g(a)=E_a(f)+E_a(g)\text{,}
\end{equation*}
and for any scalar \(c\in \R\),%
\begin{equation*}
E_a(cf) = (cf)(a) = c(f(a))=cE_a(f)\text{.}
\end{equation*}
%
\par
Note that the evaluation map can similarly be defined as a linear transformation on any vector space of polynomials.%
\item{}On the vector space \(C[a,b]\) of all \emph{continuous} functions on \([a,b]\), we have the integration map \(I:C[a,b]\to \R\) defined by \(I(f)=\int_a^b f(x)\,dx\). The fact that this is a linear map follows from properties of integrals proved in a calculus class.%
\item{}On the vector space \(C^1(a,b)\) of continuously differentiable functions on \((a,b)\), we have the differentiation map \(D: C^1(a,b)\to C(a,b)\) defined by \(D(f) = f'\). Again, linearity follows from properties of the derivative.%
\item{}Let \(\R^\infty\) denote the set of sequences \((a_1,a_2,a_3,\ldots)\) of real numbers, with term-by-term addition and scalar multiplication. The shift operators%
\begin{align*}
S_L(a_1,a_2,a_3,\ldots)  \amp = (a_2,a_3,a_4,\ldots) \\
S_R(a_1,a_2,a_3,\ldots) \amp = (0,a_1,a_2,\ldots)
\end{align*}
are both linear.%
\item{}On the space \(M_{mn}(\R)\) of \(m\times n\) matrices, the trace defines a linear map \(\operatorname{tr}:M_{mn}(\R)\to \R\), and the transpose defines a linear map \(T:M_{mn}(\R)\to M_{nm}(\R)\). The determinant and inverse operations on \(M_{nn}\) are \emph{not} linear.%
\end{itemize}
%
\par
For finite-dimensional vector spaces, it is often convenient to work in terms of a basis. The properties of a linear transformation tell us that we can completely define any linear transformation by giving its values on a basis. In fact, it's enough to know the value of a transformation on a spanning set. The argument given in \hyperref[x:example:ex-matrix-trans]{Example~{\xreffont\ref{x:example:ex-matrix-trans}}} can be applied to any linear transformation, to obtain the following result.%
\begin{theorem}{}{}{x:theorem:thm-agree-span}%
Let \(T:V\to W\) and \(S:V\to W\) be two linear transformations. If \(V = \spn\{\vv_1,\ldots, \vv_n\}\) and \(T(\vv_i)=S(\vv_i)\) for each \(i=1,2,\ldots, n\), then \(T=S\).%
\end{theorem}
\alert{Caution}: If the above spanning set is not also independent, then we can't just define the values \(T(\vv_i)\) however we want. For example, suppose we want to define \(T:\R^2\to\R^2\), and we set \(\R^2=\spn{(1,2),(4,-1),(5,1)}\). If \(T(1,2)=(3,4)\) and \(T(4,-1)=(-2,2)\), then we \emph{must} have \(T(5,1)=(1,6)\). Why? Because \((5,1)=(1,2)+(4,1)\), and if \(T\) is to be linear, then we have to have \(T((1,2)+(4,-1))=T(1,2)+T(4,-1)\).%
\par
If for some reason we already know that our transformation is linear, we might still be concerned about the fact that if a spanning set is not independent, there will be more than one way to express a vector as linear combination of vectors in that set. If we define \(T\) by giving its values on a spanning set, will it be well-defined? (That is, could we get two different values for \(T(\vv)\) by expressing \(\vv\) in terms of the spanning set in two different ways?) Suppose that we have scalars \(a_1,\ldots, a_n, b_1,\ldots, b_n\) such that%
\begin{align*}
\vv \amp = a_1\vv_1+\cdots + a_n\vv_n \quad \text{ and }\\
\vv \amp = b_1\vv_1+\cdots + b_n\vv_n
\end{align*}
We then have%
\begin{align*}
a_1T(\vv_1)+\cdots + a_nT(\vv_n) \amp =T(a_1\vv_1+\cdots + a_n\vv_n) \\
\amp =T(b_1\vv_1+\cdots +b_n\vv_n)\\
\amp =b_1T(\vv_1)+\cdots +b_nT(\vv_n)\text{.}
\end{align*}
%
\par
Of course, we can avoid all of this unpleasantness by using a \emph{basis} to define a transformation. Given a basis \(B = \{\vv_1,\ldots, \vv_n\}\) for a vector space \(V\), we can define a transformation \(T:V\to W\) by setting \(T(\vv_i)=\ww_i\) for some choice of vectors \(\ww_1,\ldots, \ww_n\) and defining%
\begin{equation*}
T(c_1\vv_1+\cdots +c_n\vv_n)=c_1T(\vv_1)+\cdots + c_nT(\vv_n)\text{.}
\end{equation*}
Because each vector \(\vv\in V\) can be written \emph{uniquely} in terms of a basis, we know that our transformation is well-defined.%
\par
The next theorem seems like an obvious consequence of the above, and indeed, one might wonder where the assumption of a basis is needed. The distinction here is that the vectors \(\ww_1,\ldots, \ww_n\in W\) are chosen in advance, and then we set \(T(vec{b}_i)=\ww_i\), rather than simply defining each \(\ww_i\) as \(T(\mathbf{b}_i)\).%
\begin{theorem}{}{}{x:theorem:thm-define-using-basis}%
Let \(V,W\) be vector spaces. Let \(B=\{\mathbf{b}_1,\ldots, \mathbf{b}_n\}\) be a basis of \(V\), and let \(\ww_1,\ldots, \ww_n\) be any vectors in \(W\). (These vectors need not be distinct.) Then there exists a unique linear transformation \(T:V\to W\) such that \(T(\mathbf{b}_i)=\ww_i\) for each \(i=1,2,\ldots, n\); indeed, we can define \(T\) as follows: given \(\vv\in V\), write \(\vv=c_1\vv_1+\cdots +c_n\vv_n\). Then%
\begin{equation*}
T(\vv)=T(c_1\vv_1+\cdots + c_n\vv_n) = c_1\ww_1+\cdots +c_n\ww_n\text{.}
\end{equation*}
%
\end{theorem}
With the basic theory out of the way, let's look at a few basic examples.%
\begin{inlineexercise}{}{g:exercise:idp51}%
Suppose \(T:\R^2\to \R^2\) is a linear transformation. If \(T\bbm 1\\0\ebm = \bbm 3\\-4\ebm\) and \(T\bbm 0\\1\ebm =\bbm 5\\2\ebm\), find \(T=\bbm -2\\4\ebm\).%
\end{inlineexercise}%
\begin{inlineexercise}{}{g:exercise:idp52}%
Suppose \(T:\R^2\to \R^2\) is a linear transformation. Given that \(T\bbm 3\\1\ebm = \bbm 1\\4\ebm\) and \(T\bbm 2\\-5\ebm = \bbm 2\\-1\ebm\), find \(T\bbm 4\\3\ebm\).%
\end{inlineexercise}%
\begin{inlineexercise}{}{g:exercise:idp53}%
Suppose \(T:P_2(\R)\to \R\) is defined by%
\begin{equation*}
T(x+2)=1, T(1)=5, T(x^2+x)=0.
\end{equation*}
Find \(T(2-x+3x^2)\).%
\end{inlineexercise}%
\begin{inlineexercise}{}{g:exercise:idp54}%
Find a linear transformation \(T:\R^2\to \R^3\) such that%
\begin{equation*}
T(1,2)=(1,1,0) \quad \text{ and } \quad T(-1,1) = (0,2,-1)\text{.}
\end{equation*}
Then, determine the value of \(T(3,2)\).%
\end{inlineexercise}%
\begin{inlineexercise}{}{x:exercise:ex_lintrans-indep}%
Let \(T:V\to W\) be a linear transformation. Prove that for any vectors \(\vv_1,\ldots, \vv_n\in V\), if \(\{T(\vv_1),\ldots, T(\vv_n)\}\) is linearly independent in \(W\), then \(\{\vv_1,\ldots, \vv_n\}\) is linearly independent in \(V\).%
\end{inlineexercise}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 2.2 Kernel and Image}
\typeout{************************************************}
%
\begin{sectionptx}{Kernel and Image}{}{Kernel and Image}{}{}{x:section:sec-kernel-image}
Given any linear transformation \(T:V\to W\) we can associate two important subspaces: the \terminology{kernel} of \(T\) (also known as the \terminology{nullspace}), and the \terminology{image} of \(T\) (also known as the \terminology{range}).%
\begin{definition}{}{x:definition:def-kernel-image}%
Let \(T:V\to W\) be a linear transformation. The \terminology{kernel} of \(T\), denoted \(\ker T\), is defined by%
\begin{equation*}
\ker T = \{\vv\in V \,|\, T(\vv)=\mathbf{0}\}\text{.}
\end{equation*}
The \terminology{image} of \(T\), denoted \(\im T\), is defined by%
\begin{equation*}
\im T = \{T(\vv) \,|\, \vv\in V\}\text{.}
\end{equation*}
%
\end{definition}
Note that the kernel of \(T\) is just the set of all vectors \(T\) sends to zero. The image of \(T\) is the range of \(T\) in the usual sense of the range of a function.%
\begin{theorem}{}{}{x:theorem:thm-ker-img-subspace}%
For any linear transformation \(T:V\to W\),%
\begin{enumerate}
\item{}\(\ker T\) is a subspace of \(V\).%
\item{}\(\im T\) is a subspace of \(W\).%
\end{enumerate}
%
\end{theorem}
\begin{proof}{}{g:proof:idp55}
%
\begin{enumerate}
\item{}To show that \(\ker T\) is a subspace, first note that \(\mathbf{0}\in \ker T\), since \(T(\mathbf{0})=\mathbf{0}\) for any linear transformation \(T\). If \(\vv,\ww\in \ker T\), then \(T(\vv)=\mathbf{0}\) and \(T(\ww)=0\), and therefore,%
\begin{equation*}
T(\vv+\ww)=T(\vv)+T(\ww)=\mathbf{0}+\mathbf{0}=\mathbf{0}\text{.}
\end{equation*}
Similarly, for any scalar \(c\) and \(\vv\in \ker T\),%
\begin{equation*}
T(c\vv)=cT(\vv)=c\mathbf{0}=\mathbf{0}\text{.}
\end{equation*}
By the subspace test, \(\ker T\) is a subspace.%
\item{}Again, since \(T(\mathbf{0})=\mathbf{0}\), we see that \(\mathbf{0}\in \im T\), so \(\im T\) is nonempty. If \(\ww_1,\ww_2\in \im T\), then there exist \(\vv_1,\vv_2\in V\) such that \(T(\vv_1)=\ww_1\) and \(T(\vv_2)=\ww_2\). It follows that%
\begin{equation*}
\ww_1+\ww_2 = T(\vv_1)+T(\vv_2) = T(\vv_1+\vv_2)\text{,}
\end{equation*}
so \(\ww_1+\ww_2\in \im T\). Similarly, if \(c\) is any scalar and \(\ww=T(\vv)\in\im T\), then%
\begin{equation*}
c\ww=cT(\vv)=T(c\vv)\text{,}
\end{equation*}
so \(c\ww\in \im T\).%
\end{enumerate}
%
\end{proof}
A familiar setting that you may already have encountered in a previous linear algebra course is that of a matrix transformation. Let \(A\) be an \(m\times n\) matrix. Then we can define \(T:\R^n\to \R^m\) by \(T(\xx)=A\xx\), where elements of \(\R^n,\R^m\) are considered as column vectors. We then have%
\begin{equation*}
\ker T = \nll(A) = \{\xx\in \R^n \,|\, A\xx=\mathbf{0}\}
\end{equation*}
and%
\begin{equation*}
\im T = \csp(A) = \{A\xx\,|\, \xx\in \R^n\}\text{,}
\end{equation*}
where \(\csp(A)\) denotes the \terminology{column space} of \(A\). Recall further that if we write \(A\) in terms of its columns as%
\begin{equation*}
A = \bbm C_1 \amp C_2 \amp \cdots \amp C_n\ebm
\end{equation*}
and a vector \(\xx\in \R^n\) as \(\xx=\bbm x_1\\x_2\\\vdots \\x_n\ebm\), then%
\begin{equation*}
A\xx = x_1C_1+x_2C_2+\cdots +x_nC_n\text{.}
\end{equation*}
Thus, any element of \(\csp(A)\) is a linear combination of its columns, explaining the name \emph{column space}.%
\par
Determining \(\nll(A)\) and \(\csp(A)\) for a given matrix \(A\) is, unsurprisingly, a matter of reducing \(A\) to row-echelon form. Finding \(\nll(A)\) is simply a matter of describing the set of all solutions to the homogeneous system \(A\xx=\mathbf{0}\). Finding \(\csp(A)\) relies on the following theorem.%
\begin{theorem}{}{}{x:theorem:thm-colspace}%
Let \(A\) be an \(m\times n\) matrix with columns \(C_1,C_2,\ldots, C_n\). If the reduced row-echelon form of \(A\) has leading ones in columns \(j_1,j_2,\ldots, j_k\), then%
\begin{equation*}
\{C_{j_1},C_{j_2},\ldots, C_{j_k}\}
\end{equation*}
is a basis for \(\csp(A)\).%
\end{theorem}
The truth of this theorem is demonstrated in Section 5.4 of the text by Nicholson. To see why it works, we need to remember a few basic facts from elementary linear algebra. First, recall that performing an elementary row operation on a matrix \(A\) is equivalent to multiplying on the left by an elementary matrix \(E\) defined using the same row operation.%
\par
Since every elementary matrix is invertible, and any product of invertible matrices is invertible, and we can transform \(A\) into a row-echelon matrix \(R\) using elementary row operations, it follows that \(R = UA\) for an invertible matrix \(U\); indeed, we have \(U = E_kE_{k-1}\cdots E_2E_1\), where \(E_1,\ldots, E_k\) are the elementary matrices corresponding to the row operations used to carry \(A\) to \(R\).%
\par
A basis for \(\csp(R)\) is given by the columns of \(R\) containing the leading ones. The reason for this is as follows. First, recall that each nonzero row begins with a leading one. So if the leading ones of \(R\) are in columns \(i_1,\ldots, i_k\), then there are \(k\) nonzero rows. Since all rows of zeros go at the bottom, each column in \(R\) has its last \(m-k\) entries identically zero. Thus,%
\begin{equation*}
\csp(R)\subseteq \left\{\bbm a_1\\\vdots \\a_k\\0\\\vdots 0\ebm\in \R^m \,|\, a_1,\ldots, a_k\in\R\right\}\text{,}
\end{equation*}
so \(\dim \csp(R)\leq k\). But the columns containing leading ones are easily shown to be independent, so they form a basis of \(\csp(R)\), which therefore has dimension \(k=\operatorname{rank}(A)\).%
\par
Next, since \(R=UA\), where \(U\) is invertible, if \(R=\bbm Y_1\amp Y_2\amp \cdots \amp Y_n\ebm\) and \(A = \bbm C_1\amp C_2\amp \cdots \amp C_n\ebm\), then \(C_i = U^{-1}Y_i\) for each \(i\). It follows from the fact that \(U\) is invertible and that the columns containing leading ones in \(R\) form a basis for \(\csp(R)\) that the corresponding columns in \(A\) form a basis for \(\csp(A)\). (For details, see Section 5.4 in Nicholson.)%
\par
For example, consider the linear transformation \(T:\R^4\to \R^3\) defined by the matrix%
\begin{equation*}
A = \bbm 1 \amp 3 \amp 0 \amp -2\\
-2 \amp -1 \amp 2 \amp 0\\
1 \amp 8 \amp 2 \amp -6\ebm\text{.}
\end{equation*}
Let's determine the \initialism{RREF} of \(A\):%
\begin{sageinput}
from sympy import Matrix,init_printing
init_printing()
A=Matrix(3,4,[1,3,0,-2,-2,-1,2,0,1,8,2,-6])
A.rref()
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp 0\amp -\frac65 \amp \frac25\\ 0\amp 1\amp \frac25 \amp -\frac45\\0\amp 0\amp 0\amp 0\ebm,(0,1)\]
\end{sageoutput}
We see that there are leading ones in the first and second column. Therefore, \(\csp(A) = \im(T) = \spn\left\{\bbm 1\\-2\\1\ebm, \bbm 3\\-1\\8\ebm\right\}\). Indeed, note that%
\begin{equation*}
\bbm 0\\2\\2\ebm = -\frac65\bbm 1\\-2\\1\ebm + \frac25\bbm 3\\-1\\8\ebm
\end{equation*}
and%
\begin{equation*}
\bbm -2\\0\\-6\ebm = \frac25\bbm 1\\-2\\1\ebm -\frac45\bbm 3\\-1\\8\ebm\text{,}
\end{equation*}
so that indeed, the third and fourth columns are in the span of the first and second.%
\par
Furthermore, we can determine the nullspace: if \(A\xx=\mathbf{0}\) where \(\xx=\bbm x_1\\x_2\\x_3\\x_4\ebm\), then we must have%
\begin{align*}
x_1 \amp =\frac65 x_3-\frac25 x_4\\
x_2 \amp =-\frac25 x_3+\frac 45 x_4\text{,}
\end{align*}
so%
\begin{equation*}
\xx = \bbm \frac65x_3-\frac25x_4\\ -\frac25x_3+\frac45x_4\\x_3\\x_4\ebm = \frac{x_3}{5}\bbm 6\\-2\\5\\0\ebm + \frac{x_4}{5}\bbm -2\\4\\0\\5\ebm\text{.}
\end{equation*}
It follows that a basis for \(\nll(A)=\ker T\) is \(\left\{\bbm 6\\-2\\5\\0\ebm, \bbm -2\\4\\0\\5\ebm\right\}\).%
\par
Incidentally, the SymPy library for Python has built-in functions for computing nullspace and column space. But it's probably worth your while to know how to determine these from the \initialism{RREF} of a matrix, without additional help from the computer. That said, let's see how the computer's output compares to what we found:%
\begin{sageinput}
A.nullspace()
\end{sageinput}
\begin{sageoutput}
\[\left[\bbm \frac65\\-\frac25\\1\\0\ebm, \bbm -\frac25\\ \frac45\\0\\1\ebm\right]\]
\end{sageoutput}
\begin{sageinput}
A.columnspace()
\end{sageinput}
\begin{sageoutput}
\[\left[\bbm 1\\-2\\1\ebm, \bbm 3\\-1\\8\ebm\right]\]
\end{sageoutput}
Note that the output from the computer simply states the basis for each space. Of course, for computational purposes, this is typically good enough.%
\par
An important result that comes out while trying to show that the ``pivot columns'' of a matrix (the ones that end up with leading ones in the \initialism{RREF}) are a basis for the column space is that the column rank (defined as the dimension of \(\csp(A)\)) and the row rank (the dimension of the space spanned by the columns of \(A\)) are equal. One can therefore speak unambiguously about the \terminology{rank} of a matrix \(A\), and it is just as it's defined in a first course in linear algebra: the number of leading ones in the \initialism{RREF} of \(A\).%
\par
For a general linear transformation, we can't necessarily speak in terms of rows and columns, but if \(T:V\to W\) is linear, and either \(V\) or \(W\) is finite-dimensional, then we can define the rank of \(T\) as follows.%
\begin{definition}{}{x:definition:def-rank-transformation}%
Let \(T:V\to W\) be a linear transformation. Then the \terminology{rank} of \(T\) is defined by%
\begin{equation*}
\operatorname{rank} T = \dim \im T\text{,}
\end{equation*}
and the \terminology{nullity} of \(T\) is defined by%
\begin{equation*}
\operatorname{nullity} T = \dim \ker T\text{.}
\end{equation*}
%
\end{definition}
Note that if \(W\) is finite-dimensional, then so is \(\im T\), since it's a subspace of \(W\). On the other hand, if \(V\) is finite-dimensional, then we can find a basis \(\{\vv_1,\ldots, \vv_n\}\) of \(V\), and the set \(\{T(\vv_1),\ldots, T(\vv_n)\}\) will span \(\im T\), so again the image is finite-dimensional, so the rank of \(T\) is finite. It is possible for either the rank or the nullity of a transformation to be infinite.%
\par
Knowing that the kernel and image of an operator are subspaces gives us an easy way to define subspaces. From the textbook, we have the following nice example.%
\begin{example}{}{g:example:idp56}%
Let \(T:M_{nn}\to M_{nn}\) be defined by \(T(A)=A-A^T\). Then%
\begin{enumerate}
\item{}\(T\) is a linear map.%
\item{}\(\ker T\) is equal to the set of all symmetric matrices.%
\item{}\(\im T\) is equal to the set of all skew-symmetric matrices.%
\end{enumerate}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp57}{}\quad{}%
\begin{enumerate}
\item{}We have \(T(0)=0\) since \(0^T=0\). Using proerties of the transpose and matrix algebra, we have%
\begin{equation*}
T(A+B) = (A+B)-(A+B)^T = (A-A^T)+(B-B^T) = T(A)+T(B)
\end{equation*}
and%
\begin{equation*}
T(kA) = (kA) - (kA)^T = kA-kA^T = k(A-A^T) = kT(A)\text{.}
\end{equation*}
%
\item{}It's clear that if \(A^T=A\), then \(T(A)=0\). On the other hand, if \(T(A)=0\), then \(A-A^T=0\), so \(A=A^T\). Thus, the kernel consists of all symmetric matrices.%
\item{}If \(B=T(A)=A-A^T\), then%
\begin{equation*}
B^T = (A-A^T)^T = A^T-A = -B\text{,}
\end{equation*}
so certainly every matrix in \(\im A\) is skew-symmetric. On the other hand, if \(B\) is skew-symmetric, then \(B=T(\frac12 B)\), since%
\begin{equation*}
T\Bigl(\frac12 B\Bigr) = \frac12 T(B) = \frac12(B-B^T) = \frac12(B-(-B))= B\text{.}
\end{equation*}
%
\end{enumerate}
%
\end{example}
You'll recall from a course like Math 2000 that in the study of functions, the properties of being injective (one-to-one) and surjective (onto) are important. They're important for linear transformations as well, and defined in exactly the same way.%
\par
It's clear that being surjective is closely tied to image. Indeed, by definition, \(T:V\to W\) is onto if \(\im T = W\). What might not be immediately obvious is that the kernel tells us if a linear map is injective.%
\begin{theorem}{}{}{x:theorem:thm-injective-kernel}%
Let \(T:V\to W\) be a linear transformation. Then \(T\) is injective if and only if \(\ker T = \{\mathbf{0}\}\).%
\end{theorem}
\begin{proof}{}{g:proof:idp58}
Suppose \(T\) is injective, and let \(\vv\in \ker T\). Then \(T(\vv)=\mathbf{0}\). On the other hand, we know that \(T(\mathbf{0})=\mathbf{0}\), and since \(T\) is injective, we must have \(\vv=\mathbf{0}\). Conversely, suppose that \(\ker T = \{0\}\) and that \(T(\vv_1)=T(\vv_2)\) for some \(\vv_1,\vv_2\in V\). Then%
\begin{equation*}
\mathbf{0} = T(\vv_1)-T(\vv_2) = T(\vv_1-\vv_2)\text{,}
\end{equation*}
so \(\vv_1-\vv_2\in \ker T\). Therefore, we must have \(\vv_1-\vv_2=\mathbf{0}\), so \(\vv_1=\vv_2\), and it follows that \(T\) is injective.%
\end{proof}
Let us return to the case of a matrix transformation \(T_A:\R^n\to \R^m\). Notice that \(\ker T_A\) is simply the set of all solutions to \(A\xx=\mathbf{0}\), while \(\im T_A\) is the set of all \(\yy\in\R^m\) for which \(A\xx=\yy\) \emph{has} a solution.%
\par
Recall from the discussion above that \(\rank A = \dim \csp(A) = \dim \im T_A\). It follows that \(T_A\) is surjective if and only if \(\rank A = m\). On the other hand, \(T_A\) is injective if and only if \(\rank A = n\), because we know that the system \(A\xx=\mathbf{0}\) has a unique solution if and only if each column of \(A\) contains a leading one.%
\par
This has some interesting consequences. If \(m=n\) (that is, if \(A\) is square), then each increase in \(\dim \nll(A)\) produces a corresponding decrease in \(\dim \csp(A)\), since both correspond to the ``loss'' of a leading one. Moreover, if \(\rank A = n\), then \(T_A\) is both injective and surjective. Recall that a function is invertible if and only if it is both injective and surjective. It should come as no surprise that invertibility of \(T_A\) (as a function) is equivalent to invertibility of \(A\) (as a matrix).%
\par
Also, note that if \(m \lt n\), then \(\rank A\leq m \lt n\), so \(T_A\) could be surjective, but can't possibly be injective. On the other hand, if \(m\gt n\), then \(\rank A\leq n \lt m\), so \(T_A\) could be injective, but can't possibly be surjective. These results generalize to linear transformations between any finite-dimensional vector spaces. The first step towards this is the following theorem, which is sometimes known as the Fundamental Theorem of Linear Transformations.%
\begin{theorem}{Dimension Theorem.}{}{x:theorem:thm-dimension-lintrans}%
Let \(T:V\to W\) be any linear transformation such that \(\ker T\) and \(\im T\) are finite-dimensional. Then \(V\) is finite-dimensional, and%
\begin{equation*}
\dim V = \dim \ker T + \dim \im T\text{.}
\end{equation*}
%
\end{theorem}
\begin{proof}{}{g:proof:idp59}
The trick with this proof is that we aren't assuming \(V\) is finite-dimensional, so we can't start with a basis of \(V\). But we do know that \(\im T\) is finite-dimensional, so we start with a basis \(\{\ww_1,\ldots, \ww_m\}\) of \(\im T\). Of course, every vector in \(\im T\) is the image of some vector in \(V\), so we can write \(\ww_i =T(\vv_i)\), where \(\vv_i\in V\), for \(i=1,2,\ldots, m\).%
\par
Since \(\{T(\vv_1),\ldots, T(\vv_m)\}\) is a basis, it is linearly independent. The results of \hyperref[x:exercise:ex_lintrans-indep]{Exercise~{\xreffont\ref{x:exercise:ex_lintrans-indep}}} tell us that the set \(\{\vv_1,\ldots, \vv_m\}\) must therefore be independent.%
\par
We now introduce a basis \(\{\uu_1,\ldots, \uu_n\}\) of \(\ker T\), which we also know to be finite-dimensional. If we can show that the set \(\{\uu_1,\ldots, \uu_n,\vv_1,\ldots, \vv_m\}\) is a basis for \(V\), we'd be done, since the number of vectors in this basis is \(\dim\ker T + \dim \im T\). We must therefore show that this set is independent, and that it spans \(V\).%
\par
To see that it's independent, suppose that%
\begin{equation*}
a_1\uu_1+\cdots + a_n\uu_n+b_1\vv_1+\cdots +b_m\vv_m=\mathbf{0}\text{.}
\end{equation*}
Applying \(T\) to this equation, and noting that \(T(\uu_i)=\mathbf{0}\) for each \(i\), by definition of the \(\uu_i\), we get%
\begin{equation*}
b_1T(\vv_1)+\cdots +b_mT(\vv_m)=\mathbf{0}\text{.}
\end{equation*}
We assumed that the vectors \(T(\vv_i)\) were independent, so all the \(b_i\) must be zero. But then we get%
\begin{equation*}
a_1\uu_1+\cdots +a_n\uu_n=\mathbf{0}\text{,}
\end{equation*}
and since the \(\uu_i\) are independent, all the \(a_i\) must be zero.%
\par
To see that these vectors span, choose any \(\xx\in V\). Since \(T(\xx)\in \im T\), there exist scalars \(c_1,\ldots, c_m\) such that%
\begin{equation}
T(\xx)=c_1T(\vv_1)+\cdots +c_mT(\vv_m)\text{.}\label{x:men:eqn-almost-span}
\end{equation}
We'd like to be able to conclude from this that \(\xx=c_1\vv_1+\cdots +c_m\vv_m\), but this would be false, unless \(T\) was known to be injective (which it isn't). Failure to be injective involves the kernel -{}-{} how do we bring that into the picture?%
\par
The trick is to realize that the reason we might have \(\xx\neq c_1\vv_1+\cdots +c_m\vv_m\) is that we're off by something in the kernel. Indeed, \hyperref[x:men:eqn-almost-span]{({\xreffont\ref{x:men:eqn-almost-span}})} can be re-written as%
\begin{equation*}
T(\xx-c_1\vv_1-\cdots -c_m\vv_m) = \mathbf{0}\text{,}
\end{equation*}
so \(\xx-c_1\vv_1-\cdots -c_m\vv_m\in\ker T\). But we have a basis for \(\ker T\), so we can write%
\begin{equation*}
\xx-c_1\vv_1-\cdots -c_m\vv_m=t_1\uu_1+\cdots +t_n\uu_n
\end{equation*}
for some scalars \(t_1,\ldots, t_n\), and this can be rearanged to give%
\begin{equation*}
\xx=t_1\uu_1+\cdots +t_n\uu_n+c_1\vv_1+\cdots + c_m\vv_m\text{,}
\end{equation*}
which completes the proof.%
\end{proof}
This is sometimes known as the \emph{Rank-Nullity Theorem}, since it can be stated in the form%
\begin{equation*}
\dim V = \rank T + \operatorname{nullity} T\text{.}
\end{equation*}
We will see that this result is frequently useful for providing simple arguments that establish otherwise difficult results. A basic situation where the theorem is useful is as follows: we are given \(T:V\to W\), where the dimensions of \(V\) and \(W\) are known. Since \(\im T\) is a subspace of \(W\), we know from \hyperref[x:theorem:thm-subspace-dim]{Theorem~{\xreffont\ref{x:theorem:thm-subspace-dim}}} that \(T\) is onto if and only if \(\dim \im T = \dim W\). In many cases it is easier to compute \(\ker T\) than it is \(\im T\), and the Dimension Theorem lets us determine \(\dim\im T\) if we know \(\dim V\) and \(\dim \ker T\).%
\par
A useful consequence of this result is that if we know \(V\) is finite-dimensional, we can order any basis such that the first vectors in the list are a basis of \(\ker T\), and the images of the remaining vectors produce a basis of \(\im T\).%
\par
Note that one consequence of the dimension theorem is that we must have%
\begin{equation*}
\dim \ker T\leq \dim V \quad \text{ and } \quad \dim \im T\leq \dim V\text{.}
\end{equation*}
Of course, we must also have \(\dim\im T\leq \dim W\), since \(\im T\) is a subspace of \(W\). In the case of a matrix transformation \(T_A\), this means that the rank of \(T_A\) is at most the minimum of \(\dim V\) and \(\dim W\). This once again has consequences for the existence and uniqueness of solutions for linear systems with the coefficient matrix \(A\).%
\begin{inlineexercise}{}{x:exercise:ex-dimension-injection-surjection}%
Let \(V\) and \(W\) be finite-dimensional vector spaces. Prove the following:%
\begin{enumerate}
\item{}\(\dim V\leq \dim W\) if and only if there exists an injection \(T:V\to W\).%
\item{}\(\dim V\geq \dim W\) if and only if there exists a surjection \(T:V\to W\).%
\end{enumerate}
%
\end{inlineexercise}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 2.3 Isomorphisms (a.k.a. invertible linear maps)}
\typeout{************************************************}
%
\begin{sectionptx}{Isomorphisms (a.k.a. invertible linear maps)}{}{Isomorphisms (a.k.a. invertible linear maps)}{}{}{x:section:sec-isomorphism}
We ended the last section with an important result. \hyperref[x:exercise:ex-dimension-injection-surjection]{Exercise~{\xreffont\ref{x:exercise:ex-dimension-injection-surjection}}} showed that existence of an injective linear map \(T:V\to W\) is equivalent to \(\dim V\leq \dim W\), and that existence of a surjective linear map is equivalent to \(\dim V\geq \dim W\). It's probably not surprising than that existence of a \emph{bijective} linear map \(T:V\to W\) is equivalent to \(\dim V = \dim W\).%
\begin{definition}{}{x:definition:def-isomorphism}%
A bijective linear transformation \(T:V\to W\) is called an \terminology{isomorphism}. If such a map exists, we say that \(V\) and \(W\) are \terminology{isomorphic}, and write \(V\cong W\).%
\end{definition}
\begin{theorem}{}{}{x:theorem:thm-iso-dimension}%
For any finite-dimensional vector spaces \(V\) and \(W\), \(V\cong W\) if any only if \(\dim V = \dim W\).%
\end{theorem}
\begin{proof}{}{g:proof:idp60}
If \(T:V\to W\) is a bijection, then it is both injective and surjective. Since \(T\) is injective, \(\dim V\leq \dim W\), by \hyperref[x:exercise:ex-dimension-injection-surjection]{Exercise~{\xreffont\ref{x:exercise:ex-dimension-injection-surjection}}}. By this same exercise,  since \(T\) is surjective, we must have \(\dim V\geq \dim W\). It follows that \(\dim V=\dim W\).%
\par
Suppose now that \(\dim V =\dim W\). Then we can choose bases \(\{\vv_1,\ldots, \vv_n\}\) of \(V\), and \(\{\ww_1,\ldots, \ww_n\}\) of \(W\). \hyperref[x:theorem:thm-define-using-basis]{Theorem~{\xreffont\ref{x:theorem:thm-define-using-basis}}} then guarantees the existence of a linear map \(T:V\to W\) such that \(T(\vv_i)=\ww_i\) for each \(i=1,2,\ldots, n\). Repeating the arguments of \hyperref[x:exercise:ex-dimension-injection-surjection]{Exercise~{\xreffont\ref{x:exercise:ex-dimension-injection-surjection}}} shows that \(T\) is a bijection.%
\end{proof}
Note that buried in the theorem above is the following useful fact: \emph{an isomorphism \(T:V\to W\) takes any basis of \(V\) to a basis of \(W\)}. Another remarkable result of the above theorem is that \emph{any two vector spaces of the same dimension are isomorphic}! In particular, we have the following theorem.%
\begin{theorem}{}{}{x:theorem:thm-iso-rn}%
If \(\dim V=n\), then \(V\cong \R^n\).%
\end{theorem}
This theorem is a direct consequence of \hyperref[x:theorem:thm-iso-dimension]{Theorem~{\xreffont\ref{x:theorem:thm-iso-dimension}}}. But it's useful to understand how it works in practice.%
\begin{definition}{}{x:definition:def-coefficient-iso}%
Let \(V\) be a finite-dimensional vector space, and let \(B=\{\mathbf{e}_1,\ldots, \mathbf{e}_n\}\) be a basis for \(V\). The \terminology{coefficient isomorphism} associated to \(B\) is the map \(C_B:V\to \R^n\) defined by%
\begin{equation*}
C_B(c_1\mathbf{e}_1+c_2\mathbf{e}_2+\cdots +c_n\mathbf{e}_n)=\bbm c_1\\c_2\\\vdots \\c_n\ebm\text{.}
\end{equation*}
%
\end{definition}
Note that this is a well-defined map since every vector in \(V\) can be written uniquely in terms of the basis \(B\).%
\par
The coefficient isomorphism is especially useful when we want to analyze a linear map computationally. Suppose we're given \(T:V\to W\) where \(V, W\) are finite-dimensional. Let us choose bases \(B=\{\vv_1,\ldots, \vv_n\}\) of \(V\) and \(B' = \{\ww_1,\ldots, \ww_m\}\) of \(W\). The choice of these two bases determines scalars \(a_{ij}, 1\leq i\leq n, 1\leq j\leq m\), such that%
\begin{equation*}
T(\vv_j) = a_{1j}\ww_1+a_{2j}\ww_2+\cdots + a_{mj}\ww_j,
\end{equation*}
for each \(i=1,2,\ldots, n\). The resulting matrix \(A=[a_{ij}]\) defines a matrix transformation \(T_A:\R^n\to \R^m\) such that%
\begin{equation*}
T_A\circ C_B = C_{B'}\circ T\text{.}
\end{equation*}
The relationship among the four maps used here is best captured by the ``commutative diagram'' in \hyperref[x:figure:fig_transformation_matrix]{Figure~{\xreffont\ref{x:figure:fig_transformation_matrix}}}.%

\parmarginbox{%
\begin{figureptx}{Defining the matrix of a linear map with respect to choices of basis.}{x:figure:fig_transformation_matrix}{}%
\begin{image}{0}{1}{0}%
\resizebox{\linewidth}{!}{%
\begin{tikzpicture}
\matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
{
V \amp W \\
\R^n \amp \R^m \\};
\path[-stealth]
(m-1-1) edge node [left] {$C_B$} (m-2-1)
        edge node [above] {$T$} (m-1-2)
(m-2-1) edge node [above] {$T_A$} (m-2-2)
(m-1-2) edge node [right] {$C_{B'}$} (m-2-2);
\end{tikzpicture}
}%
\end{image}%
\tcblower
\end{figureptx}%
}{0pt}

With this connection between linear maps (in general) and matrices, it can be worthwhile to pause and consider invertibility in the context of matrices. Recall that an \(n\times n\) matrix \(A\) is \emph{invertible} if there exists a matrix \(A^{-1}\) such that \(AA^{-1}=I_n\) and \(A^{-1}A=I_n\).%
\par
The same definition can be made for linear maps. We've defined what it means for a map \(T:V\to W\) to be invertible as a \emph{function}. In particular, we relied on the fact that any bijection has an inverse.%
\par
First, a note on notation. Given linear maps \(U\xrightarrow{T} V\xrightarrow{S} W\), we typically write the composition \(S\circ T:U\to W\) as a ``product'' \(ST\). The reason for this is again to mimic the case of matrices. Let \(A\) be an \(m\times n\) matrix, and let \(B\) be an \(n\times k\) matrix. Then we have linear maps%
\begin{equation*}
\R^k \xrightarrow{T_B} \R^n\xrightarrow{T_A} \R^m\text{,}
\end{equation*}
and the composition \(T_A\circ T_B:\R^k\to \R^m\) satisfies%
\begin{equation*}
T_A\circ T_B(\xx) = T_A(T_B(\xx)) = T_A(B\xx)=A(B\xx)=(AB)\xx=T_{AB}(\xx)\text{.}
\end{equation*}
Note that the rules given in elementary linear algebra, for the relative sizes of matrices that can be multiplied, are simply a manifestation of the fact that to compose functions, the range of the first must be contained in the domain of the second.%
\begin{inlineexercise}{}{g:exercise:idp61}%
Show that the composition of two linear maps is again a linear map.%
\end{inlineexercise}%
Moreover, \hyperref[x:theorem:thm-iso-dimension]{Theorem~{\xreffont\ref{x:theorem:thm-iso-dimension}}} tells us why we can only consider invertibility for square matrices: we know that invertible linear maps are only defined between spaces of equal dimension. In analogy with matrices, some texts will define a linear map \(T:V\to W\) to be invertible if there exists a linear map \(S:W\to V\) such that%
\begin{equation*}
ST = 1_V \quad \text{ and } \quad TS = 1_W\text{.}
\end{equation*}
From this definition, one can show that \(S\) and \(T\) must be bijections, and of course, if \(T\) is a bijection (as in our definition) then we know it has an inverse. What remains to be seen is that this inverse is also a linear map.%
\begin{inlineexercise}{}{g:exercise:idp62}%
Let \(T:V\to W\) be a bijective linear transformation. Show that \(T^{-1}:W\to V\) is a linear transformation.%
\end{inlineexercise}%
\begin{inlineexercise}{}{g:exercise:idp63}%
Show that if \(ST=1_V\), then \(S\) is surjective and \(T\) is injective. Conclude that if \(ST=1_V\) and \(TS=1_w\), then \(S\) and \(T\) are both bijections.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp64}{}\quad{}This is really a Math 2000 problem.%
\end{inlineexercise}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Worksheet 2.4 Worksheet: matrix transformations}
\typeout{************************************************}
%
\newgeometry{left=1.25cm, right=1.25cm, top=1.25cm, bottom=1.25cm}
\begin{worksheet-section}{Worksheet: matrix transformations}{}{Worksheet: matrix transformations}{}{}{x:worksheet:worksheet-transformations}
This worksheet deals with matrix transformations, and in particular, kernel and image. The goal is to understand these important subspaces in a familiar context.%
\par
Let \(A\) be an \(m\times n\) matrix. We can use \(A\) to define a transformation \(T_A:\R^n\to \R^m\) given by \(T_A(\xx)=A\xx\), where we view \(\xx\) as an \(n\times 1\) column vector.%
\par
The \terminology{kernel} of \(T_A\) is the set of vectors \(\xx\) such that \(T_A(\xx)=\zer\). That is, \(\ker T_A\) is the set of solutions to the homogeneous system \(A\xx = \zer\).%
\par
The \terminology{image} of \(T_A\) (also known as the range of \(T_A\)) is the set of vectors \(\yy\in \R^m\) such that \(\yy = A\xx\) for some \(\xx\in\R^n\). In other words, \(\im(T_A)\) is the set of vectors \(\yy\) for which the non-homogeneous system \(A\xx=\yy\) is consistent.%
\par
Because \(T_A\) is a linear transformation, we can compute it as long as we're given its values on a basis. If \(\{\vv_1, \vv_2,\ldots, \vv_n\}\) is a basis for \(\R^n\), then for any \(\xx\in\R^n\) there exist unique scalars \(c_1,c_2,\ldots, c_n\) such that%
\begin{equation*}
\xx = c_1\vv_1+c_2\vv_2+\cdots + c_n\vv_n\text{,}
\end{equation*}
and since \(T_A\) is linear, we have%
\begin{equation*}
T_A(\xx)=c_1T_A(\vv_1)+c_2T_A(\vv_2)+\cdots +c_nT_A(\vv_n)\text{.}
\end{equation*}
%
\par
The main challenge, computationally speaking, is that if our basis is not the standard basis, some effort will be required to write \(\xx\) in terms of the given basis.%
\begin{divisionexercise}{1}{}{3in}{g:exercise:idp65}%
Confirm that%
\begin{equation*}
B=\left\{\begin{bmatrix}1\\0\\2\\3\end{bmatrix},\begin{bmatrix}4\\2\\0\\-3\end{bmatrix},
\begin{bmatrix}0\\4\\-3\\2\end{bmatrix}, \begin{bmatrix}3\\5\\-2\\1\end{bmatrix}\right\}
\end{equation*}
is a basis for \(\R^4\).%
\end{divisionexercise}%
To assist with solving this problem, a code cell is provided below. Recall that you can enter the matrix \(\begin{bmatrix}a\amp b\amp c\\d\amp e\amp f\\g\amp h\amp i\end{bmatrix}\) as \mono{Matrix([[a,b,c],[d,e,f],[g,h,i]])} or as \mono{Matrix(3,3,[a,b,c,d,e,f,g,h,i])}.%
\clearpage
The reduced row-echeleon form of \mono{A} is given by \mono{A.rref()}. The product of matrices \mono{A} and \mono{B} is simply \mono{A*B}. The inverse of a matrix \mono{A} can be found using \mono{A.inv()} or simply \mono{A**(-1)}.%
\par
One note of caution: in the \initialism{HTML} worksheet, if you don't import \mono{sympy} as your first line of code, you'll instead use Sage syntax. Sage uses \mono{A.inverse()} instead of \mono{A.inv()}.%
\par
In a Jupyter notebook, remember you can generate additional code cells by clicking on the \mono{+} button.%
\begin{sageinput}
from sympy import Matrix,init_printing
init_printing()
\end{sageinput}
You can also use the cell below to write down any necessary explanation.%
\begin{divisionexercise}{2}{}{4in}{g:exercise:idp66}%
Write each of the standard basis vectors in terms of this basis.%
\par
\emph{Suggestion:} in each case, this can be done by solving a matrix equation, using the inverse of an appropriate matrix.%
\end{divisionexercise}%
\clearpage
A linear transformation \(T:\R^4\to \R^4\) is now defined as follows:%
\begin{equation*}
T\left(\begin{bmatrix}1\\0\\2\\3\end{bmatrix}\right)=\begin{bmatrix}3\\0\\2\\-1\end{bmatrix},
T\left(\begin{bmatrix}4\\2\\0\\-3\end{bmatrix}\right)=\begin{bmatrix}1\\2\\0\\5\end{bmatrix},
T\left(\begin{bmatrix}0\\4\\-3\\2\end{bmatrix}\right)=\begin{bmatrix}4\\2\\2\\4\end{bmatrix},
T\left(\begin{bmatrix}3\\5\\-2\\1\end{bmatrix}\right)=\begin{bmatrix}2\\4\\0\\10\end{bmatrix}\text{.}
\end{equation*}
%
\par
Let \(\{\vece_1,\vece_2,\vece_3, \vece_4\}\) denote the standard basis for \(\R^4\).%
\begin{divisionexercise}{3}{}{2.5in}{g:exercise:idp67}%
Determine \(T(\vece_i)\) for \(i=1,2,3,4\), and in so doing, determine the matrix \(A\) such that \(T=T_A\).%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{2.5in}{g:exercise:idp68}%
Let \(M\) be the matrix whose columns are given by the values of \(T\) on the basis \(B\). (This would be the matrix of \(T\) if \(B\) was actually the standard basis.) Let \(N\) be the matrix whose inverse you used to solve part (b). Can you find a way to combine these matrices to obtain the matrix \(A\)? If so, explain why your result makes sense.%
\end{divisionexercise}%
\clearpage
Next we will compute the kernel and image of the transformation from the previous exercises. Recall that when solving a homogeneous system \(A\xx = \zer\), we find the \initialism{RREF} of \(A\), and any variables whose columns do not contain a leading 1 are assigned as parameters. We then express the general solution \(x\) in terms of those parameters.%
\par
The image of a matrix transformation \(T_A\) is also known as the \terminology{column space} of \(A\), because the range of \(T_A\) is precisely the span of the columns of \(A\). The \initialism{RREF} of \(A\) tells us which columns to keep: the columns of \(A\) that correspond to the columns in the \initialism{RREF} of \(A\) with a leading 1.%
\par
Let \(T\) be the linear transformation given in the previous exercises.%
\begin{divisionexercise}{5}{}{1.5in}{g:exercise:idp69}%
Determine the kernel of \(T\).%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{1.5in}{g:exercise:idp70}%
Determine the image of \(T\).%
\end{divisionexercise}%
\begin{divisionexercise}{7}{}{1in}{g:exercise:idp71}%
The \terminology{Dimension Theorem} states that for a linear transformation \(T:V\to W\), where \(V\) is finite-dimensional,%
\begin{equation*}
\dim V = \dim\ker(T)+ \dim\im(T)\text{.}
\end{equation*}
Explain why this result makes sense using your results for this problem.%
\end{divisionexercise}%
\end{worksheet-section}
\restoregeometry
%
%
\typeout{************************************************}
\typeout{Worksheet 2.5 Worksheet: linear recurrences}
\typeout{************************************************}
%
\newgeometry{left=1.25cm, right=1.25cm, top=1.25cm, bottom=1.25cm}
\begin{worksheet-section}{Worksheet: linear recurrences}{}{Worksheet: linear recurrences}{}{}{x:worksheet:worksheet-recurrence}
For this worksheet, the reader is directed to Section 7.5 of \emph{Linear Algebra with Applications}, by Keith Nicholson. You will probably want to read this section before proceeding.%
\par
A \emph{linear recurrence} of length \(k\) is a sequence \([x_n)\) that is recursively defined, with successive terms in the sequence defined in terms of the previous \(k\) terms, via a linear recursion formula of the form%
\begin{equation*}
x_{n+k} = a_0x_k + a_1x_{k+1}+\cdots + a_{k-1}x_{n+k-1}\text{.}
\end{equation*}
(Here we assume \(a_0\neq 0\) to have the appropriate length.) The most famous example of a linear recurrence is, of course, the Fibonacci sequence, which is defined by \(x_0=1, x_1=1\), and \(x_{n+2}=x_n+x_{n+1}\) for all \(n\geq 0\).%
\par
As demonstrated in the Nicholson's book, the set of all sequences satisfying a linear recursion of length \(k\) form a subspace \(V\) of the vector space \(\mathbb{R}^\infty\) of all real-valued sequences. Since each sequence is determined by the \(k\) initial conditions \(x_0, x_1, \ldots, x_{k-1}\), we see that \(V\) is isomorphic to \(\mathbb{R}^k\).%
\par
The goal of this worksheeet is to understand how to obtain \emph{closed form} expressions for a recursively defined sequence using linear algebra. That is, rather than having to generate terms of the sequence one-by-one using the recursion formula, we want a function of \(n\) that will produce each term \(x_n\) in the sequence.%
\par
Since we know the dimension of the space \(V\) of solutions, it suffices to understand two things:%
\begin{itemize}[label=\textbullet]
\item{}How to produce a basis for \(V\).%
\item{}How to write a given solution in terms of that basis.%
\end{itemize}
%
\par
The key observation is that for each recursion relation, there is an associated polynomial. For the recursion%
\begin{equation*}
x_{n+k} = a_0x_n + a_1x_{n+1}+\cdots + a_{k-1}x_{n+k-1}\text{,}
\end{equation*}
the associated polynomial is%
\begin{equation*}
p(x) = x^k - a_{k-1}x^{k-1}-\cdots -a_1x-a_0\text{.}
\end{equation*}
%
\par
The main result is Theorem 7.5.4 in Nicholson. If we can factor \(p(x)\) completely over the reals as%
\begin{equation*}
p(x) = (x-\lambda_1)^{m_1}(x-\lambda_2)^{m_2}\cdots (x-\lambda_p)^{m_p}\text{,}
\end{equation*}
then a basis for the space of solutions is given by%
\begin{align*}
\left[\lambda_1^n\right), \amp \left[n\lambda_1^n\right),\ldots, \left[n^{m_1-1}\lambda_1^n\right)\\
\left[\lambda_2^n\right), \amp \left[n\lambda_2^n\right),\ldots, \left[n^{m_2-1}\lambda_2^n\right)\\
\amp \vdots \\
\left[\lambda_p^n\right), \amp \left[n\lambda_p^n\right),\ldots, \left[n^{m_p-1}\lambda_p^n\right)\text{.}
\end{align*}
%
\par
Once we have a basis, we can apply the given coefficients to determine how to write a particular sequence as a linear combination of the basis vectors.%
\clearpage
\begin{divisionexercise}{1}{}{3in}{g:exercise:idp72}%
Find a basis for the space \(V\) of sequences \([x_n)\) satisfying the recurrence%
\begin{equation*}
x_{n+3}=-2x_n+x_{n+1}+2x_{n+2}\text{.}
\end{equation*}
%
\par
Then find a formula for the sequence satisfying the initial conditions \(x_0=1, x_1=2, x_2=1\).%
\end{divisionexercise}%
To solve this problem, you may use Python code, as outlined below. To get started, load the functions you'll need from the SymPy library.%
\begin{sageinput}
from sympy import symbols,factor,init_printing
x = symbols('x')
init_printing()
\end{sageinput}
First, determine the associated polynomial for the recurrence.%
\par
(Input your polynomial in the cell below. To get proper formatting, wrap your math in \mono{\$} delimiters, and can use \mono{\textasciicircum{}} to enter exponents.)%
Next, factor the polynomial. You can do this using the \mono{factor()} command. In Python, you will need to enter \mono{**} for the exponents.%
In the cell below, list the roots of the polynomial, and the resulting basis \(B\) for the space \(V\) of solutions. Recall that if \(\lambda\) is a root of the polynomial, then \([\lambda^n)\) will be a basis vector for the vector space \(V\) of solutions. You may wish to confirm that each of your basis sequences indeed satisfies our recursion.%
\clearpage
Next, let \(s=[x_n)\) be the recursion that satisfies the given initial conditions. We want to write \([x_n)\) in terms of the basis we just found. Recall, from Nicholson, that since our basis has three elements, there is an isomorphism \(T:\R^3\to V\), where \(T(a,b,c)\) is equal to the sequence \([x_n)\) in \(V\) that satisfies the initial conditions \(x_0=a, x_1=b, x_2=c\). Thus, our desired sequence is given by \(s=T(1,2,1)\).%
\par
Let \(\vv_1, \vv_2, \vv_3\in\R^3\) be the vectors such that \(B=\{T(\vv_1), T(\vv_2), T(\vv_3)\}\). (That is, write out the first three terms in each sequence in your basis to get three vectors.) We then need to find scalars \(c_1,c_2,c_3\) such that%
\begin{equation*}
c_1\vv_1+c_2\vv_2+c_3\vv_3=(1,2,1)\text{.}
\end{equation*}
We will then have%
\begin{align*}
s \amp = T(1,2,1)\\
\amp = c_1T(\vv_1)+c_2T(\vv_2)+c_3T(\vv_3)\text{,}
\end{align*}
and we recall that the sequences \(T(\vv_i)\) are the sequences in our basis \(B\).%
\par
Set up this system, and then use the computer to solve. Let \mono{A} be the coefficient matrix for the system, which you will need to input into the cell below, and let \mono{B} be the column vector containing the initial conditions.%
\begin{sageinput}
from sympy import Matrix
A = Matrix()
B = Matrix([1,2,1])
X = A**(-1)*B
X
\end{sageinput}
Using the solution above, state the answer to this exercise.%
Now, we leave you with a few more exercises. Recall that if the associated polynomial for your recursion has a repeated root \((x-\lambda)^k\), then your basis will include the sequences \([\lambda^n), [n\lambda^n), \ldots, [n^{k-1}\lambda^n)\).%
\begin{divisionexercise}{2}{}{2in}{g:exercise:idp73}%
Find a basis for the space \(V\) of sequences \([x_n)\) satisfying the recurrence%
\begin{equation*}
x_{n+3} = -4x_n+3x_{n+2}\text{.}
\end{equation*}
Then find a formula for the sequence satisfying the initial conditions \(x_0=1, x_1=-1, x_2=1\).%
\end{divisionexercise}%
\clearpage
\begin{divisionexercise}{3}{}{3in}{g:exercise:idp74}%
Find a basis for the space \(V\) of sequences \([x_n)\) satisfying the recurrence%
\begin{equation*}
x_{n+3} = 8x_n-12x_{n+1}+6x_{n+2}\text{.}
\end{equation*}
Then find a formula for the sequence satisfying the initial conditions \(x_0=1, x_1=-1, x_2=1\).%
\end{divisionexercise}%
Our last exercise is more theoretical, and less computational. Recall that the \emph{shift operator} \(S\) on the vector space \(\R^\infty\) of sequences is the operator that deletes the first entry of a sequence. (So the remaining entries get ``shifted'' once to the left.)%
\begin{divisionexercise}{4}{}{3in}{g:exercise:idp75}%
Show that the shift operator is onto, but not one-to-one. Then, determine the kernel of the shift operator.%
\end{divisionexercise}%
\end{worksheet-section}
\restoregeometry
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 3 Orthogonality and Applications}
\typeout{************************************************}
%
\begin{chapterptx}{Orthogonality and Applications}{}{Orthogonality and Applications}{}{}{x:chapter:ch-orthogonality}
%
%
\typeout{************************************************}
\typeout{Section 3.1 Orthogonal sets of vectors}
\typeout{************************************************}
%
\begin{sectionptx}{Orthogonal sets of vectors}{}{Orthogonal sets of vectors}{}{}{x:section:sec-orthogonal-sets}
\begin{introduction}{}%
You may recall from elementary linear algebra, or a calculus class, that vectors in \(\R^2\) or \(\R^3\) are considered to be quantities with both \emph{magnitude} and \emph{direction}. Interestingly enough, neither of these properties is inherent to a general vector space. The vector space axioms specify only algebra; they say nothing about geometry. (What, for example, should be the ``angle'' between two polynomials?)%
\par
Because vector algebra is often introduced as a consequence of geometry (like the ``tip-to-tail'' rule), you may not have thought all that carefully about what, exactly, is responsible for making the connection between algebra and geometry. It turns out that the missing link is the humble dot product. This should be plausible after a bit of thought. After all, you probably encountered the following result, perhaps as a consequence of the law of cosines: for any two vectors \(\uu,\vv\in\R^2\),%
\begin{equation*}
\uu\dotp\vv = \len{\uu}\,\len{\vv}\cos\theta\text{,}
\end{equation*}
where \(\theta\) is the angle between \(\uu\) and \(\vv\). Here we see both magnitude and direction (encoded by the angle) defined in terms of the dot product.%
\par
While it is possible to generalize the idea of the dot product to something called an \emph{inner product}, we will first focus on the basic dot product in \(\R^n\). Once we have a good understanding of things in that setting, we can move on to consider the abstract counterpart.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection 3.1.1 Basic definitions and properties}
\typeout{************************************************}
%
\begin{subsectionptx}{Basic definitions and properties}{}{Basic definitions and properties}{}{}{x:subsection:subsec-dot-basics}
For most of this chapter (primarily for typographical reasons) we will denote elements of \(\R^n\) as ordered \(n\)-tuples \((x_1,\ldots, x_n)\) rather than as column vectors.%
\begin{definition}{}{x:definition:def-dot-prod-norm}%
Let \(\xx=(x_1,x_2,\ldots, x_n)\) and \(\yy=(y_1,y_2,\ldots, y_n)\) be vectors in \(\R^n\). The \terminology{dot product} of \(\xx\) and \(\yy\), denoted by \(\xx\dotp\yy\) is the scalar defined by%
\begin{equation*}
\xx\dotp \yy = x_1y_1+x_2y_2+\cdots + x_ny_n\text{.}
\end{equation*}
The \terminology{norm} of a vector \(\xx\) is denoted \(\len{\xx}\) and defined by%
\begin{equation*}
\len{\xx} = \sqrt{x_1^2+x_2^2+\cdots + x_n^2}\text{.}
\end{equation*}
%
\end{definition}
Note that both the dot product and the norm produce \emph{scalars}. Through the Pythagorean Theorem, we recognize the norm as the length of \(\xx\). The dot product can still be thought of as measuring the angle between vectors, although the simple geometric proof used in two dimensions is not that easily translated to \(n\) dimensions. At the very least, the dot product lets us extend the notion of right angles to higher dimensions.%
\begin{definition}{}{x:definition:def-orthogonal}%
We say that two vectors \(\xx,\yy\in\R^n\) are \terminology{orthogonal} if \(\xx\dotp\yy = 0\).%
\end{definition}
It should be no surprise that all the familiar properties of the dot product work just as well in any dimension. The folowing properties are all easily confirmed by routine computation.%
\begin{theorem}{}{}{x:theorem:thm-dot-props}%
For any vectors \(\xx,\yy,\zz\in\R^n\),%
\begin{enumerate}
\item{}\(\displaystyle \xx\dotp\yy = \yy\dotp\xx\)%
\item{}\(\displaystyle \xx\dotp(\yy+\zz)=\xx\dotp\yy+\xx\dotp\zz\)%
\item{}For any scalar \(c\), \(\xx\dotp(c\yy) = (c\xx)\dotp\yy=c(\xx\dotp\yy)\)%
\item{}\(\xx\dotp\xx\geq 0\), and \(\xx\dotp\xx=0\) if and only if \(\xx=\mathbf{0}\)%
\end{enumerate}
%
\end{theorem}
The above properties, when properly abstracted, become the defining properties of a (real) inner product. (A complex inner product also involves complex conjugates.) For a general inner product, the requirement \(\xx\dotp\xx\geq 0\) is referred to as being \emph{positive-definite}, and the property that only the zero vector produces zero when dotted with itself is called \emph{nondegenerate}. Note that we have the following connection between norm and dot product:%
\begin{equation*}
\len{\xx}^2 = \xx\dotp \xx\text{.}
\end{equation*}
For a general inner product, this can be used as a \emph{definition} of the norm associated to an inner product.%
\begin{inlineexercise}{}{g:exercise:idp76}%
Given that \(\len{\xx}=3, \len{\yy}=1\), and \(\xx\dotp\yy=-2\), compute \((4\xx-3\yy)\dotp (\xx+5\yy)\).%
\end{inlineexercise}%
\begin{inlineexercise}{}{x:exercise:ex-norm-sum-square}%
Show that for any vectors \(\xx,\yy\in\R^n\), we have%
\begin{equation*}
\len{\xx+\yy}^2 = \len{\xx}^2+2\xx\dotp\yy+\len{\yy}^2\text{.}
\end{equation*}
%
\end{inlineexercise}%
\begin{inlineexercise}{}{g:exercise:idp77}%
Suppose \(\mathbb{R}^n=\spn\{\vv_1,\vv_2,\ldots, \vv_k\}\). Prove that \(\xx=\mathbf{0}\) if and only if \(\xx\dotp \vv_i=0\) for each \(i=1,2,\ldots, k\).%
\end{inlineexercise}%
There are two important inequalities associated to the dot product and norm. We state them both in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm-cauchy-triangle}%
Let \(\xx,\yy\) be any vectors in \(\R^n\). Then%
\begin{enumerate}
\item{}\(\displaystyle \lvert \xx\dotp \yy\rvert \leq \len{\xx}\len{\yy}\)%
\item{}\(\displaystyle \len{\xx+\yy}\leq \len{\xx}+\len{\yy}\)%
\end{enumerate}
%
\end{theorem}
The first of the above inequalities is called the \emph{Cauchy-Schwarz inequality}, which be viewed as a manifestation of the formula%
\begin{equation*}
\xx\dotp \yy = \len{\xx}\len{\yy}\cos\theta\text{,}
\end{equation*}
since after all, \(\lvert \cos\theta\rvert\leq 1\) for any angle \(\theta\). For a direct proof, we note that for any scalars \(r,s\) we have%
\begin{equation*}
\len{r\xx\pm s\yy}^2 = r^2\len{\xx}^2\pm 2rs\xx\dotp\yy+s^2\len{\yy}^2\text{.}
\end{equation*}
Putting \(r=\len{\yy}, s=\len{\xx}\) gives%
\begin{equation*}
\len{s\xx\pm r\yy}^2 = 2rs(rs\pm\xx\dotp\yy)\text{.}
\end{equation*}
Since the left-hand side is non-negative, we must have%
\begin{equation*}
\xx\yy\leq \len{\xx}\len{\yy} \quad \text{ and } -\len{\xx}\len{\yy}\leq \xx\dotp\yy\text{,}
\end{equation*}
and the result follows.%
\par
The second result, called the  \emph{triangle inequality}, follows immediately from the Cauchy-Scwarz inequality and \hyperref[x:exercise:ex-norm-sum-square]{Exercise~{\xreffont\ref{x:exercise:ex-norm-sum-square}}}. The triangle inequality gets its name from the ``tip-to-tail'' picture for vector addition. Essentially, it tells us that the length of any side of a triangle must be less than the sum of the lengths of the other two sides. The importance of the triangle inequality is that it tells us that the norm can be used to define distance.%
\begin{definition}{}{x:definition:def-vector-distance}%
For any vectors \(\xx,\yy\in \R^n\), the \terminology{distance} from \(\xx\) to \(\yy\) is denoted \(d(\xx,\yy)\), and defined as%
\begin{equation*}
d(\xx,\yy) = \len{\xx-\yy}\text{.}
\end{equation*}
%
\end{definition}
Using properties of the norm, we can show that this distance function meets the criteria of what's called a \emph{metric}. A metric is any function that takes a pair of vectors (or points) as input, and returns a number as output, with the following properties:%
\begin{enumerate}
\item{}\(d(\xx,\yy)=d(\yy,\xx)\) for any \(\xx,\yy\)%
\item{}\(d(\xx,\yy)\geq 0\), and \(d(\xx,\yy)=0\) if and only if \(\xx=\yy\)%
\item{}\(d(\xx,\yy)\leq d(\xx,\zz)+d(\zz,\yy)\) for any \(\xx,\yy,\zz\)%
\end{enumerate}
We leave it as an exercise to confirm that the distance function defined above is a metric.%
\par
In more advanced courses (e.g.\@ topology or analysis) you might go into detailed study of these structures. There are three interrelated structures: inner products, norms, and metrics. You might consider questions like: does every norm come from an inner product? Does every metric come from a norm? (No.) Things get even more interesting for infinite-dimensional spaces. Of special interest are spaces such as \emph{Hilbert spaces} (a special type of infinite-dimensional inner product space) and \emph{Banach spaces} (a special type of infinite-dimensional normed space).%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection 3.1.2 Orthogonal sets of vectors}
\typeout{************************************************}
%
\begin{subsectionptx}{Orthogonal sets of vectors}{}{Orthogonal sets of vectors}{}{}{x:subsection:subsec-ortho-sets}
In earlier chapters, we've seen that among different sets of vectors one could consider, independent sets and spanning sets are both worthy of study. One of the main themes of this chapter is that \emph{orthogonal} sets are equally worthy, and in many cases, easier to work with.%
\begin{definition}{}{x:definition:def-ortho-set}%
A set of vectors \(\{\vv_1,\vv_2,\ldots, \vv_k\}\) in \(\R^n\) is called \terminology{orthogonal} if:%
\begin{itemize}[label=\textbullet]
\item{}\(\vv_i\neq \mathbf{0}\) for each \(i=1,2\ldots, k\)%
\item{}\(\vv_i\dotp\vv_j = 0\) for all \(i\neq j\)%
\end{itemize}
%
\end{definition}
\begin{inlineexercise}{}{x:exercise:ex-orthogonal-set}%
Show that the following is an orthogonal subset of \(\R^4\).%
\begin{equation*}
\{(1,0,1,0), (-1,0,1,1), (1,1,-1,2)\}
\end{equation*}
Can you find a fourth vector that is orthogonal to each vector in this set?%
\end{inlineexercise}%
The requirement that the vectors in an orthogonal set be nonzero is partly because the alternative would be boring, and partly because it lets us state the following theorem.%
\begin{theorem}{}{}{x:theorem:thm-ortho-independent}%
Any orthogonal set of vectors is linearly independent.%
\end{theorem}
\begin{proof}{}{g:proof:idp78}
Suppose \(S=\{\vv_1,\vv_2,\ldots, \vv_k\}\) is orthogonal, and suppose%
\begin{equation*}
c_1\vv_1+c_2\vv_2+\cdots + c_k\vv_k = \mathbf{0}
\end{equation*}
for scalars \(c_1,c_2,\ldots, c_k\). Taking the dot product of both sides of the above equation with \(\vv_1\) gives%
\begin{align*}
c_1(\vv_1\dotp \vv_1)+c_2(\vv_1\dotp \vv_2)+\cdots +c_k(\vv_1\dotp \vv_k) \amp =\vv_1\dotp \mathbf{0}\\
c_1\len{\vv_1}^2+0+\cdots + 0\amp = 0 \text{.}
\end{align*}
Since \(\len{\vv_1}^2\neq 0\), we must have \(c_1=0\). We similarly find that all the remaining scalars are zero by taking the dot product with \(\vv_2,\ldots, \vv_k\).%
\end{proof}
Another useful consequence of orthogonality: recall that in two dimensions, we have the Pythagorean Theorem for right-angled triangles, but have to settle for the Law of Cosines otherwise. In \(n\) dimensions, we have the following, which follows from the fact that all ``cross terms'' (dot products of different vectors) will vanish.%
\begin{theorem}{}{}{x:theorem:thm-pythagoras}%
For any orthogonal set of vectors \(\{\xx_1,\ldots, \xx_k\}\) we have%
\begin{equation*}
\len{\xx_1+\cdots +\xx_k}^2 = \len{\xx_1}^2+\cdots + \len{\xx_k}^2\text{.}
\end{equation*}
%
\end{theorem}
Our final initial result about orthogonal sets of vectors relates to span. In general, we know that if \(\yy\in\spn\{\xx_1,\ldots, \xx_k\}\), then it is possible to solve for scalars \(c_1,\ldots, c_k\) such that \(\yy=c_1\xx_1+\cdots+ c_k\xx_k\). The trouble is that finding these scalars generally involves setting up, and then solving, a system of linear equations. The great thing about orthogonal sets of vectors is that we can provide explicit formulas for the scalars.%
\begin{theorem}{Fourier expansion theorem.}{}{x:theorem:thm-fourier-expansion}%
Let \(S=\{\vv_1,\vv_2,\ldots, \vv_k\}\) be an orthogonal set of vectors. For any \(\yy\in \spn S\), we have%
\begin{equation*}
\yy = \left(\frac{\yy\dotp\mathbf{v}_1}{\vv_1\dotp\vv_1}\right)\vv_1+
\left(\frac{\yy\dotp\mathbf{v}_2}{\vv_2\dotp\vv_2}\right)\vv_2+\cdots +
\left(\frac{\yy\dotp\mathbf{v}_k}{\vv_k\dotp\vv_k}\right)\vv_k\text{.}
\end{equation*}
%
\end{theorem}
\begin{proof}{}{g:proof:idp79}
Let \(\yy=c_1\vv_1+\cdots + c_k\vv_k\). Taking the dot product of both sides of this equation with \(\vv_i\) gives%
\begin{equation*}
\vv_i\dotp\yy = c_i(\vv_i\dotp\vv_i)\text{,}
\end{equation*}
since the dot product of \(\vv_i\) with \(\vv_j\) for \(i\neq j\) is zero.%
\end{proof}
One use of \hyperref[x:theorem:thm-fourier-expansion]{Theorem~{\xreffont\ref{x:theorem:thm-fourier-expansion}}} is determining whether or not a given vector is in the span of an orthogonal set. If it is in the span, then its coefficients must satisfy the Fourier expansion formula. Therefore, if we compute the right hand side of the above formula and do not get our original vector, then that vector must not be in the span.%
\begin{inlineexercise}{}{x:exercise:ex-test-span}%
Determine whether or not the vectors \(\vv=(1,-4,3,-11), \ww=(3,1,-4,2)\) belong to the span of the vectors \(\xx_1=(1,0,1,0), \xx_2=(-1,0,1,1), \xx_3=(1,1,-1,2)\). (We confirmed that these vectors form an orthogonal set in \hyperref[x:exercise:ex-orthogonal-set]{Exercise~{\xreffont\ref{x:exercise:ex-orthogonal-set}}}.)%
\end{inlineexercise}%
The Fourier expansion is especially simple if our basis vectors have norm one, since the denominators in each coefficient disappear. Recall (from elementary linear algebra) that for any nonzero vector \(\vv\), a \emph{unit vector} (that is, a vector of norm one) in the direction of \(\vv\) is given by%
\begin{equation*}
\hat{u} = \frac{1}{\len{\vv}}\vv\text{.}
\end{equation*}
We often say that the vector \(\uu\) is \emph{normalized}. (The convention of using a ``hat'' for unit vectors is common but not universal.)%
\begin{definition}{}{x:definition:def-onb}%
A basis \(B\) of \(\R^n\) is called an \terminology{orthonormal basis} if \(B\) is orthogonal, and all the vectors in \(B\) are unit vectors.%
\end{definition}
\begin{example}{}{g:example:idp80}%
In \hyperref[x:exercise:ex-orthogonal-set]{Exercise~{\xreffont\ref{x:exercise:ex-orthogonal-set}}} we saw that the set%
\begin{equation*}
\{(1,0,1,0), (-1,0,1,1), (1,1,-1,2),(1,-6,-1,2)\}
\end{equation*}
is orthogonal. Since it's orthogonal, it must be independent, and since it's a set of four independent vectors in \(\R^4\), it must be a basis. To get an orthonormal basis, we normalize each vector:%
\begin{align*}
\hat{u}_1 \amp = \frac{1}{\sqrt{1^2+0^2+1^2+0^2}}(1,0,1,0) = \frac{1}{\sqrt{2}}(1,0,1,0)\\
\hat{u}_2 \amp = \frac{1}{\sqrt{(-1)^2+0^2+1^2+1^2}}(-1,0,1,1,) = \frac{1}{\sqrt{3}}(-1,0,1,1)\\
\hat{u}_3 \amp = \frac{1}{\sqrt{1^2+1^2+(-1)^2+2^2}}(1,1,-1,2) = \frac{1}{\sqrt{7}}(1,1,-1,2)\\
\hat{u}_4 \amp = \frac{1}{\sqrt{1^2+(-6)^2+(-1)^2+2^2}}(1,-6,-1,2) = \frac{1}{\sqrt{42}}(1,-6,-1,2)\text{.}
\end{align*}
The set \(\{\hat{u}_1,\hat{u}_2,\hat{u}_3,\hat{u}_4\}\) is then an orthonormal basis of \(\R^4\).%
\end{example}
The process of creating unit vectors does typically introduce square root coefficients in our vectors. This can seem undesirable, but there remains value in having an orthonormal basis. For example, suppose we wanted to write the vector \(\vv=(3,5,-1,2)\) in terms of our basis. We can quickly compute%
\begin{align*}
\vv\dotp\hat{u}_1 \amp = \frac{3}{\sqrt{2}}-\frac{1}{\sqrt{2}}=\sqrt{2}\\
\vv\dotp\hat{u}_2 \amp = -\frac{3}{\sqrt{3}}-\frac{1}{\sqrt{3}}+\frac{2}{\sqrt{3}}=-\frac{2}{\sqrt{3}}\\
\vv\dotp\hat{u}_3 \amp = \frac{3}{\sqrt{7}}+\frac{5}{\sqrt{7}}+\frac{1}{\sqrt{7}}+\frac{4}{\sqrt{7}} = \frac{11}{\sqrt{7}}\\
\vv\dotp\hat{u}_4 \amp = \frac{3}{\sqrt{42}}-\frac{30}{\sqrt{42}}+\frac{1}{\sqrt{42}}+\frac{4}{\sqrt{42}} = -\frac{22}{\sqrt{42}}\text{,}
\end{align*}
and so%
\begin{equation*}
\vv = \sqrt{2}\hat{u}_1-\frac{2}{\sqrt{3}}\hat{u}_2+\frac{11}{\sqrt{7}}\hat{u}_3-\frac{22}{\sqrt{42}}\hat{u}_4\text{.}
\end{equation*}
There's still work to be done, but it is comparatively simpler than solving the corresponding system of equations.%
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 3.2 Orthogonal Projection}
\typeout{************************************************}
%
\begin{sectionptx}{Orthogonal Projection}{}{Orthogonal Projection}{}{}{x:section:sec-ortho-projection}
\begin{introduction}{}%
In \hyperref[x:exercise:ex-test-span]{Exercise~{\xreffont\ref{x:exercise:ex-test-span}}}, we saw that \hyperref[x:theorem:thm-fourier-expansion]{Fourier expansion theorem} gives us an efficient way of testing whether or not a given vector belongs to the span of an orthogonal set. When the answer is ``no'', the quantity we compute while testing turns out to be very useful: it gives the \emph{orthogonal projection} of that vector onto the span of our orthogonal set. This turns out to be exactly the ingredient needed to solve certain minimum distance problems.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection 3.2.1 The Gram-Schmidt Procedure}
\typeout{************************************************}
%
\begin{subsectionptx}{The Gram-Schmidt Procedure}{}{The Gram-Schmidt Procedure}{}{}{g:subsection:idp81}
You may recall the following from elementary linear algebra, or vector calculus. Given an nonzero vector \(\uu\) and a vector \(\vv\), the \emph{projection} of \(\vv\) onto \(\uu\) is given by%
\begin{equation*}
\proj{\uu}{\vv} = \left(\frac{\vv\dotp\uu}{\len{\uu}^2}\right)\uu\text{.}
\end{equation*}
Note that this looks just like one of the terms in \hyperref[x:theorem:thm-fourier-expansion]{Fourier expansion theorem}. Recall also that the vector \(\vv-\proj{\uu}{\vv}\) is orthogonal to \(\uu\). Our next result is a generalization of this observation.%
\begin{theorem}{Orthogonal Lemma.}{}{x:theorem:thm-orthogonal-lemma}%
Let \(\{\vv_1,\vv_2,\ldots, \vv_m\}\) be an orthogonal set of vectors in \(\R^n\), and let \(\xx\) be any vector in \(\R^n\). Define the vector \(\vv_{m+1}\) by%
\begin{equation*}
\vv_{m+1} = \xx-\left(\frac{\xx\dotp\vv_1}{\len{\vv_1}^2}\vv_1+\cdots + \frac{\xx\dotp\vv_m}{\len{\vv_m}^2}\vv_m\right)\text{.}
\end{equation*}
Then:%
\begin{enumerate}
\item{}\(\vv_{m+1}\dotp \vv_i = 0\) for each \(i=1,\ldots, m\).%
\item{}If \(\xx\notin\spn\{\vv_1,\ldots, \vv_m\}\), then \(\vv_{m+1}\neq \mathbf{0}\), and therefore, \(\{\vv_1,\ldots, \vv_m,\vv_{m+1}\}\) is an orthogonal set.%
\end{enumerate}
%
\end{theorem}
\begin{proof}{}{g:proof:idp82}
%
\begin{enumerate}
\item{}For any \(i=1,\ldots m\), we have%
\begin{equation*}
\vv_{m+1}\dotp\vv_i = \xx\dotp\vv_i - \frac{\xx\dotp\vv_i}{\len{\vv_i}^2}(\vv_i\dotp\vv_i)=0\text{,}
\end{equation*}
since \(\vv_i\dotp\vv_j = 0\) for \(i\neq j\).%
\item{}It follows from the \hyperref[x:theorem:thm-fourier-expansion]{Fourier expansion theorem} that \(\vv_{m+1}=\mathbf{0}\) if and only if \(\xx\in\spn\{\vv_1,\ldots, \vv_m\}\), and the fact that \(\{\vv_1,\ldots, \vv_m,\vv_{m+1}\}\) is an orthogonal set then follows from the first part.%
\end{enumerate}
%
\end{proof}
It follows from the \hyperref[x:theorem:thm-orthogonal-lemma]{Orthogonal Lemma} that for any subspace \(U\subseteq \R^n\), any set of orthogonal vectors in \(U\) can be extended to an orthogonal basis of \(U\). Since any set containing a single nonzero vector is orthogonal, it follows that every subspace has an orthogonal basis. (If \(U=\{\mathbf{0}\}\), we consider the empty basis to be orthogonal.)%
\par
The procedure for creating an orthogonal basis is clear. Start with a single nonzero vector \(\vv_1\in U\). If \(U\neq \spn\{\vv_1\}\), choose a vector \(\xx_1\in U\) with \(\xx_1\notin\spn\{\vv_1\}\). The \hyperref[x:theorem:thm-orthogonal-lemma]{Orthogonal Lemma} then provides us with a vector%
\begin{equation*}
\vv_2 = \xx_1-\frac{\xx_1\dotp\vv_1}{\len{\vv_1}^2}\vv_1
\end{equation*}
such that \(\{\vv_1,\vv_2\}\) is orthogonal. If \(U=\spn\{\vv_1,\vv_2\}\), we're done. Otherwise, we repeat the process, choosing \(\xx_2\notin\spn\{\vv_1,\vv_2\}\), and then using the \hyperref[x:theorem:thm-orthogonal-lemma]{Orthogonal Lemma} to obtain \(\vv_3\), and so on, until an orthogonal basis is obtained.%
\par
With one minor modification, the above procedure provides us with a major result. Suppose \(U\) is a subspace of \(\R^n\), and start with \emph{any} basis \(\{\xx_1,\ldots, \xx_m\}\) of \(U\). By choosing our \(\xx_i\) in the procedure above to be these basis vectors, we obtain the \emph{Gram-Schmidt algorithm} for constructing an orthogonal basis.%
\begin{theorem}{Gram-Schmidt Orthonormalization Algorithm.}{}{x:theorem:thm-gram-schmidt}%
Let \(U\) be a subspace of \(\R^n\), and let \(\{\xx_1,\ldots, \xx_m\}\) be a basis of \(U\). Define vectors \(\vv_1,\ldots, \vv_m\) in \(U\) as follows:%
\begin{align*}
\vv_1 \amp = \xx_1 \\
\vv_2 \amp = \xx_2 - \frac{\xx_2\dotp\vv_1}{\len{\vv_1}^2}\vv_1\\
\vv_3 \amp = \xx_3 - \frac{\xx_3\dotp\vv_1}{\len{\vv_1}^2}\vv_1-\frac{\xx_3\dotp\vv_2}{\len{\vv_2}^2}\vv_2\\
\vdots \amp \\
\vv_m \amp = \xx_m - \frac{\xx_m\dotp\vv_1}{\len{\vv_1}^2}\vv_1-\cdots - \frac{\xx_m\dotp\vv_{m-1}}{\len{\vv_{m-1}}^2}\vv_{m-1}\text{.}
\end{align*}
Then \(\{\vv_1,\ldots, \vv_m\}\) is an orthogonal basis for \(U\). Moreover, for each \(k=1,2,\ldots, m\), we have%
\begin{equation*}
\spn\{\vv_1,\ldots, \vv_k\} = \spn\{\xx_1,\ldots, \xx_k\}\text{.}
\end{equation*}
%
\end{theorem}
Of course, once we've used Gram-Schmidt to find an orthogonal basis, we can normalize each vector to get an orthonormal basis. The Gram-Schmidt algorithm is ideal when we know how to find \emph{a} basis for a subspace, but we need to know an orthogonal basis. For example, suppose we want an orthonormal basis for the nullspace of the matrix%
\begin{equation*}
A = \bbm 2 \amp -1 \amp 3 \amp 0 \amp 5\\0 \amp 2 \amp -3  \amp 1 \amp 4\\ -4 \amp 2 \amp -6 \amp 0 \amp -10\\ 2 \amp 1 \amp 0 \amp 1 \amp 9\ebm\text{.}
\end{equation*}
First, we find \emph{any} basis for the nullspace.%
\begin{sageinput}
from sympy import Matrix, init_printing
init_printing()
A = Matrix([[2,-1,3,0,5],
            [0,2,-3,1,4],
            [-4,2,-6,0,-10],
            [2,1,0,1,9]])
A.nullspace()
\end{sageinput}
\begin{sageoutput}
\[\left[\bbm -\frac34\\ \frac32\\1\\0\\0\ebm, \bbm -\frac14\\ -\frac12\\0\\1\\0\ebm, \bbm -\frac72\\-2\\0\\0\\1\ebm\right]\]
\end{sageoutput}
Let's make that basis look a little nicer by using some scalar multiplication to clear fractions.%
\begin{equation*}
B=\left\{\xx_1=\bbm 3\\-6\\-4\\0\\0\ebm, \xx_2=\bbm 1\\2\\0\\-4\\0\ebm, \xx_3=\bbm 7\\4\\0\\0\\-2\ebm\right\}
\end{equation*}
Definitely not an orthogonal basis. So we take \(\vv_1=\xx_1\), and%
\begin{align*}
\vv_2 \amp = \xx_2-\left(\frac{\xx_2\dotp\vv_1}{\len{\vv_1}^2}\right)\vv_1\\
\amp = \bbm 1\\2\\0\\-4\\0\ebm -\frac{-9}{61}\bbm 3\\-6\\-4\\-0\\0\ebm \text{,}
\end{align*}
which equals something I'm not sure I want to try to simplify. Finally, we find%
\begin{equation*}
\vv_3 = \xx_3-\left(\frac{\xx_3\dotp \vv_1}{\len{\vv_1}^2}\right)\vv_1-\left(\frac{\xx_3\dotp\vv_2}{\len{\vv_2}^2}\right)\vv_2\text{.}
\end{equation*}
And now you probably get about five minutes into the fractions and say something that shouldn't appear in print. This sounds like a job for the computer.%
\begin{sageinput}
from sympy import GramSchmidt
B = A.nullspace()
GramSchmidt(B)
\end{sageinput}
\begin{sageoutput}
\[\left[\bbm -\frac34\\ \frac32\\1\\0\\0\ebm, \bbm -\frac{22}{61}\\-\frac{17}{61}\\ \frac{9}{61}\\1\\0\ebm, \bbm -\frac{76}{25}\\-\frac{36}{25}\\-\frac{3}{25}\\-\frac{37}{25}\\1\ebm\right]\]
\end{sageoutput}
Oh wait, you wanted that normalized? Turns out the \mono{GramSchmidt} function has an optional argument of true or false. The default is false, which is to not normalize. Setting it to true gives an orthonormal basis:%
\begin{sageinput}
GramSchmidt(B,true)
\end{sageinput}
\begin{sageoutput}
\[\left[\bbm -\frac{3\sqrt{61}}{61}\\ \frac{6\sqrt{61}}{61}\\ \frac{4\sqrt{61}}{61}\\0\\0\ebm,
\bbm -\frac{22\sqrt{183}}{915}\\-\frac{17\sqrt{183}}{915}\\ \frac{3\sqrt{183}}{305}\\ \frac{\sqrt{183}}{15}\\0\ebm,
\bbm -\frac{76\sqrt{3}}{165}\\-\frac{12\sqrt{3}}{55}\\-\frac{\sqrt{3}}{55}\\-\frac{37\sqrt{3}}{165}\\ \frac{5\sqrt{3}}{33}\ebm\right]\]
\end{sageoutput}
OK, so that's nice, and fairly intimidating looking. Did it work? We can specify the vectors in our list by giving their positions, which are 0, 1, and 2, respectively.%
\begin{sageinput}
L=GramSchmidt(B)
L[0],L[1],L[2]
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm -\frac34\\ \frac32\\1\\0\\0\ebm, \bbm -\frac{22}{61}\\-\frac{17}{61}\\ \frac{9}{61}\\1\\0\ebm, \bbm -\frac{76}{25}\\-\frac{36}{25}\\-\frac{3}{25}\\-\frac{37}{25}\\1\ebm\right)\]
\end{sageoutput}
Let's compute dot products:%
\begin{sageinput}
L[0].dot(L[1]),L[1].dot(L[2]),L[0].dot(L[2])
\end{sageinput}
\begin{sageoutput}
\[(0,0,0)\]
\end{sageoutput}
Let's also confirm that these are indeed in the nullspace.%
\begin{sageinput}
A*L[0],A*L[1],A*L[2]
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 0\\0\\0\\0\ebm, \bbm 0\\0\\0\\0\ebm, \bbm 0\\0\\0\\0\ebm\right)\]
\end{sageoutput}
Boom. Let's try another example. This time we'll keep the vectors a little smaller in case you want to try it by hand.%
\begin{inlineexercise}{}{g:exercise:idp83}%
Confirm that the set \(B=\{(1,-2,1), (3,0,-2), (-1,1,2)\}\) is a basis for \(\R^3\), and use the \hyperref[x:theorem:thm-gram-schmidt]{Gram-Schmidt Orthonormalization Algorithm} to find an orthonormal basis.%
\end{inlineexercise}%

\parmarginbox{%
\begin{aside}{}{g:aside:idp84}%
You'll notice that I used \(6\vv_2\) rather than \(\vv_2\) in the calculation of \(\vv_3\). This lets me avoid fractions (momentarily), and doesn't affect the answer, since for any nonzero scalar \(c\),%
\begin{align*}
\left(\frac{c\vv\dotp \xx}{\len{c\vv}^2}\right)\amp(c\vv)\\
\amp= \left(\frac{c(\vv\dotp\xx)}{c^2\len{\vv}^2}\right)(c\vv)\\
\amp=\left(\frac{\vv\dotp\xx}{\len{\vv^2}}\right)\vv\text{.}
\end{align*}
%
\end{aside}
}{0pt}%

\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection 3.2.2 Projections}
\typeout{************************************************}
%
\begin{subsectionptx}{Projections}{}{Projections}{}{}{x:subsection:pars-projections}
We hinted above that the calculations we've been doing have a lot to do with projection. Since any single nonzero vector forms an orthogonal basis for its span, the projection%
\begin{equation*}
\proj{\uu}{\vv}=\left(\frac{\uu\dotp\vv}{\len{\uu}^2}\right)\uu
\end{equation*}
can be viewed as the orthogonal projection of the vector \(\vv\), not onto the vector \(\uu\), but onto the subspace \(\spn\{\uu\}\). This is, after all, how we viewed projections in elementary linear algebra: we drop the perpendicular from the tip of \(\vv\) onto the \emph{line} in the direction of \(\uu\).%
\par
Now that we know how to define an orthogonal basis for a subspace, we can define orthogonal projection onto subspaces of dimension greater than one.%
\begin{definition}{}{x:definition:def-ortho-projection}%
Let \(U\) be a subspace of \(\R^n\) with orthogonal basis \(\{\uu_1,\ldots, \uu_k\}\). For any vector \(\vv\in \R^n\), we define the \terminology{orthogonal projection} of \(\vv\) onto \(U\) by%
\begin{equation*}
\proj{U}{\vv} = \sum_{i=1}^k\left(\frac{\uu_i\dotp\vv}{\len{\uu_i}^2}\right)\uu_i\text{.}
\end{equation*}
%
\end{definition}
Note that \(\proj{U}{\vv}\) is indeed an element of \(U\), since it's a linear combination of its basis vectors. In the case of the trivial subspace \(U=\{\mathbf{0}\}\), we define orthogonal projection of any vector to be \(\mathbf{0}\), since really, what other choice do we have? (This case isn't really of any interest, we just like being thorough.)%
\par
Let's see how this might be put to use in a classic problem: finding the distance from a point to a plane.%

\parmarginbox{%
\begin{aside}{}{g:aside:idp85}%
One limitation of this approach to projection is that we must project onto a \emph{subspace}. Given a plane like \(x-2y+4z=4\), we would need to modify our approach. One way to do it would be to find a point on the plane, and then try to translate everything to the origin. It's interesting to think about how this might be accomplished (in particular, in what direction would the translation have to be performed?) but someone external to the questions we're interested in here.%
\end{aside}
}{0pt}%

\begin{example}{}{g:example:idp86}%
Find the distance from the point \((3,1,-2)\) to the plane \(P\) defined by \(x-2y+4z=0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution 1} (Using projection onto a normal vector).\hypertarget{g:solution:idp87}{}\quad{}In an elementary linear algebra (or calculus) course, we would solve this problem as follows. First, we would need two vectors parallel to the plane. If \(\bbm x\\y\\z\ebm\) lies in the plane, then \(x-2y+4z=0\), so \(x=2y-4z\), and%
\begin{equation*}
\bbm x\\y\\z\ebm = \bbm 2y-4z\\y\\z\ebm = y\bbm 2\\1\\0\ebm + z\bbm -4\\0\\1\ebm\text{,}
\end{equation*}
so \(\uu=\bbm 2\\1\\0\ebm\) and \(\vv\bbm -4\\0\\1\ebm\) are parallel to the plane. We then compute the normal vector%
\begin{equation*}
\mathbf{n}=\uu\times\vv=\bbm 1\\-2\\4\ebm\text{,}
\end{equation*}
and compute the projection of the position vector \(\mathbf{p}=\bbm 3,1,-2\ebm\) for the point \(P=(3,1,-2)\) onto \(\mathbf{n}\). This gives the vector%
\begin{equation*}
\xx = \left(\frac{\mathbf{p}\dotp\mathbf{n}}{\len{\mathbf{n}}^2}\right)\mathbf{n} = \frac{-7}{21}\bbm 1\\-2\\4\ebm =\bbm-1/3\\2/3\\-4/3\ebm\text{.}
\end{equation*}
%
\par
Now, this vector is \emph{parallel} to \(\mathbf{n}\), so it's perpendicular to the plane. Subtracting it from \(\mathbf{p}\) gives a vector parallel to the plane, and this is the position vector for the point we seek.%
\begin{equation*}
\mathbf{q}=\mathbf{p}-\xx=\bbm 3\\1\\-2\ebm-\bbm -1/3\\-2/3\\-4/3\ebm = \bbm 10/3\\1/3\\-2/3\ebm
\end{equation*}
so the closest point is \(Q=\bigl(\frac{10}{3},\frac13,-\frac{2}{3}\bigr)\). We weren't asked for it, but note that if we wanted the distance from the point \(P\) to the plane, this is given by \(\len{\xx}=\frac13\sqrt{21}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution 2} (Using orthogonal projection).\hypertarget{g:solution:idp88}{}\quad{}Let's solve the same problem using orthogonal projection. First, we have to deal with the fact that the vectors \(\uu\) and \(\vv\) are probably not orthogonal. To get around this, we replace \(\vv\) with%
\begin{equation*}
\ww = \vv-\left(\frac{\vv\dotp\uu}{\len{\uu}^2}\right)\uu = \bbm -4\\0\\1\ebm+\frac 85\bbm 2\\1\\0\ebm = \bbm -4/5\\8/5\\1\ebm\text{.}
\end{equation*}
We now set%
\begin{align*}
\mathbf{q} \amp =\left(\frac{\mathbf{p}\dotp\uu}{\len{\uu}^2}\right)\uu-\left(\frac{\mathbf{p}\dotp\ww}{\len{\ww}^2}\right)\ww\\
\amp = \frac{7}{5}\bbm 2\\1\\0\ebm +\frac{-14}{105}\bbm -4\\8\\5\ebm \\
\amp = \bbm 10/3\\1/3\\-2/3\ebm\text{.}
\end{align*}
Lo and behold, we get the same answer as before.%
\end{example}
The only problem with \hyperref[x:definition:def-ortho-projection]{Definition~{\xreffont\ref{x:definition:def-ortho-projection}}} is that it appears to depend on the choice of orthogonal basis. To see that it doesn't, we need one more definition.%
\begin{definition}{}{x:definition:def-ortho-comp}%
For any subspace \(U\) of \(\R^n\), we define the \terminology{orthogonal complement} of \(U\), denoted \(U^\bot\), by%
\begin{equation*}
U^\bot = \{\xx\in\R^n \,|\, \xx\dotp\yy = 0 \text{ for all } \yy\in U\}\text{.}
\end{equation*}
%
\end{definition}
The term ``complement'' comes from terminology we mentioned early on, but didn't spend much time on. \hyperref[x:theorem:thm-construct-complement]{Theorem~{\xreffont\ref{x:theorem:thm-construct-complement}}} told us that for any subspace \(U\) of a vector space \(V\), it is possible to construct another subspace \(W\) of \(V\) such that \(V = U\oplus W\). The subspace \(W\) is known as a complement of \(U\). A complement is not unique, but the orthogonal complement is. As you might guess from the name, \(U^\bot\) is also a subspace of \(\R^n\).%
\begin{inlineexercise}{}{g:exercise:idp89}%
Show that \(U^\bot\) is a subspace of \(\R^n\).%
\end{inlineexercise}%
\begin{theorem}{Projection Theorem.}{}{x:theorem:thm-projection}%
Let \(U\) be a subspace of \(\R^n\), let \(\xx\) be any vector in \(\R^n\), and let \(\mathbf{p}=\proj{U}{\xx}\). Then:%
\begin{enumerate}
\item{}\(\mathbf{p}\in U\), and \(\xx-\mathbf{p}\in U^\bot\).%
\item{}\(\mathbf{p}\) is the \emph{closest} vector in \(U\) to the vector \(\xx\), in the sense that the distance \(d(\mathbf{p},\xx)\) is minimal among all vectors in \(U\). That is, for all \(\uu\neq \mathbf{p}\in U\), we have%
\begin{equation*}
\len{\xx-\mathbf{p}}\lt\len{\xx-\uu}\text{.}
\end{equation*}
%
\end{enumerate}
%
\end{theorem}
\begin{inlineexercise}{}{g:exercise:idp90}%
Show that \(U\cap U^\bot = \{\mathbf{0}\}\). Use this fact to show that \hyperref[x:definition:def-ortho-projection]{Definition~{\xreffont\ref{x:definition:def-ortho-projection}}} does not depend on the choice orthogonal basis.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp91}{}\quad{}Suppose we find vectors \(\mathbf{p}\) and \(\mathbf{p}'\) using basis \(B\) and \(B'\). Note that \(\mathbf{p}-\mathbf{p}'\in U\), but also that%
\begin{equation*}
\mathbf{p}-\mathbf{p}' = (\mathbf{p}-\xx)-(\mathbf{p}'-\xx)
\end{equation*}
Now use \hyperref[x:theorem:thm-projection]{Theorem~{\xreffont\ref{x:theorem:thm-projection}}}%
\end{inlineexercise}%
Finally, we note one more useful fact. The process of sending a vector to its orthogonal projection defines an operator on \(\R^n\), and yes, it's linear.%
\begin{theorem}{}{}{x:theorem:thm-proj-operator}%
Let \(U\) be a subspace of \(\R^n\), and define a function \(P_U:\R^n\to \R^n\) by%
\begin{equation*}
P_U(\xx) = \proj{U}{\xx} \text{ for any } \xx\in\R^n\text{.}
\end{equation*}
Then \(T\) is a linear operator such that \(U=\im P_U\) and \(U^\bot = \ker P_U\).%
\end{theorem}
Note: it follows from this result and the \hyperref[x:theorem:thm-dimension-lintrans]{Dimension Theorem} that%
\begin{equation*}
\dim U + \dim U^\bot = n\text{,}
\end{equation*}
and since \(U\cap U^\bot = \{\mathbf{0}\}\), \(U^\bot\) is indeed a complement of \(U\) in the sense introduced in \hyperref[x:theorem:thm-construct-complement]{Theorem~{\xreffont\ref{x:theorem:thm-construct-complement}}}. It's also fairly easy to see that \(\dim U + \dim U^\bot = n\) directly. If \(\ww\in U^\bot\), and \(\{\uu_1,\ldots, \uu_k\}\) is a basis for \(U\), then we have%
\begin{equation*}
\ww\dotp \uu_1= 0, \ldots, \ww\dotp \uu_k=0\text{,}
\end{equation*}
and for an unknown \(\ww\), this is simply a homogeneous system of \(k\) equations with \(n\) variables. Moreover, they are \emph{independent} equations, since the \(\uu_i\) form a basis. We thus expect \(n-k\) free parameters in the general solution.%
\begin{inlineexercise}{}{g:exercise:idp92}%
Let \(U = \{(a-b+3c, 2a+b, 3c, 4a-b+3c,a-4c)\,|\, a,b,c\in\R\}\subseteq \R^5\). Determine a basis for \(U^\bot\).%
\end{inlineexercise}%
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Worksheet 3.3 Worksheet: dual basis.}
\typeout{************************************************}
%
\newgeometry{left=1.25cm, right=1.25cm, top=1.25cm, bottom=1.25cm}
\begin{worksheet-section}{Worksheet: dual basis.}{}{Worksheet: dual basis.}{}{}{x:worksheet:worksheet-dual-basis}
Let \(V\) be a vector space over \(\R\). (That is, scalars are real numbers, rather than, say, complex.) A linear transformation \(\phi:V\to \R\) is called a \terminology{linear functional}.%
\par
Here are some examples of linear functionals:%
\begin{itemize}[label=\textbullet]
\item{}The map \(\phi:\R^3\to \R\) given by \(\phi(x,y,z) = 3x-2y+5z\).%
\item{}The evaluation map \(ev_a:P_n(\R)\to \R\) given by \(ev_a(p) = p(a)\). (For example, \(ev_2(3-4x+5x^2) = 2-4(2)+5(2^2) = 14\).)%
\item{}The map \(\phi:\mathcal{C}[a,b]\to \R\) given by \(\phi(f) = \int_a^b f(x)\,dx\), where \(\mathcal{C}[a,b]\) denotes the space of all continuous functions on \([a,b]\).%
\end{itemize}
%
\par
Note that for any vector spaces \(V,W\), the set \(\mathcal{L}(V,W)\) of linear transformations from \(V\) to \(W\) is itself a vector space, if we define%
\begin{equation*}
(S+T)(v) = S(v)+T(v),\quad \text{ and } \quad (kT)(v)=k(T(v))\text{.}
\end{equation*}
In particular, given a vector space \(V\), we denote the set of all linear functionals on \(V\) by \(V^*=\mathcal{L}(V,\R)\), and call this the \emph{dual space} of \(V\).%
\par
We make the following observations:%
\begin{itemize}[label=\textbullet]
\item{}If \(\dim V=n\) and \(\dim W=m\), then \(\mathcal{L}(V,W)\) is isomorphic to the space \(M_{mn}\) of \(m\times n\) matrices, so it has dimension \(mn\).%
\item{}Since \(\dim \R=1\), if \(V\) is finite-dimensional, then \(V^*=\mathcal{L}(V,\R)\) has dimension \(1n=n\).%
\item{}Since \(\dim V^*=\dim V\), \(V\) and \(V^*\) are isomorphic.%
\end{itemize}
%
\par
Here is a basic example that is intended as a guide to your intuition regarding dual spaces. Take \(V = \R^3\). Given any \(v\in V\), define a map \(\phi_{v}:V\to \R\) by \(\phi_{v}(w) = v\dotp w\) (the usual dot product).%
\par
One way to think about this: if we write \(v\in V\) as a column vector \(\bbm v_1\\v_2\\v_3\ebm\), then we can identify \(\phi_{v}\) with \(v^T\), where the action is via multiplication:%
\begin{equation*}
\phi_{v}(w) = \bbm v_1\amp v_2\amp v_3\ebm\bbm w_1\\w_2\\w_3\ebm = v_1w_1+v_2w_2+v_3w_3\text{.}
\end{equation*}
It turns out that this example can be generalized, but the definition of \(\phi_v\) involves the dot product, which is particular to \(\R^n\).%
\par
There is a generalization of the dot product, known as an inner product. (See Chapter 10 of Nicholson, for example.) On any inner product space, we can associate each vector \(v\in V\) to a linear functional \(\phi_v\) using the procedure above.%
\par
Another way to work concretely with dual vectors (without the need for inner products) is to define things in terms of a basis.%
\par
Given a basis \(\{v_1,v_2,\ldots, v_n\}\) of \(V\), we define the corresponding \terminology{dual basis} \(\{\phi_1,\phi_2,\ldots, \phi_n\}\) of \(V^*\) by%
\begin{equation*}
\phi_i(v_j) = \begin{cases} 1, \amp \text{ if } i=j\\ 0, \amp \text{ if } i\neq j\end{cases}\text{.}
\end{equation*}
Note that each \(\phi_j\) is well-defined, since any linear transformation can be defined by giving its values on a basis.%
\par
For the standard basis on \(\R^n\), note that the corresponding dual basis functionals are given by%
\begin{equation*}
\phi_j(x_1,x_2,\ldots, x_n) = x_j\text{.}
\end{equation*}
That is, these are the \emph{coordinate functions} on \(\R^n\).%
\clearpage
\begin{divisionexercise}{1}{}{2.5in}{g:exercise:idp93}%
Show that the dual basis is indeed a basis for \(V^*\).%
\end{divisionexercise}%
Next, let \(V\) and \(W\) be vector spaces, and let \(T:V\to W\) be a linear transformation. For any such \(T\), we can define the \emph{dual map} \(T^*:W^*\to V^*\) by \(T^*(\phi) = \phi\circ T\) for each \(\phi\in W^*\).%
\begin{divisionexercise}{2}{}{2.5in}{g:exercise:idp94}%
Confirm that (a) \(T^*(\phi)\) does indeed define an element of \(V^*\); that is, a linear map from \(V\) to \(\R\), and (b) that \(T^*\) is linear.%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{2.5in}{g:exercise:idp95}%
Let \(V=P(\R)\) be the space of all polynomials, and let \(D:V\to V\) be the derivative transformation \(D(p(x))=p'(x)\). Let \(\phi:V\to \R\) be the linear functional defined by \(\phi(p(x)) = \int_0^1 p(x)\,dx\).%
\par
What is the linear functional \(D^*(\phi)\)?%
\end{divisionexercise}%
\clearpage
\begin{divisionexercise}{4}{}{3in}{g:exercise:idp96}%
Show that dual maps satisfy the following properties: for any \(S,T\in \mathcal{L}(V,W)\) and \(k\in \R\),%
\begin{enumerate}[label=(\alph*)]
\item{}\(\displaystyle (S+T)^* = S^*+T^*\)%
\item{}\(\displaystyle (kS)^* = kS^*\)%
\item\hypertarget{x:li:list-property-last}{}\(\displaystyle (ST)^* = T^*S^*\)%
\end{enumerate}
%
\par
In item \hyperlink{x:li:list-property-last}{Item~{\xreffont 3.3.4.c}}, assume \(S\in \mathcal{L}(V,W)\) and \(T\in \mathcal{L}(U,V)\). (Reminder: the notation \(ST\) is sometimes referred to as the ``product'' of \(S\) and \(T\), in analogy with matrices, but actually represents the composition \(S\circ T\).)%
\end{divisionexercise}%
We have one topic remaining in relation to dual spaces: determining the kernel and image of a dual map \(T^*\) (in terms of the kernel and image of \(T\)). Let \(V\) be a vector space, and let \(U\) be a subspace of \(V\). Any such subspace determines an important subspace of \(V^*\): the \terminology{annihilator} of \(U\), denoted by \(U^0\) and defined by%
\begin{equation*}
U^0 = \{\phi\in V^* \,|\, \phi(u)=0 \text{ for all } u\in U\}\text{.}
\end{equation*}
%
\begin{divisionexercise}{5}{}{3in}{g:exercise:idp97}%
Determine a basis (in terms of the standard dual basis for \((\R^4)^*\)) for the annihilator \(U^0\) of the subspace \(U\subseteq \R^4\) given by%
\begin{equation*}
U = \{(2a+b,3b,a,a-2b)\,|\, a,b\in\R\}\text{.}
\end{equation*}
%
\end{divisionexercise}%
\clearpage
Here is a fun theorem about annihilators that I won't ask you to prove.%
\begin{theorem}{}{}{x:theorem:thm-anihilator-dimension}%
Let \(V\) be a finite dimensional vector space. For any subspace \(U\) of \(V\),%
\begin{equation*}
\dim U + \dim U^0 = \dim V\text{.}
\end{equation*}
%
\end{theorem}
Here's an outline of the proof. For any subspace \(U\subseteq V\), we can define the \emph{inclusion} map \(i:U\to V\), given by \(i(u)=u\). (This is not the identity on \(V\) since it's only defined on \(U\). In particular, it is not onto unless \(U=V\), although it is clearly one-to-one.)%
\par
Then \(i^*\) is a map from \(V^*\) to \(U^*\). Moreover, note that for any \(\phi\in V^*\), \(i^*(\phi)\in U^*\) satisfies, for any \(u\in U\),%
\begin{equation*}
i^*(\phi)(u) = \phi(i(u))=\phi(u)\text{.}
\end{equation*}
Thus, \(\phi\in \ker i^*\) if and only if \(i^*(\phi)=0\), which is if and only if \(\phi(u)=0\) for all \(u\in U\), which is if and only if \(\phi\in U^0\). Therefore, \(\ker i^* = U^0\).%
\par
By the dimension theorem, we have:%
\begin{equation*}
\dim V^* = \dim \ker i^* + \dim \operatorname{im} i^*\text{.}
\end{equation*}
With a bit of work, one can show that \(\operatorname{im} i^* = U^*\), and we get the result from the fact that \(\dim V^*=\dim V\) and \(\dim U^* = \dim U\).%
\par
There are a number of interesting results of this flavour. For example, one can show that a map \(T\) is injective if and only if \(T^*\) is surjective, and vice-versa.%
\par
One final, optional task: return to the example of \(\R^n\), viewed as column vectors, and consider a matrix transformation \(T_A:\R^n\to \R^m\) given by \(T_A(\vec{x}) = A\vec{x}\) as usual. Viewing \((\R^n)^*\) as row vectors, convince yourself that \((T_A)^* = T_{A^T}\); that is, that what we've really been talking about all along is just the transpose of a matrix!%
\end{worksheet-section}
\restoregeometry
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 4 Diagonalization}
\typeout{************************************************}
%
\begin{chapterptx}{Diagonalization}{}{Diagonalization}{}{}{x:chapter:ch-diagonalization}
\begin{introduction}{}%
In this chapter we look at the diagonalization problem for real symmetric matrices. You probably saw how to compute eigenvalues and eigenvectors in your elementary linear algebra course. You may have also seen that in some cases, the number of independent eigenvectors associated to an \(n\times n\) matrix \(A\) is \(n\), in which case it is possible to ``diagonalize'' \(A\). In other cases, we don't get ``enough'' eigenvectors for diagonalization.%
\par
In the first part of this section, we review some basic facts about eigenvalues and eigenvectors. We will then move on to look at the special case of symmetric matrices, where we will see that it is always possible to diagonalize, and moreover, that it is possible to do so using an orthonormal basis of eigenvectors.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Section 4.1 Eigenvalues and Eigenvectors}
\typeout{************************************************}
%
\begin{sectionptx}{Eigenvalues and Eigenvectors}{}{Eigenvalues and Eigenvectors}{}{}{x:section:subsec-eigen-basics}
\begin{definition}{}{x:definition:def-eigenvalue}%
Let \(A\) be an \(n\times n\) matrix. A number \(\lambda\) is called an \terminology{eigenvalue} of \(A\) if there exists a nonzero vector \(\xx\) such that%
\begin{equation*}
A\xx = \lambda\xx\text{.}
\end{equation*}
Any such vector \(\xx\) is called an \terminology{eigenvector} associated to the eigenvalue \(\lambda\).%
\end{definition}
Note that eigenvalues and eigenvectors can just as easily be defined for a general linear operator \(T:V\to V\). In this context, and eigenvector \(\xx\) is sometimes referred to as a \emph{characteristic vector} (or characteristic direction) for \(T\), since the property \(T(\xx)=\lambda \xx\) simply states that the transformed vector \(T(\xx)\) is parallel to the original vector \(\xx\). Some linear algebra textbooks that focus more on general linear transformations frame this topic in the context of \emph{invariant subspaces} for a linear operator.%
\par
A subspace \(U\subseteq V\) is \emph{invariant} with respect to \(T\) if \(T(\uu)\in U\) for all \(\uu\in U\). Note that if \(\xx\) is an eigenvector of \(T\), then \(\spn\{\xx\}\) is an invariant subspace. To see this, note that if \(T(\xx)=\lambda \xx\) and \(\yy=k\xx\), then%
\begin{equation*}
T(\yy)=T(k\xx)=kT(\xx)=k(\lambda \xx)=\lambda(k\xx)=\lambda\yy\text{.}
\end{equation*}
%
\par
Note that if \(\xx\) is an eigenvector of the matrix \(A\), then we have%
\begin{equation}
(A-\lambda I_n)\xx=\mathbf{0}\text{,}\label{x:men:eq-eigen-null}
\end{equation}
where \(I_n\) denotes the \(n\times n\) identity matrix. Thus, if \(\lambda\) is an eigenvalue of \(A\), any corresponding eigenvector is an element of \(\nll(A-\lambda I_n)\).%
\begin{definition}{}{x:definition:def-eigenspace}%
For any real number \(\lambda\) and matrix \(A\), we define the \terminology{eigenspace} \(E_\lambda(A)\) by%
\begin{equation*}
E_\lambda(A) = \nll (A-\lambda I_n)\text{.}
\end{equation*}
%
\end{definition}
Note that \(E_\lambda(A)\) can be defined for any real number \(\lambda\), whether or not \(\lambda\) is an eigenvalue. However, the eigenvalues of \(A\) are distinguished by the property that there is a \emph{nonzero} solution to \hyperref[x:men:eq-eigen-null]{({\xreffont\ref{x:men:eq-eigen-null}})}. Furthermore, we know that \hyperref[x:men:eq-eigen-null]{({\xreffont\ref{x:men:eq-eigen-null}})} can only have nontrivial solutions if the matrix \(A-\lambda I_n\) is not invertible. We also know that \(A-\lambda I_n\) is non-invertible if and only if \(\det (A-\lambda I_n) = 0\). This gives us the following theorem.%
\begin{theorem}{}{}{x:theorem:thm-eigenspace-nonzero}%
The following are equivalent for any \(n\times n\) matrix \(A\) and real number \(\lambda\):%
\begin{enumerate}
\item{}\(\lambda\) is an eigenvalue of \(A\).%
\item{}\(\displaystyle E_\lambda(A)\neq \{\mathbf{0}\}\)%
\item{}\(\displaystyle \det(A-\lambda I_n) = 0\)%
\end{enumerate}
%
\end{theorem}
The polynomial \(p_A(x)=\det(xI_n -A)\) is called the \terminology{characteristic polynomial} of \(A\). (Note that \(\det(x I_n-A) = (-1)^n\det(A-x I_n)\). We choose this order so that the coefficient of \(x^n\) is always 1.) The equation%
\begin{equation}
\det(xI_n - A) = 0\label{x:men:eq-characteristic}
\end{equation}
is called the \terminology{characteristic equation} of \(A\). The solutions to this equation are precisely the eigenvalues of \(A\).%
\par
Recall that a matrix \(B\) is said to be \terminology{similar} to a matrix \(A\) if there exists an invertible matrix \(P\) such that \(B = P^{-1}AP\). Much of what follows concerns the question of whether or not a given \(n\times n\) matrix \(A\) is \terminology{diagonalizable}.%
\begin{definition}{}{x:definition:def-diagonalizable}%
An \(n\times n\) matrix \(A\) is said to be \terminology{diagonalizable} if \(A\) is similar to a diagonal matrix.%
\end{definition}
The following results will frequently be useful.%
\begin{theorem}{}{}{x:theorem:thm-similar-properties}%
The relation \(A\sim B\) if and only if \(A\) is similar to \(B\) is an equivalence relation. Moreover, if \(A\sim B\), then:%
\begin{itemize}[label=\textbullet]
\item{}\(\displaystyle \det A = \det B\)%
\item{}\(\displaystyle \tr A = \tr B\)%
\item{}\(\displaystyle c_A(x) = c_B(x)\)%
\end{itemize}
In other words, \(A\) and \(B\) have the same determinant, trace, and characteristic polynomial (and thus, the same eigenvalues).%
\end{theorem}
\begin{proof}{}{g:proof:idp98}
The first two follow directly from properties of the determinant and trace. For the last, note that if \(B = P^{-1}AP\), then%
\begin{equation*}
P^{-1}(xI_n-A)P = P^{-1}(xI_n)P-P^{-1}AP = xI_n B\text{,}
\end{equation*}
so \(xI_n-B\sim xI_n-A\), and therefore \(\det(xI_n-B)=\det(xI_n-A)\).%
\end{proof}
\begin{example}{}{g:example:idp99}%
Determine the eigenvalues and eigenvectors of \(A = \bbm 0\amp 1\amp 1\\1\amp 0\amp 1\\1\amp 1\amp 0\ebm\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp100}{}\quad{}We begin with the characteristic polynomial. We have%
\begin{align*}
\det(xI_n - A) \amp =\det\bbm x \amp -1\amp -1\\-1\amp x \amp -1\\-1\amp -1\amp x\ebm\\
\amp = x \begin{vmatrix}x \amp -1\\-1\amp x\end{vmatrix}
+1\begin{vmatrix}-1\amp -1\\-1\amp x\end{vmatrix}
-1\begin{vmatrix}-1\amp x\\-1\amp -1\end{vmatrix}\\
\amp = x(x^2-1)+(-x-1)-(1+x)\\
\amp x(x-1)(x+1)-2(x+1)\\
\amp (x+1)[x^2-x-2]\\
\amp (x+1)^2(x-2)\text{.}
\end{align*}
%
\par
The roots of the characteristic polynomial are our eigenvalues, so we have \(\lambda_1=-1\) and \(\lambda_2=2\). Note that the first eigenvalue comes from a repeated root. This is typically where things get interesting. If an eigenvalue does not come from a repeated root, then there will only be one (independent) eigenvector that corresponds to it. (That is, \(\dim E_\lambda(A)=1\).) If an eigenvalue is repeated, it could have more than one eigenvector, but this is not guaranteed.%
\par
We find that \(A-(-1)I_n = \bbm 1\amp 1\amp 1\\1\amp 1\amp 1\\1\amp 1\amp 1\ebm\), which has reduced row-echelon form \(\bbm 1\amp 1\amp 1\\0\amp 0\amp 0\\0\amp 0\amp 0\ebm\). Solving for the nullspace, we find that there are two independent eigenvectors:%
\begin{equation*}
\xx_{1,1}=\bbm 1\\-1\\0\ebm, \quad \text{ and } \quad \xx_{1,2}=\bbm 1\\0\\-1\ebm\text{,}
\end{equation*}
so%
\begin{equation*}
E_{-1}(A) = \spn\left\{\bbm 1\\-1\\0\ebm, \bbm 1\\0\\-1\ebm\right\}\text{.}
\end{equation*}
%
\par
For the second eigenvector, we have \(A-2I = \bbm -2\amp 1\amp 1\\1\amp -2\amp 1\\1\amp 1\amp -2\ebm\), which has reduced row-echelon form \(\bbm 1\amp 0\amp -1\\0\amp 1\amp -1\\0\amp 0\amp 0\ebm\). An eigenvector in this case is given by%
\begin{equation*}
\xx_2 = \bbm 1\\1\\1\ebm\text{.}
\end{equation*}
%
\end{example}
In general, if the characteristic polynomial can be factored as%
\begin{equation*}
p_A(x)=(x-\lambda)^mq(x)\text{,}
\end{equation*}
where \(q(x)\) is not divisible by \(x-\lambda\), then we say that \(\lambda\) is an eigenvalue of \terminology{multiplicity} \(m\). A main result is the following.%
\begin{theorem}{}{}{x:theorem:thm-multiplicity}%
Let \(\lambda\) be an eigenvalue of \(A\) of multiplicity \(m\). Then \(\dim E_\lambda(A)\leq m\).%
\end{theorem}
Some textbooks refer to the multiplicity \(m\) of an eigenvalue as the \emph{algebraic multiplicity} of \(\lambda\), and the number \(\dim E_\lambda(A)\) as the \emph{geometric multiplicity} of \(\lambda\).%
\par
To prove \hyperref[x:theorem:thm-multiplicity]{Theorem~{\xreffont\ref{x:theorem:thm-multiplicity}}} we need the following lemma, from Section 5.5 of Nicholson's textbook.%
\begin{lemma}{}{}{x:lemma:lem-block-eigen}%
Let \(\{\xx_1,\ldots, \xx_k\}\) be a set of linearly independent eigenvectors of a matrix \(A\), with corresponding eigenvalues \(\lambda_1,\ldots, \lambda_k\) (not necessarily distinct). Extend this set to a basis \(\{\xx_1,\ldots, \xx_k,\xx_{k+1},\ldots, \xx_n\}\), and let \(P=\bbm \xx_1\amp \cdots \amp \xx_n\ebm\) be the matrix whose columns are the basis vectors. (Note that \(P\) is necessarily invertible.) Then%
\begin{equation*}
P^{-1}AP = \bbm \diag(\lambda_1,\ldots, \lambda_k) \amp B\\0\amp A_1\ebm\text{,}
\end{equation*}
where \(B\) has size \(k\times (n-k)\), and \(A_1\) has size \((n-k)\times (n-k)\).%
\end{lemma}
\begin{proof}{}{g:proof:idp101}
We have%
\begin{align*}
P^{-1}AP \amp = P^{-1}A\bbm \xx_1\amp \cdots \amp \xx_n\ebm\\
\amp =\bbm (P^{-1}A)\xx_1\amp \cdots \amp (P^{-1}A)\xx_n\ebm\text{.}
\end{align*}
For \(1\leq i\leq k\), we have%
\begin{equation*}
(P^{-1}A)(\xx_i) = P^{-1}(A\xx_i) = P^{-1}(\lambda_i\xx_i)=\lambda_i(P^{-1}\xx_i)\text{.}
\end{equation*}
But \(P^{-1}\xx_i\) is the \(i\)th column of \(P^{-1}P = I_n\), which proves the result.%
\end{proof}
We can use \hyperref[x:lemma:lem-block-eigen]{Lemma~{\xreffont\ref{x:lemma:lem-block-eigen}}} to prove that \(\dim E_\lambda(A)\leq m\) as follows. Suppose \(\{\xx_1,\ldots, \xx_k\}\) is a basis for \(E_\lambda(A)\). Then this is a linearly independent set of eigenvectors, so our lemma guarantees the existence of a matrix \(P\) such that%
\begin{equation*}
P^{-1}AP = \bbm \lambda I_k \amp B\\0\amp A_1\ebm\text{.}
\end{equation*}
Let \(\tilde{A}=P^{-1}AP\). On the one hand, since \(\tilde{A}\sim A\), we have \(c_A(x)=c_{\tilde{A}}(x)\). On the other hand,%
\begin{equation*}
\det(xI_n-\tilde{A}) = \det\bbm (x-\lambda)I_k \amp -B\\0 \amp xI_{n-k}-A_1\ebm = (x-\lambda)^k\det(xI_{n-k}-A_1)\text{.}
\end{equation*}
This shows that \(c_A(x)\) is divisible by \((x-\lambda)^k\). Since \(m\) is the largest integer such that \(c_A(x)\) is divisible by \((x-\lambda)^m\), we must have \(\dim E_\lambda(A)=k\leq m\).%
\par
\hyperref[x:theorem:thm-multiplicity]{Theorem~{\xreffont\ref{x:theorem:thm-multiplicity}}} provides an initial criterion for diagonalization: if the dimension of each eigenspace \(E_\lambda(A)\) is equal to the multiplicity of \(\lambda\), then \(A\) is diagonalizable. The truth of this statement relies on one additional fact: any set of eigenvectors corresponding to \emph{distinct} eigenvalues is linearly independent. The proof of this fact is a relatively straightforward proof by induction. It can be found in Section 5.5 of Nicholson for those who are interested. However, our focus for the remainder of the section will be on diagonalization of \emph{symmetric} matrices, and soon we will see that for such matrices, eigenvectors corresponding to different eigenvalues are, in fact, \emph{orthogonal}.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 4.2 Diagonalization of symmetric matrices}
\typeout{************************************************}
%
\begin{sectionptx}{Diagonalization of symmetric matrices}{}{Diagonalization of symmetric matrices}{}{}{x:section:subsec-ortho-diag}
Recall that an \(n\times n\) matrix \(A\) is \terminology{symmetric} if \(A^T=A\). Symmetry of \(A\) is equivalent to the following: for any vectors \(\xx,\yy\in\R^n\),%
\begin{equation*}
\xx\dotp (A\yy) = (A\xx)\dotp \yy\text{.}
\end{equation*}
To see that this is implied by the symmetry of \(A\), note that%
\begin{equation*}
\xx\dotp (A\yy) = \xx^T(A\yy)=(\xx^TA^T)\yy = (A\xx)^T\yy=(A\xx)\dotp\yy\text{.}
\end{equation*}
For inner product spaces, the above is taken as the \emph{definition} of what it means for an operator to be symmetric.%
\begin{inlineexercise}{}{g:exercise:idp102}%
Prove that if \(\xx\dotp(A\yy)=(A\xx)\dotp \yy\) for any \(\xx,\yy\in\R^n\), then \(A\) is symmetric.%
\end{inlineexercise}%
A useful property of symmetric matrices, mentioned earlier, is that eigenvectors corresponding to distinct eigenvalues are orthogonal.%
\begin{theorem}{}{}{x:theorem:thm-ortho-eigen-symm}%
If \(A\) is a symmetric matrix, then eigenvectors corresponding to \emph{distinct} eigenvalues are orthogonal.%
\end{theorem}
\begin{proof}{}{g:proof:idp103}
To see this, suppose \(A\) is symmetric, and that we have%
\begin{equation*}
A\xx_1=\lambda_1\xx_1\quad \text{ and } A\xx_2=\lambda_2\xx_2\text{,}
\end{equation*}
with \(\xx_1\neq\mathbf{0},\xx_2\neq \mathbf{0}\), and \(\lambda_1\neq \lambda_2\). We then have, since \(A\) is symmetric, and using the result above,%
\begin{equation*}
\lambda_1(\xx_1\dotp \xx_2) = (\lambda_1\xx_1)\dotp \xx_2 = (A\xx_1)\dotp \xx_2 = \xx_1\dotp(A\xx_2) = \xx_1(\lambda_2\xx_2) = \lambda_2(\xx_1\dotp\xx_2)\text{.}
\end{equation*}
It follows that \((\lambda_1-\lambda_2)(\xx_1\dotp \xx_2)=0\), and since \(\lambda_1\neq \lambda_2\), we must have \(\xx_1\dotp \xx_2=0\).%
\end{proof}
The procedure for diagonalizing a matrix is as follows: assuming that \(\dim E_\lambda(A)\) is equal to the multiplicity of \(\lambda\) for each distinct eigenvalue \(\lambda\), we find a basis for \(E_\lambda(A)\). The union of the bases for each eigenspace is then a basis of eigenvectors for \(\R^n\), and the matrix \(P\) whose columns are those eigenvectors will satisfy \(P^{-1}AP = D\), where \(D\) is a diagonal matrix whose diagonal entries are the eigenvalues of \(A\).%
\par
If \(A\) is symmetric, we know that eigenvectors from \emph{different} eigenspaces will be orthogonal to each other. If we further choose an orthogonal basis of eigenvectors for each eigenspace (which is possible via the Gram-Schmidt procedure), then we can construct an orthogonal basis of eigenvectors for \(\R^n\). Furthermore, if we normalize each vector, then we'll have an orthonormal basis. The matrix \(P\) whose columns consist of these orthonormal basis vectors has a name.%
\begin{definition}{}{x:definition:def-orthogonal-matrix}%
A matrix \(P\) is called \terminology{orthogonal} if \(P^T = P^{-1}\).%
\end{definition}
\begin{theorem}{}{}{x:theorem:thm-ortho-matrix}%
A matrix \(P\) is orthogonal if and only if the columns of \(P\) form an orthonormal basis for \(\R^n\).%
\end{theorem}
A fun fact is that if the columns of \(P\) are orthonormal, then so are the rows. But this is not true if we ask for the columns to be merely orthogonal. For example, the columns of \(A = \bbm 1\amp 0\amp 5\\-2\amp 1\amp 2\\1\amp 2\amp -1\ebm \) are orthogonal, but (as you can check) the rows are not. But if we normalize the columns, we get%
\begin{equation*}
P = \bbm 1/\sqrt{6}\amp 0 \amp 1/\sqrt{30}\\-2/\sqrt{6}\amp 1/\sqrt{5}\amp 2/\sqrt{30}\\1/\sqrt{6}\amp 2/\sqrt{5}\amp -1/\sqrt{30}\ebm\text{,}
\end{equation*}
which, as you can confirm, is an orthogonal matrix.%
\begin{definition}{}{x:definition:def-ortho-diag}%
An \(n\times n\) matrix \(A\) is said to be \emph{orthogonally diagonalizable} if there exists an orthogonal matrix \(P\) such that \(P^TAP\) is diagonal.%
\end{definition}
The above definition leads to the following result, also known as the Principal Axes Theorem.%
\begin{theorem}{Real Spectral Theorem.}{}{x:theorem:thm-real-spectral}%
The following are equivalent for a real \(n\times n\) matrix \(A\):%
\begin{enumerate}
\item{}\(A\) is symmetric.%
\item{}There is an orthonormal basis for \(\R^n\) consisting of eigenvectors of \(A\).%
\item{}\(A\) is orthogonally diagonalizable.%
\end{enumerate}
%
\end{theorem}
\begin{inlineexercise}{}{g:exercise:idp104}%
Determine the eigenvalues of \(A=\bbm 5\amp -2\amp -4\\-2\amp 8\amp -2\\-4\amp -2\amp 5\ebm\), and find an orthogonal matrix \(P\) such that \(P^TAP\) is diagonal.%
\end{inlineexercise}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 4.3 Quadratic forms}
\typeout{************************************************}
%
\begin{sectionptx}{Quadratic forms}{}{Quadratic forms}{}{}{x:section:sec-quadratic}
If you've done a couple of calculus courses, you've probably encountered conic sections, like the ellipse \(\frac{x^2}{a^2}+\frac{y^2}{b^2}=1\) or the parabola \(\frac{y}{b}=\frac{x^2}{a^2}\). You might also recall that your instructor was careful to avoid conic sections with equations including ``cross-terms'' like \(xy\). The reason for this is that sketching a conic section like \(x^2+4xy+y^2=1\) requires the techniques of the previous section.%
\par
A basic fact about orthogonal matrices is that they \emph{preserve length}. Indeed, for any vector \(\xx\) in \(\R^n\) and any orthogonal matrix \(P\),%
\begin{equation*}
\len{P\xx}^2 = (P\xx)\dotp (P\xx) = (P\xx)^T(P\xx) = (\xx^TP^T)(P\xx) = \xx^T\xx=\len{\xx}^2\text{,}
\end{equation*}
since \(P^TP=I_n\).%
\par
Note also that since \(P^TP=I_n\) and \(\det P^T=\det P\), we have%
\begin{equation*}
\det(P)^2=\det(P^TP)=\det(I_n)=1\text{,}
\end{equation*}
so \(\det(P)=\pm 1\). If \(\det P=1\), we have what is called a \emph{special orthogonal matrix}. In \(\R^2\) or \(\R^3\), multiplication by a special orthogonal matrix is simply a rotation. (If \(\det P=-1\), there is also a reflection.)%
\par
We mentioned in the previous section that the \hyperref[x:theorem:thm-real-spectral]{Real Spectral Theorem} is also referred to as the principal axes theorem. The name comes from the fact that one way to interpret the orthogonal diagonalization of a symmetric matrix is that we are rotating our coordinate system. The original coordinate axes are rotated to new coordinate axes, with respect to which the matrix \(A\) is diagonal. This will become more clear once we apply these ideas to the problem of conic sections mentioned above. First, a definition.%
\begin{definition}{}{x:definition:def-quadratic-form}%
A \terminology{quadratic form} on variables \(x_1, x_2,\ldots, x_n\) is any expression of the form%
\begin{equation*}
q(x_1,\ldots, x_n) = \sum_{i\leq j}a_{ij}x_ix_j\text{.}
\end{equation*}
%
\end{definition}
For example, \(q_1(x,y)=4 x^2-4xy+4y^2\) and \(q_2(x,y,z)=9x^2-4 y^2-4xy-2xz+z^2\) are quadratic forms. Note that each term in a quadratic form is of degree two. We omit linear terms, since these can be absorbed by completing the square. The important observation is that every quadratic form can be associated to a symmetric matrix. The diagonal entries are the coefficients \(a_{ii}\) appearing in \hyperref[x:definition:def-quadratic-form]{Definition~{\xreffont\ref{x:definition:def-quadratic-form}}}, while the off-diagonal entries are \emph{half} the corresponding coefficients \(a_{ij}\).%
\par
For example the two quadratic forms given above have the following associated matrices:%
\begin{equation*}
A_1 = \bbm 4 \amp -2\\-2\amp 4\ebm \text{ and } A_2 = \bbm 9 \amp -2 \amp -1\\-2\amp 4\amp 0\\-1\amp 0\amp 1\ebm\text{.}
\end{equation*}
The reason for this is that we can then write%
\begin{equation*}
q_1(x,y)=\bbm x\amp y\ebm\bbm 4 \amp -1\\-1\amp 1\ebm\bbm x\\y\ebm
\end{equation*}
and%
\begin{equation*}
q_2(x,y,z)=\bbm x\amp y\amp z\ebm\bbm 9 \amp -2 \amp -1\\-2\amp 4\amp 0\\-1\amp 0\amp 1\ebm\bbm x\\y\\z\ebm\text{.}
\end{equation*}
%
\par
Of course, the reason for wanting to associate a \emph{symmetric} matrix to a quadratic form is that it can be orthogonally diagonalized. Consider the matrix \(A_1\).%
\begin{sageinput}
from sympy import Matrix, init_printing, factor
init_printing()
A1 = Matrix(2,2,[4,-2,-2,4])
p = A1.charpoly().as_expr()
factor(p)
\end{sageinput}
\begin{sageoutput}
\[(\lambda-6)(\lambda-2)\]
\end{sageoutput}
We find distinct eigenvalues \(\lambda_1=2\) and \(\lambda_2=6\). Since \(A\) is symmetric, we know the corresponding eigenvectors will be orthogonal.%
\begin{sageinput}
A1.eigenvects()
\end{sageinput}
\begin{sageoutput}
\[\left[\left(2,1,\bbm 1\\1\ebm\right),\left(6,1,\bbm -1\\1\ebm\right)\right]\]
\end{sageoutput}
The resulting orthogonal matrix is \(P=\frac{1}{\sqrt{2}}\bbm 1\amp -1\\1\amp 1\ebm\), and we find%
\begin{equation*}
P^TAP = \bbm 2\amp 0\\0\amp 6\ebm, \text{ or } A = PDP^T,
\end{equation*}
where \(D = \bbm 2\amp 0\\0\amp 6\ebm\). If we define new variables \(y_1,y_2\) by%
\begin{equation*}
\bbm y_1\\y_2\ebm = P^T\bbm x_1\\x_2\ebm\text{,}
\end{equation*}
then we find that%
\begin{align*}
\bbm x_1\amp x_2\ebm A\bbm x_1\\x_2\ebm \amp = (\bbm x_1\amp x_2\ebm P)D\left(P^T\bbm x_1\\x_2\ebm\right) \\
\amp = \bbm y_1 \amp y_2\ebm\bbm 2\amp 0\\0\amp 6\ebm\bbm y_1\\y_2\ebm\\
\amp = 2y_1^2+6y_2^2\text{.}
\end{align*}
Note that there is no longer any cross term.%
\par
Now, suppose we want to graph the conic \(4x_1^2-4x_1x_2+4x_2^2=12\). By changing to the variables \(y_1,y_2\) this becomes \(2y_1^2+6y_2^2=12\), or \(\frac{y_1^2}{6}+\frac{y_2^2}{2}=1\). This is the standard from of an ellipse, but in terms of new variables. How do we graph it? Returning to the definition of our new variables, we find \(y_1=\frac{1}{\sqrt{2}}(x_1+x_2)\) and \(y_2=\frac{1}{\sqrt{2}}(-x_1+x_2)\). The \(y_1\) axis should be the line \(y_2=0\), or \(x_1=x_2\). (Note that this line points in the direction of the eigenvector \(\bbm 1\\1\ebm\).) The \(y_2\) axis should be the line \(y_1=0\), or \(x_1=-x_2\), which is in the direction of the eigenvector \(\bbm -1\\1\ebm\).%
\par
This lets us see that our new coordinate axes are simply a rotation (by \(\pi/4\)) of the old coordinate axes, and our conic section is, accordingly, an ellipse that has been rotated by the same angle.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 4.4 Diagonalization of complex matrices}
\typeout{************************************************}
%
\begin{sectionptx}{Diagonalization of complex matrices}{}{Diagonalization of complex matrices}{}{}{x:section:sec-complex}
\begin{introduction}{}%
Recall that when we first defined vector spaces, we mentioned that a vector space can be defined over any \emph{field} \(\mathbb{F}\). To keep things simple, we've mostly assumed \(\mathbb{F}=\mathbb{R}\). But most of the theorems and proofs we've encountered go through unchanged if we work over a general field. (This is not quite true: over a \emph{finite} field things can get more complicated. For example, if \(\mathbb{F}=\mathbb{Z}_2=\{0,1\}\), then we get weird results like \(\vv+\vv=\mathbf{0}\), since \(1+1=0\).)%
\par
In fact, if we replace \(\R\) by \(\C\), about the only thing we'd have to go back and change is the definition of the dot product. The reason for this is that although the complex numbers seem computationally more complicated, (which might mostly be because you don't use them often enough) they follow the exact same algebraic rules as the real numbers. In other words, the \emph{arithmetic} might be different, but the \emph{algebra} is the same. There is one key difference between the two fields: over the complex numbers, every polynomial can be factored. This is important if you're interested in finding eigenvalues.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection 4.4.1 Review of complex numbers}
\typeout{************************************************}
%
\begin{subsectionptx}{Review of complex numbers}{}{Review of complex numbers}{}{}{x:subsection:subsec-complex-review}
Let's quickly review some basic facts about complex numbers that are typically covered in an earlier course. First, we define the set of complex numbers by%
\begin{equation*}
\C = \{x+iy \,|\, x,y\in \R\},
\end{equation*}
where \(i=\sqrt{-1}\). We have a bijection \(\C \to \R^2\) given by \(x+iy\mapsto (x,y)\); because of this, we often picture \(\C\) as the \emph{complex plane}, with a ``real'' \(x\) axis, and an ``imaginary'' \(y\) axis.%
\par
Arithmetic with complex numbers is defined by%
\begin{align*}
(x_1+iy_1)+(x_2+iy_2) \amp = (x_1+x_2)+i(y_1+y_2) \\
(x_1+iy_1)(x_2+iy_2) \amp = (x_1x_2-y_1y_2)+i(x_1y_2+x_2y_1)\text{.}
\end{align*}
The multiplication rule looks complicated, but it's really just ``\initialism{FOIL}'', along with the fact that \(i^2=-1\). Note that if \(c=c+i0\) is real, we have \(c(x+iy)=(cx)+i(cy)\), so that \(\C\) has the structure of a two dimensional vector space over \(\R\) (isomorphic to \(\R^2\)).%
\par
Subtraction is defined in the obvious way. Division is less obvious. To define division, it helps to first introduce the \terminology{complex conjugate}. Given a complex number \(z=x+iy\), we define \(\overline{z}=x-iy\). The importance of the conjugate is that we have the identity%
\begin{equation*}
z\bz = (x+iy)(x-iy)=x^2+y^2\text{.}
\end{equation*}
So \(z\bz\) is \emph{real}, and \emph{non-negative}. This lets us define the \terminology{modulus} of \(z\) by%
\begin{equation*}
\abs{z} = \sqrt{z\bz} = \sqrt{x^2+y^2}\text{.}
\end{equation*}
This gives a measure of the magnitude of a complex number, in the same way as the vector norm on \(\R^2\).%
\par
Now, given \(z=x+iy\) and \(w=s+it\), we have%
\begin{equation*}
\frac{z}{w}=\frac{z\bar{w}}{w\bar{w}} = \frac{(x+iy)(s-it)}{s^2+t^2} = \frac{xs-yt}{s^2+t^2}+i\frac{xt+ys}{s^2+t^2}\text{.}
\end{equation*}
And of course, we have \(w\bar{w}\neq 0\) unless \(w=0\), and as usual, we don't divide by zero.%
\par
An important thing to keep in mind when working with complex numbers is that they follow the same algebraic rules as real numbers. For example, given \(a,b,z,w\) all complex, and \(a\neq 0\), where \(az+b=w\), if we want to solve for \(z\), the answer is \(z=\frac1a(w-b)\), as it would be in \(\R\). The difference between \(\R\) and \(\C\) only really materializes when we want to \emph{compute} \(z\), by plugging in values for \(a,b\) and \(w\).%
\par
One place where \(\C\) is \emph{computationally} more complicated is finding powers and roots. For this, it is often more convenient to write our complex numbers in \terminology{polar form}. The key to the polar form for complex numbers is \emph{Euler's identity}. For a \emph{unit} complex number \(z\) (that is with \(\abs{z}=1\)), we can think of \(z\) as a point on the unit circle, and write%
\begin{equation*}
z = \cos(\theta)+i\sin(\theta)\text{.}
\end{equation*}
If \(\abs{z}=r\), we simply change the radius of our circle, so in general, \(z = r(\cos(\theta)+i\sin(\theta))\). Euler's identity states that%
\begin{equation}
\cos(\theta)+i\sin(\theta)=e^{i\theta}\text{.}\label{x:men:eq-euler}
\end{equation}
%
\par
This idea of putting a complex number in an exponential function seems odd at first. If you take a course in complex variables, you'll get a better understanding of why this makes sense. But for now, we can take it as a convenient piece of notation. The reason it's convenient is that the rules for complex arithmetic turn out to align quite nicely with properties of the exponential function. For example, de Moivre's Theorem states that%
\begin{equation*}
(\cos(\theta)+i\sin(\theta))^n = \cos(n\theta)+i\sin(n\theta)\text{.}
\end{equation*}
This can be proved by induction (and the proof is not even all that bad), but it seems perfectly obvious in exponential notation:%
\begin{equation*}
(e^{i\theta})^n = e^{in\theta}\text{,}
\end{equation*}
since you multiply exponents when you raise a power to a power.%
\par
Similarly, if we want to multiply two unit complex numbers, we have%
\begin{align*}
(\cos\alpha+i\sin\alpha)(\cos\beta+i\sin\beta) \amp = (\cos\alpha\cos\beta-\sin\alpha\sin\beta)\\
\amp \quad\quad +i(\sin\alpha\cos\beta+\cos\alpha\sin\beta)\\
\amp = \cos(\alpha+\beta)+i\sin(\alpha+\beta)\text{.}
\end{align*}
But in exponential notation, this is simply%
\begin{equation*}
e^{i\alpha}e^{i\beta} = e^{i(\alpha+\beta)}\text{,}
\end{equation*}
which makes sense, since when you multiply exponentials, you add the exponents.%
\par
Generally, problems involving addition and subtraction are best handled in ``rectangular'' (\(x+iy\)) form, while problems involving multiplication and powers are best handled in polar form.%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection 4.4.2 Complex vectors}
\typeout{************************************************}
%
\begin{subsectionptx}{Complex vectors}{}{Complex vectors}{}{}{x:subsection:subsec-complex-vector}
A complex vector space is simply a vector space where the scalars are elements of \(\C\) rather than \(\R\). Examples include polynomials with complex coefficients, complex-valued functions, and \(\C^n\), which is defined exactly how you think it should be. In fact, one way to obtain \(\C^n\) is to start with the exact same standard basis we use for \(\R^n\), and then take linear combinations using complex scalars.%
\par
We'll write elements of \(\C^n\) as \(\zz = (z_1,z_2,\ldots, z_n)\). Notice that we've dropped the arrow notation for vectors in favour of a bold font. The reason is that we'll want to consider complex conjugates, and things get cluttered if we try to fit an arrow and a bar over our vector. The complex conjugate of \(\zz\) is given by%
\begin{equation*}
\bar{\zz} = (\bz_1,\bz_2,\ldots, \bz_n)\text{.}
\end{equation*}
%
\par
The standard inner product on \(\C^n\) looks a lot like the dot product on \(\R^n\), with one important difference: we apply a complex conjugate to the second vector.%
\begin{definition}{}{x:definition:def-complex-inner}%
The \terminology{standard inner product} on \(\C^n\) is defined as follows: given \(\zz=(z_1,z_2,\ldots, z_n)\) and \(\ww=(w_1,w_2,\ldots, w_n)\),%
\begin{equation*}
\langle \zz,\ww\rangle = \zz\dotp\bar{\ww} = z_1\bar{w}_1+z_2\bar{w}_2+\cdots + z_n\bar{w}_n\text{.}
\end{equation*}
%
\end{definition}
If \(\zz,\ww\) are real, this is just the usual dot product. The reason for using the complex conjugate is to ensure that we still have a positive-definite inner product on \(\C^n\):%
\begin{equation*}
\langle \zz,\zz\rangle = z_1\bz_1+z_2\bz_2+\cdots + z_n\bz_n = \abs{z_1}^2+\abs{z_2}^2+\cdots + \abs{z_n}^2\text{,}
\end{equation*}
which shows that \(\langle \zz,\zz\rangle \geq 0\), and \(\langle \zz,\zz\rangle = 0\) if and only if \(\zz=\mathbf{0}\).%
\begin{inlineexercise}{}{g:exercise:idp105}%
Compute the dot product of \(\zz = (2-i, 3i, 4+2i)\) and \(\ww = (3i,4-5i,-2+2i)\).%
\end{inlineexercise}%
This isn't hard to do by hand, but it's useful to know how to ask the computer to do it, too. Unfortunately, the dot product in SymPy does not include the complex conjugate. One likely reason for this is that while most mathematicians take the complex conjugate of the \emph{second} vector, some mathematicians, and most physicists, put the conjugate on the first vector. So they may have decided to remain agnostic about this choice. We can manually apply the conjugate, using \mono{Z.dot(W.H)}.%
\begin{sageinput}
from sympy import Matrix,init_printing
init_printing()
Z = Matrix(3,1,[2-I,3*I,4+2*I])
W = Matrix(3,1,[3*I,4-5*I,-2+2*I])
Z, W, Z.dot(W.H)
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 2-i\\3i\\4+2i\ebm, \bbm 3i\\4-5i\\-2+2i\ebm, (-2-2i)(4+2i)-3i(2-i)+3i(4+5i)\right)\]
\end{sageoutput}
Again, you might want to wrap that last term in \mono{simplify()} (in which case you'll get \(-22-6i\) for the dot product). Above, we saw that the complex inner product is designed to be positive definite, like the real inner product. The remaining properties of the complex inner product are given as follows.%
\begin{theorem}{}{}{x:theorem:thm-complex-inner-props}%
For any vectors \(\zz_1,\zz_2,\zz_3\) and any complex number \(\alpha\),%
\begin{enumerate}
\item{}\(\langle \zz_1+\zz_2,\zz_3\rangle = \langle \zz_1,\zz_3\rangle + \langle zz_2,\zz_3\rangle\) and \(\langle \zz_1,\zz_2+\zz_3\rangle = \langle \zz_1,\zz_2\rangle + \langle zz_1,\zz_3\rangle\).%
\item{}\(\langle \alpha\zz_1,\zz_2\rangle = \alpha\langle\zz_1,\zz_2\rangle\) and \(\langle \zz_1,\alpha\zz_2\rangle=\bar{\alpha}\langle \zz_1,\zz_2\rangle\).%
\item{}\(\displaystyle \langle \zz_2,\zz_1\rangle = \overline{\langle \zz_1,\zz_2\rangle}\)%
\item{}\(\langle \zz_1,\zz_1\rangle\geq 0\), and \(\langle \zz_1,\zz_1\rangle =0\) if and only if \(\zz_1=\mathbf{0}\).%
\end{enumerate}
%
\end{theorem}
\begin{proof}{}{g:proof:idp106}
%
\begin{enumerate}
\item{}Using the distributive properties of matrix multiplication and the transpose,%
\begin{align*}
\langle \zz_1+\zz_2,\zz_3\rangle \amp= (\zz_1+\zz_2)^T\bar{\zz_3}\\
\amp =(\zz_1^T+\zz_2^T)\bar{\zz_3}\\
\amp =\zz_1^T\bar{\zz_3}+\zz_2^T\bar{\zz_3}\\
\amp =\langle \zz_1,\zz_3\rangle + \langle \zz_2,\zz_3\rangle\text{.}
\end{align*}
The proof is similar when addition is in the second component. (But not identical -{}-{} you'll need the fact that the complex conjugate is distributive, rather than the transpose.)%
\item{}These again follow from writing the inner product as a matrix product.%
\begin{equation*}
\langle \alpha\zz_1,\zz_2\rangle = (\alpha \zz_1)^T\bar{\zz_2} = \alpha(\zz_1^T\bar{\zz_2}) = \alpha\langle\zz_1,\zz_2\rangle\text{,}
\end{equation*}
and%
\begin{equation*}
\langle \zz_1,\alpha\zz_2\rangle = \zz_1^T\overline{\alpha \zz_2} = \zz_1^T(\bar{\alpha}\bar{\zz_2}) = \bar{\alpha}(\zz_1^T\zz_2)=\alpha\langle \zz_1,\zz_2\rangle\text{.}
\end{equation*}
%
\item{}Note that for any vectors \(\zz,\ww\), \(\zz^T\ww\) is a number, and therefore equal to its own transpose. Thus, we have \(\zz^T\ww = (\zz^T\ww)^T=\ww^T\zz\), and%
\begin{equation*}
\overline{\langle \zz_1,\zz_2\rangle} = \overline{\zz_1^T\bar{\zz_2}} = \overline{\bar{\zz_2}^T\zz_1} = \zz_2^T\overline{\zz_1}=\langle \zz_2,\zz_1\rangle\text{.}
\end{equation*}
%
\item{}This was already demonstrated above.%
\end{enumerate}
%
\end{proof}
\begin{definition}{}{x:definition:def-complex-norm}%
The \terminology{norm} of a vector \(\zz = (z_1,z_2,\ldots, z_n)\) in \(\C^n\) is given by%
\begin{equation*}
\len{\zz} = \sqrt{\langle \zz,\zz\rangle} = \sqrt{\abs{z_1}^2+\abs{z_2}^2+\cdots +\abs{z_n}^2}\text{.}
\end{equation*}
%
\end{definition}
Note that much like the real norm, the complex norm satisfies \(\len{\alpha\zz}=\abs{\alpha}\len{\zz}\) for any (complex) scalar \(\alpha\).%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection 4.4.3 Complex matrices}
\typeout{************************************************}
%
\begin{subsectionptx}{Complex matrices}{}{Complex matrices}{}{}{x:subsection:subsec-complex-matrix}
Linear transformations are defined in exactly the same way, and a complex matrix is simply a matrix whose entries are complex numbers. There are two important operations defined on complex matrices: the conjugate, and the conjugate transpose (also known as the hermitian transpose).%
\begin{definition}{}{x:definition:def-conjugate-transpose}%
The \terminology{conjugate} of a matrix \(A=[a_{ij}]\in M_{mn}(\C)\) is the matrix \(\bar{A}=[\bar{a}_{ij}]\). The \terminology{conjugate transpose} of \(A\) is the matrix \(A^H\) defined by%
\begin{equation*}
A^H = (\bar{A})^T=\overline{(A^T)}\text{.}
\end{equation*}
%
\end{definition}
Note that many textbooks use the notation \(A^\dagger\) for the conjugate transpose.%
\begin{definition}{}{x:definition:def-hermitian-unitary}%
An \(n\times n\) matrix \(A\in M_{nn}(\C)\) is called \terminology{hermitian} if \(A^H = A\), and \terminology{unitary} if \(A^H = A^{-1}\). (A matrix is \terminology{skew-hermitian} if \(A^H=-A\).)%
\end{definition}
Hermitian and unitary matrices (or more accurately, linear operators) are very important in quantum mechanics. Indeed, hermitian matrices represent ``observable'' quantities, in part because their eigenvalues are real, as we'll soon see. For us, hermitian and unitary matrices can simply be viewed as the complex counterparts of symmetric and orthogonal matrices, respectively. In fact, a real symmetric matrix \emph{is} hermitian, since the conjugate has no effect on it, and similarly, a real orthogonal matrix is technically unitary. As with orthogonal matrices, a unitary matrix can also be characterized by the property that its rows and columns both form orthonormal bases.%
\begin{inlineexercise}{}{g:exercise:idp107}%
Show that the matrix \(A = \bbm 4\amp 1-i\amp -2+3i\\1+i\amp 5 \amp 7i\\-2-3i\amp -7i\amp -4\ebm\) is hermitian, and that the matrix \(B = \frac12\bbm 1+i\amp \sqrt{2}\\1-i\amp\sqrt{2}i\ebm\) is unitary.%
\end{inlineexercise}%
When using SymPy, the hermitian conjugate of a matrix \mono{A} is executed using \mono{A.H}. (There appears to also be an equivalent operation named \mono{Dagger} coming from \mono{sympy.physics.quantum}, but I've had more success with \mono{.H}.) The complex unit is entered as \mono{I}. So for the exercise above, we can do the following.%
\begin{sageinput}
A = Matrix(3,3,[4,1-I,-2+3*I,1+I,5,7*I,-2-3*I,-7*I,-4])
A == A.H
\end{sageinput}
\begin{sageoutput}
True
\end{sageoutput}
The last line verifies that \(A=A^H\). We could also replace it with \mono{A,A.H} to explicitly see the two matrices side by side. Now, let's confirm that \(B\) is unitary.%
\begin{sageinput}
B = Matrix(2,2,[1/2+1/2*I, sqrt(2)/2,1/2-1/2*I,(sqrt(2)/2)*I])
B,B*B.H
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm \frac12+\frac{i}{2}\amp \frac{\sqrt{2}}{2}\\ \frac12-\frac{i}{2}\amp \frac{\sqrt{2}i}{2}\ebm,
 \bbm \frac12 + \left(\frac12-\frac{i}{2}\right)\left(\frac12+\frac{i}{2}\right)\amp
-\frac{i}{2}+\left(\frac12+\frac{i}{2}\right)^2\\ \left(\frac12-\frac{i}{2}\right)^2\amp
\frac12 + \left(\frac12-\frac{i}{2}\right)\left(\frac12+\frac{i}{2}\right)\ebm\right)\]
\end{sageoutput}
Hmm... That doesn't look like the identity on the right. Maybe try replacing \mono{B*B.H} with \mono{simplify(B*B.H)}. (You will want to add \mono{from sympy import simplify} at the top of the cell.) Or you could try \mono{B.H, B**-1} to compare results. Actually, what's interesting is that in a Sage cell, \mono{B.H == B**-1} yields \mono{False}, but \mono{B.H == simplify(B**-1)} yields \mono{True}!%
\par
As mentioned above, hermitian matrices are the complex analogue of symmetric matrices. Recall that a key property of a symmetric matrix is its symmetry with respect to the dot product. For a symmetric matrix \(A\), we had \(\mathbf{x}\dotp (A\mathbf{y})=(A\mathbf{x})\dotp \mathbf{y}\). Hermtian matrices exhibit the same behaviour with respect to the complex inner product.%
\begin{theorem}{}{}{x:theorem:thm-hermitian-symmetry}%
An \(n\times n\) complex matrix \(A\) is Hermitian if and only if%
\begin{equation*}
\langle A\zz,\ww\rangle = \langle \zz, A\ww\rangle
\end{equation*}
for any \(\zz,\ww\in\C^n\)%
\end{theorem}
\begin{proof}{}{g:proof:idp108}
Note that the property \(A^H=A\) is equivalent to \(A^T=\bar{A}\). This gives us%
\begin{equation*}
\langle A\zz,\ww\rangle = (A\zz)^T\bar{\ww} = (\zz^TA^T)\bar{\ww} = (\zz^T\bar{A})\bar{\ww}=\zz^T(\overline{A\ww}) = \langle \zz,\ww\rangle\text{.}
\end{equation*}
Conversely, suppose \(\langle A\zz,\ww\rangle = \langle \zz, A\ww\rangle\) for all \(\zz,\ww\in \C^n\), and let \(\basis{e}{n}\) denote the standard basis for \(\C^n\). Then%
\begin{equation*}
a_{ji}=\langle A\mathbf{e}_i,\mathbf{e}_j\rangle = \langle \mathbf{e}_i,A\mathbf{e}_j\rangle = \overline{a_{ij}}\text{,}
\end{equation*}
which shows that \(A^T=\bar{A}\).%
\end{proof}
Next, we've noted that one advantage of doing linear algebra over \(\C\) is that every polynomial can be completely factored, including the characteristic polynomial. This means that we can always find eigenvalues for a matrix. When that matrix is Hermitian, we get a surprising result.%
\begin{theorem}{}{}{x:theorem:thm-hermitian-eigen-real}%
For any hermitian matrix \(A\),%
\begin{enumerate}
\item{}The eigenvalues of \(A\) are real.%
\item{}Eigenvectors corresponding to distinct eigenvalues are orthogonal.%
\end{enumerate}
%
\end{theorem}
\begin{proof}{}{g:proof:idp109}
%
\begin{enumerate}
\item{}Suppose \(A\zz = \lambda\zz\) for some \(\lambda\in\C\) and \(\zz\neq \mathbf{0}\). Then%
\begin{equation*}
\lambda \langle \zz,\zz\rangle  = \langle \lambda\zz,\zz\rangle = \langle A\zz,\zz \rangle = \langle \zz, A\zz\rangle = \langle \zz,\lambda\zz\rangle = \bar{\lambda}\langle \zz,\zz\rangle\text{.}
\end{equation*}
Thus, \((\lambda-\bar{\lambda})\len{\zz}^2=0\), and since \(\len{z}\neq 0\), we must have \(\bar{\lambda}=\lambda\), which means \(\lambda\in\R\).%
\item{}Similarly, suppose \(\lambda_1,\lambda_2\) are eigenvalues of \(A\), with corresponding eigenvectors \(\zz,\ww\). Then%
\begin{equation*}
\lambda_1\langle \zz,\ww\rangle = \langle \lambda_1\zz,\ww\rangle = \langle A\zz,\ww\rangle =\langle \zz,A\ww\rangle = \langle \zz,\lambda_2\ww\rangle = \bar{\lambda_2}\langle\zz,\ww\rangle\text{.}
\end{equation*}
This gives us \((\lambda_1-\bar{\lambda_2})\langle \zz,\ww\rangle=0\). And since we already know \(\lambda_2\) must be real, and \(\lambda_1\neq \lambda_2\), we must have \(\langle \zz,\ww\rangle = 0\).%
\end{enumerate}
%
\end{proof}
In light of \hyperref[x:theorem:thm-hermitian-eigen-real]{Theorem~{\xreffont\ref{x:theorem:thm-hermitian-eigen-real}}}, we realize that diagonalization of hermitian matrices will follow the same script as for symmetric matrices. Indeed, \hyperref[x:theorem:thm-gram-schmidt]{Gram-Schmidt Orthonormalization Algorithm} applies equally well in \(\C^n\), as long as we replace the dot product with the complex inner product. This suggests the following.%
\begin{theorem}{Spectral Theorem.}{}{x:theorem:thm-complex-spectral}%
If \(A\) is an \(n\times n\) hermitian matrix, then there exists an orthonormal basis of \(\C^n\) consisting of eigenvectors of \(A\). Moreover, the matrix \(U\) whose columns consist of those eigenvectors is unitary, and the matrix \(U^HAU\) is diagonal.%
\end{theorem}
\begin{inlineexercise}{}{g:exercise:idp110}%
Confirm that the matrix \(A = \bbm 4 \amp 3-i\\3+i\amp 1\ebm\) is hermitian. Then, find the eigenvalues of \(A\), and a unitary matrix \(U\) such that \(U^HAU\) is diagonal.%
\end{inlineexercise}%
To do the above exercise using SymPy, we first define \(A\) and ask for the eigenvectors.%
\begin{sageinput}
A = Matrix(2,2,[4,3-I,3+I,1])
A.eigenvects()
\end{sageinput}
\begin{sageoutput}
\[\left[\left(-1,1,\bbm -\frac35+\frac{i}{5}\\1\ebm\right),\left(6,1,\bbm \frac32-\frac{i}{2}\ebm\right)\right]\]
\end{sageoutput}
We can now manually determine the matrix \(U\), as we did above, and input it:%
\begin{sageinput}
U = Matrix([[(3-I)/sqrt(35),(3-I)/sqrt(14)],
            [-5/sqrt(35),2/sqrt(14)]])
\end{sageinput}
To confirm it's unitary, add the line \mono{U*U.H} to the above, and confirm that you get the identity matrix as output. You might need to use \mono{simplify(U*U.H)} if the result is not clear. Now, to confirm that \(U^HAU\) really is diagonal, go back to the cell above, and enter it. Try \mono{(U.H)*A*U}, just to remind yourself that adding the \mono{simplify} command is often a good idea.%
\par
If you want to cut down on the manual labour involved, we can make use of some of the other tools SymPy provides. In the next cell, we're going to assign the output of \mono{A.eigenvects()} to a list. The only trouble is that the output of the eigenvector command is a list of lists. Each list item is a list \mono{(eigenvalue, multiplicity, [eigenvectors])}.%
\begin{sageinput}
L = A.eigenvects()
L
\end{sageinput}
\begin{sageoutput}
\[\left[\left(-1,1,\bbm -\frac35+\frac{i}{5}\\1\ebm\right), \left(6,1,\bbm \frac32-\frac{i}{2}\\1\ebm\right)\right]\]
\end{sageoutput}
Try the above modifications, in sequence. First, replacing the second line by \mono{L[0]} will give the first list item, which is another list:%
\begin{equation*}
\left(-1,1,\left[\bbm -\frac35+\frac{i}{5}\ebm\right]\right)\text{.}
\end{equation*}
We want the third item in the list, so try \mono{(L[0])[2]}. But note the extra set of brackets! There could (in theory) be more than one eigenvector, so this is a list with one item. To finally get the vector out, try \mono{((L[0])[2])[0]}. (There is probably a better way to do this. Someone who is more fluent in Python is welcome to advise.)%
\par
Now that we know how to extract the eigenvectors, we can normalize them, and join them to make a matrix. The norm of a vector is simnply \mono{v.norm()}, and to join column vectors \mono{u1} and \mono{u2} to make a matrix, we can use the command \mono{u1.row\_join(u2)}. We already defined the matrix \mono{A} and list \mono{L} above, but here is the whole routine in one cell, in case you didn't run all the cells above.%
\begin{sageinput}
from sympy import *
init_printing()
A = Matrix(2,2,[4,3-I,3+I,1])
L = A.eigenvects()
v = ((L[0])[2])[0]
w = ((L[1])[2])[0]
u1 = (1/v.norm())*v
u2 = (1/w.norm())*w
U = u1.row_join(u2)
u1, u2, U, simplify(U.H*A*U)
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm\frac{\sqrt{35}(-\frac35+\frac{i}{5})}{7}\\ \frac{\sqrt{35}}{7}\ebm,\bbm \frac{\sqrt{14}(\frac32-\frac{i}{2})}{7}\\ \frac{\sqrt{14}}{7}\ebm,
\bbm \frac{\sqrt{35}(-\frac35+\frac{i}{5})}{7} \amp \frac{\sqrt{14}(\frac32-\frac{i}{2})}{7}\\ \frac{\sqrt{35}}{7} \amp\frac{\sqrt{14}}{7}\ebm,
\bbm -1\amp 0\\0\amp 6\ebm\right)\]
\end{sageoutput}
Believe me, you want the simplify command on that last matrix.%
\par
While \hyperref[x:theorem:thm-complex-spectral]{Theorem~{\xreffont\ref{x:theorem:thm-complex-spectral}}} guarantees that any hermitian matrix can be ``unitarily diagonalized'', there are also non-hermitian matrices for which this can be done as well. A classic example of this is given in Nicholson's book, so we do not repeat the details here: the matrix \(\bbm 0\amp 1\\-1\amp 0\ebm\) is a real matrix with complex eigenvalues \(\pm i\), and while it is neither symmetric nor hermitian, it can be orthogonally diagonalized. This should be contrasted with the real spectral theorem, where any matrix that can be orthogonally diagonalized is necessarily symmetric.%
\par
This suggests that perhaps hermitian matrices are not quite the correct class of matrix for which the spectral theorem should be stated. Indeed, it turns out there is a somewhat more general class of matrix: the \emph{normal} matrices.%
\begin{definition}{}{x:definition:def-normal-matrix}%
An \(n\times n\) matrix \(A\) is \terminology{normal} if \(A^HA = AA^H\).%
\end{definition}
It turns out that a matrix \(A\) is normal if and only if \(A=UDU^H\) for some unitary matrix \(U\) and diagonal matrix \(D\). A further generalization is known as \emph{Schur's Theorem}.%
\begin{theorem}{}{}{x:theorem:thm-schurr}%
For \emph{any} complex \(n\times n\) matrix \(A\), there exists a unitary matrix \(U\) such that \(U^HAU = T\) is upper-triangular, and such that the diagonal entries of \(T\) are the eigenvalues of \(A\).%
\end{theorem}
Using Schur's Theorem, we can obtain a famous result, known as the Cayley-Hamilton Theorem, for the case of complex matrices. (It is true for real matrices as well, but we don't yet have the tools to prove it.) The Cayley-Hamilton Theorem states that substituting any matrix into its characteristic polynomial results in the zero matrix. To understand this result, we should first explain how to define a polynomial of a matrix.%
\par
Given a polynomial \(p(x) = a_0+a_1x+\cdots + a_nx_n\), we define \(p(A)\) as%
\begin{equation*}
p(A) = a_0I+a_1A+\cdots + a_nA^n\text{.}
\end{equation*}
(Note the presence of the identity matrix in the first term, since it does not make sense to add a scalar to a matrix.) Note further that since \((P^{-1}AP)^n = P^{-1}A^nP\) for any invertible matrix \(P\) and positive integer \(n\), we have \(p(U^HAU)=U^Hp(A)U\) for any polynomial \(p\) and unitary matrix \(U\).%
\begin{theorem}{}{}{x:theorem:thm-cayley-hamilton-c}%
Let \(A\) be an \(n\times n\) complex matrix, and let \(c_A(x)\) denote the characteristic polynomial of \(A\). Then we have \(c_A(A)=0\).%
\end{theorem}
\begin{proof}{}{g:proof:idp111}
By \hyperref[x:theorem:thm-schurr]{Theorem~{\xreffont\ref{x:theorem:thm-schurr}}}, there exists a unitary matrix \(U\) such that \(A = UTU^H\), where \(T\) is upper triangular, and has the eigenvalues of \(A\) as diagonal entries. Since \(c_A(A)=c_A(UTU^H)=Uc_A(T)U^H\), and \(c_A(x)=c_T(x)\) (since \(A\) and \(T\) are similar) it suffices to show that \(c_A(A)=0\) when \(A\) is upper-triangular. (If you like, we are showing that \(C_T(T)=0\), and deducing that \(c_A(A)=0\).) But if \(A\) is upper-triangular, so is \(xI_A\), and therefore, \(\det(xI-A)\) is just the product of the diagonal entries. That is,%
\begin{equation*}
c_A(x) = (x-\lambda_1)(x-\lambda_2)\cdots (x-\lambda_n)\text{,}
\end{equation*}
so%
\begin{equation*}
c_A(A) = (A-\lambda_1I)(A-\lambda_2I)\cdots (A-\lambda_nI)\text{.}
\end{equation*}
%
\par
Since the first column of \(A\) is \(\bbm \lambda_1\amp 0 \amp \cdots \amp 0\ebm^T\), the first column of \(A-\lambda_1I\) is identically zero. The second column of \(A-\lambda_2I\) similarly has the form \(\bbm k\amp 0\amp\cdots\amp 0\ebm\) for some number \(k\).%
\par
It follows that the first two columns of \((A-\lambda_1I)(A-\lambda_2I)\) are identically zero. Since only the first two entries in the third column of \((A-\lambda_3I)\) can be nonzero, we find that the first three columns of \((A-\lambda_1I)(A-\lambda_2I)(A-\lambda_3I)\) are zero, and so on.%
\end{proof}
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 4.5 Matrix Factorizations and Eigenvalues}
\typeout{************************************************}
%
\begin{sectionptx}{Matrix Factorizations and Eigenvalues}{}{Matrix Factorizations and Eigenvalues}{}{}{x:section:section-matrix-factor}
\begin{introduction}{}%
This section is a rather rapid tour of some cool ideas that get a lot of use in applied linear algebra. We are rather light on details here. The interested reader can consult sections 8.3\textendash{}8.6 in Nicholson.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection 4.5.1 Matrix Factorizations}
\typeout{************************************************}
%
\begin{subsectionptx}{Matrix Factorizations}{}{Matrix Factorizations}{}{}{x:subsection:subsec-matrix-factorization}
\begin{introduction}{}%
Recall that an \(n\times n\) matrix \(A\) is symmetric if \(A^T=A\) and hermitian if \(A^H=A\), where \(A^H\) is the conjugate transpose of a complex matrix. In either case, the corresponding matrix transformation \(T_A\) is said to be \terminology{self-adjoint}, which means that it satisfies the condition%
\begin{equation*}
\langle u,T_Av\rangle = \langle T_Au,v\rangle
\end{equation*}
for all \(u,v\in \mathbb{R}^n\) (or \(\mathbb{C}^n\)).%
\par
All such matrices (or operators) can be diagonalized, in the sense that there is an orthonormal basis of eigenvectors for that matrix. These eigenvectors can be arranged to form an orthogonal matrix \(P\) (or unitary matrix \(U\)) such that%
\begin{equation*}
P^TAP = D \quad \text{ (or } U^HAU=D)\text{,}
\end{equation*}
where \(D\) is a diagonal matrix whose entries are the eigenvalues of \(A\).%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsubsection 4.5.1.1 Positive Operators}
\typeout{************************************************}
%
\begin{subsubsectionptx}{Positive Operators}{}{Positive Operators}{}{}{x:subsubsection:pars-positive-ops}
\begin{definition}{}{x:definition:def-positive-op}%
A symmetric\slash{}hermitian matrix \(A\) (or operator \(T\)) is \terminology{positive} if \(\xx^TA\xx\geq 0\) (\(\langle \xx,T\xx\rangle\geq 0\)) for all vectors \(\xx\neq \zer\). It is \terminology{positive-definite} if \(\xx^TA\xx\gt 0\) for all nonzero \(\xx\).%
\end{definition}

\parmarginbox{%
\begin{aside}{}{g:aside:idp112}%
Some books will define positive-definite operators by the condition \(\xx^TA\xx\) without the requirement that \(A\) must be symmetric\slash{}hermitian. However, we will stick to the simpler definition.%
\end{aside}
}{0pt}%

This is equivalent to requiring that all the eigenvalues of \(A\) are non-negative. Every positive matrix \(A\) has a unique positive square root: a matrix \(R\) such that \(R^2=A\).%
\begin{theorem}{}{}{x:theorem:thm-positive-prod}%
For any \(n\times n\) matrix \(U\), the matrix \(A=U^TU\) is positive. Moreover, if \(U\) is invertible, then \(A\) is positive-definite.%
\end{theorem}
\begin{proof}{}{g:proof:idp113}
For any \(\xx\neq \zer\) in \(\R^n\),%
\begin{equation*}
\xx^T A\xx = \xx^TU^T U\xx = (U\xx)^T(U\xx) = \len{U\xx}^2\geq 0\text{.}\qedhere
\end{equation*}
%
\end{proof}
What is interesting is that the converse to the above statement is also true. The \terminology{Cholesky factorization} of a positive-definite matrix \(A\) is given by \(A=U^TU\), where \(U\) is upper-triangular, with positive diagonal entries. (See Nicholson for details.)%
\par
Even better is that there is a very simple algorithm for obtaining the factorization: Carry the matrix \(A\) to triangular form, using only row operations of the type \(R_i+kR_j\to R_i\). Then divide each row by the square root of the diagonal entry.%
\par
The SymPy library contains the \mono{cholesky()} algorithm. Note however that it produces a lower triangular matrix, rather than upper triangular. (That is, the output gives \(L=U^T\) rather than \(U\), so you will have \(A=LL^T\).) Let's give it a try. First, enter a positive-definite matrix. (We'll try the one from Example 8.3.3 in Nicholson.)%
\begin{sageinput}
from sympy import Matrix,init_printing
init_printing()
A = Matrix([[10,5,2],[5,3,2],[2,2,3]])
A
\end{sageinput}
\begin{sageoutput}
\[\bbm 10\amp 5\amp 2\\5\amp 3\amp 2\\2\amp 2\amp 3\ebm\]
\end{sageoutput}
Next, find the Cholesky factorization:%
\begin{sageinput}
L = A.cholesky()
L, L*L.T
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm \sqrt{10}\amp 0\amp 0\\ \frac{\sqrt{10}}{2} \amp \frac{\sqrt{2}}{2} \amp 0\\\frac{\sqrt{10}}{5} \amp \sqrt{2}\amp \frac{\sqrt{15}}{5}\ebm, \bbm 10\amp 5\amp 2\\ 5\amp 3\amp 2\\ 2\amp 2\amp 3\ebm\right)\]
\end{sageoutput}
\begin{sageinput}
L*L.T == A
\end{sageinput}
\begin{sageoutput}
True
\end{sageoutput}
\end{subsubsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsubsection 4.5.1.2 Singular Value Decomposition}
\typeout{************************************************}
%
\begin{subsubsectionptx}{Singular Value Decomposition}{}{Singular Value Decomposition}{}{}{x:subsubsection:pars-singular-values}
For any \(n\times n\) matrix \(A\), the matrices \(A^TA\) and \(AA^T\) are both positive. (Exercise!) This means that we can define \(\sqrt{A^TA}\), even if \(A\) itself is not symmetric or positive.%
\par
%
\begin{itemize}[label=\textbullet]
\item{}Since \(A^TA\) is symmetric, we know that it can be diagonalized.%
\item{}Since \(A^TA\) is positive, we know its eigenvalues are non-negative.%
\item{}This means we can define the \terminology{singular values} \(\sigma_i = \sqrt{\lambda_i}\) for each \(i=1,\ldots, n\).%
\item{}\alert{Note:} it's possible to do this even if \(A\) is not a square matrix!%
\end{itemize}
%
\par
The SymPy library has a function for computing the singular values of a matrix. Given a matrix \mono{A}, the command \mono{A.singular\_values()} will return its singular values. Try this for a few different matrices below:%
\begin{sageinput}
A = Matrix([[1,2,3],[4,5,6]])
A.singular_values()
\end{sageinput}
\begin{sageoutput}
\[\left[\sqrt{\frac{\sqrt{8065}}{2}+\frac{91}{2}},\sqrt{\frac{91}{2}-\frac{\sqrt{8065}}{2}},0\right]\]
\end{sageoutput}
In fact, SymPy can even return singular values for a matrix with variable entries! Try the following example from the \href{https://docs.sympy.org/latest/modules/matrices/matrices.html\#sympy.matrices.matrices.MatrixEigen.singular_values}{SymPy documentation}\footnote{\nolinkurl{docs.sympy.org/latest/modules/matrices/matrices.html\#sympy.matrices.matrices.MatrixEigen.singular_values}\label{g:fn:idp114}}.%
\begin{sageinput}
from sympy import Symbol
x = Symbol('x', real=True)
M = Matrix([[0,1,0],[0,x,0],[-1,0,0]])
M,M.singular_values()
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 0\amp 1\amp 0\\ 0\amp x\amp 0\\ -1\amp 0\amp 0\ebm , \left[ \sqrt{x^2+1}, 1, 0\right]\right)\]
\end{sageoutput}
For an \(n\times n\) matrix \(A\), we might not be able to diagonalize \(A\) (with a single orthonormal basis). However, it turns out that it's \emph{always} possible to find a pair of orthonormal bases \(\{e_1,\ldots, e_n\}, \{f_1,\ldots, f_n\}\) such that%
\begin{equation*}
Ax = \sigma_1(x\cdot e_1)f_1+\cdots + \sigma_n(x\cdot e_n)f_n\text{.}
\end{equation*}
In matrix form, \(A = P\Sigma_A Q^T\) for orthogonal matrices \(P,Q\).%
\par
In fact, this can be done even if \(A\) is not square, which is arguably the more interesting case! Let \(A\) be an \(m\times n\) matrix. We will find an \(m\times m\) orthogonal matrix \(P\) and \(n\times n\) orthogonal matrix \(Q\), such that \(A=P\Sigma_A Q^T\), where \(\Sigma_A\) is also \(m\times n\).%

\parmarginbox{%
\begin{aside}{}{g:aside:idp115}%
If \(A\) is symmetric and positive-definite, the singular values of \(A\) are just the eigenvalues of \(A\), and the singular value decomposition is the same as diagonalization.%
\end{aside}
}{0pt}%

The basis \(\{f_1,\ldots, f_n\}\) is an orthonormal basis for \(A^TA\), and the matrix \(Q\) is the matrix whose columns are the vectors \(f_i\). As a result, \(Q\) is orthogonal.%
\par
The matrix \(\Sigma_A\) is the same size as \(A\). First, we list the positive singular values of \(A\) in decreasing order:%
\begin{equation*}
\sigma_1\geq \sigma_2\geq \cdots \geq \sigma_k\gt 0\text{.}
\end{equation*}
Then, we let \(D_A = \operatorname{diag}(\sigma_1,\ldots, \sigma_k)\), and set%
\begin{equation*}
\Sigma_A = \begin{bmatrix}D_A\amp 0\\0\amp 0\end{bmatrix}\text{.}
\end{equation*}
That is, we put \(D_A\) in the upper-left, and then fill in zeros as needed, until \(\Sigma_A\) is the same size as \(A\).%
\par
Next, we compute the vectors \(e_i = \frac{1}{\len{Af_i}}Af_i\), for \(i=1,\ldots, k\). As shown in Nicolson, \(\{e_1,\ldots, e_r\}\) will be an orthonormal basis for the column space of \(A\). The matrix \(P\) is constructed by extending this to an orthonormal basis of \(\R^m\).%
\par
All of this is a lot of work to do by hand, but it turns out that it can be done numerically, and more importantly, \emph{efficiently}, by a computer. The SymPy library does not have an \initialism{SVD} algorithm, but the \mono{mpmath} library does, and it works well with SymPy.%
\par
Example 8.6.1 in Nicolson shows that for the matrix \(A = \begin{bmatrix}1\amp 0\amp 1\\-1\amp 1\amp 0\end{bmatrix}\) we should have%
\begin{equation*}
P = \frac{1}{\sqrt{2}}\bbm 1\amp 1\\-1\amp 1\ebm, \Sigma_A = \bbm \sqrt{3}\amp 0\amp 0\\0\amp 1\amp 0\ebm, Q = \frac{1}{\sqrt{6}}\bbm2\amp -1\amp 1\\0\amp \sqrt{3}\amp \sqrt{3}\\-\sqrt{2}\amp -\sqrt{2}\amp \sqrt{2}\ebm\text{.}
\end{equation*}
Let us test this on the computer. First, we import the \mono{mpmath} library, and define \(A\). (Importantly, the \mono{svd} routine from \mono{mpmath} will accept a SymPy matrix as input. We do not, for example, have to first convert it to a NumPy array.) Since both SymPy and mpmath have similar functions, we \mono{import as}  for both libraries, so we can distinguish between them as needed.%
\begin{sageinput}
import sympy as sy
import mpmath as mp
sy.init_printing()
mp.pretty = True
A = mp.matrix([[1,0,1],[-1,1,0]])
A
\end{sageinput}
\begin{sageoutput}
matrix(
[['1.0', '0.0', '1.0'],
 ['-1.0', '1.0', '0.0']])
\end{sageoutput}
Note that we defined \mono{A} as a mpmath \mono{matrix}. The \mono{svd} command from the mpmath library will also work on a SymPy \mono{Matrix}, but certain mpmath commands, like the \mono{chop} command used to round decimals, will not. Next, we apply the \mono{svd} algorithm.%
\begin{sageinput}
P,S,QT = mp.svd(A)
P,S,QT
\end{sageinput}
\begin{sageoutput}
(matrix(
 [['-0.707106781186547', '0.707106781186547'],
  ['0.707106781186547', '0.707106781186547']]),
 matrix(
 [['1.73205080756888'],
  ['1.0']]),
 matrix(
 [['-0.816496580927726', '0.408248290463863', '-0.408248290463863'],
  ['3.14018491736755e-16', '0.707106781186547', '0.707106781186548']]))
\end{sageoutput}
Note that the input isn't quite so nice this time: the algorithm is numerical. For some reason, if you define \mono{A} as a SymPy matrix, then \mono{P} will be a SymPy matrix, but \mono{S} and \mono{Q} will not. (I have no idea why.) Also, the matrix \mono{S} is a column matrix, containing the positive singular values of \(A\). We can turn it into a diagonal matrix as follows, using the \mono{diag} command from mpmath:%
\begin{sageinput}
S1 = mp.diag(S)
S1
\end{sageinput}
\begin{sageoutput}
matrix(
[['1.73205080756888', '0.0'],
 ['0.0', '1.0']])
\end{sageoutput}
In case you don't recognize the square root of 3 by its decimal approximation, we can check:%
\begin{sageinput}
S1*S1
\end{sageinput}
\begin{sageoutput}
matrix(
[['1.73205080756888', '0.0'],
 ['0.0', '1.0']])
\end{sageoutput}
The matrix \(\Sigma_A\) is supposed to be \(2\times 3\), with an additional column of zeros. If we want to define this as a SymPy matrix, we can. Note that we are using the \mono{diag} command from SymPy this time, rather than mpmath.%
\begin{sageinput}
S2 = sy.Matrix(S1)
SigA = S2.row_join(sy.Matrix([0,0]))
SigA
\end{sageinput}
\begin{sageoutput}
\[\bbm 1.73205080756888 \amp 0.0 \amp 0\\ 0.0 \amp 1.0 \amp 0\ebm\]
\end{sageoutput}
We will see in a minute that this matrix is not really necessary, because the \mono{svd} algorithm in mpmath is a bit different from the one in Nicholson.%
\par
The matrix \(P\) is already a SymPy matrix, as it turns out, but \(Q\) is not.%
\begin{sageinput}
QT
\end{sageinput}
\begin{sageoutput}
matrix(
[['-0.816496580927726', '0.408248290463863', '-0.408248290463863'],
 ['3.14018491736755e-16', '0.707106781186547', '0.707106781186548']])
\end{sageoutput}
Notice that this matrix is not square! The output from the algorithm also has the transpose already applied.%
\begin{sageinput}
Q1 = QT.T
Q1
\end{sageinput}
\begin{sageoutput}
matrix(
[['-0.816496580927726', '3.14018491736755e-16'],
 ['0.408248290463863', '0.707106781186547'],
 ['-0.408248290463863', '0.707106781186548']])
\end{sageoutput}
The matrix \mono{Q1} we get here is (up to a difference in sign) the first two columns of the matrix \(Q\) found in Nicholson. This makes sense: the matrix \(S\) that we get from the algorithm is \(D_A\), not \(\Sigma_A\). The third column of \(\Sigma_A\) is zero. So when we multiply by \(Q^T\), the third row of \(Q^T\) (that is, the third column of \(Q\)) is lost. That is, \(S(QT)=\Sigma_AQ^T\).%
\par
To confirm that everything worked, we can multiply:%
\begin{sageinput}
P*S1*QT
\end{sageinput}
\begin{sageoutput}
matrix(
[['0.999999999999999', '-3.32977184235283e-17', '0.999999999999999'],
 ['-1.0', '1.0', '1.37483317152544e-16']])
\end{sageoutput}
Maybe that's not so enlightening, given all the decimal places. Let's check the difference with the matrix \(A\):%
\begin{sageinput}
A-P*S1*QT
\end{sageinput}
\begin{sageoutput}
matrix(
[['5.55111512312578e-16', '3.32977184235283e-17', '8.88178419700125e-16'],
 ['-3.33066907387547e-16', '1.11022302462516e-16', '-1.37483317152544e-16']])
\end{sageoutput}
Looks like some pretty small numbers there. The mpmath library includes the \mono{chop} command for truncating decimals.%
\begin{sageinput}
mp.chop(A-P*S1*QT)
\end{sageinput}
\begin{sageoutput}
matrix(
[['0.0', '0.0', '0.0'],
 ['0.0', '0.0', '0.0']])
\end{sageoutput}
The Singular Value Decomposition has a lot of useful appplications, some of which are described in Nicholson's book. On a very fundamental level the \initialism{SVD} provides us with information on some of the most essential properties of the matrix \(A\), and any system of equations with \(A\) as its coefficient matrix.%
\par
Recall the following definitions for an \(m\times n\) matrix \(A\):%
\begin{enumerate}
\item{}The \terminology{rank} of \(A\) is the number of leadning ones in the \initialism{RREF} of \(A\), which is also equal to the dimension of the column space of \(A\) (or if you prefer, the dimension of \(\im (T_A)\)).%
\item{}The \terminology{column space} of \(A\), denoted \(\csp(A)\), is the subspace of \(\R^m\) spanned by the columns of \(A\). (This is the image of the matrix transformation \(T_A\); it is also the space of all vectors \(\mathbf{b}\) for which the system \(A\xx=\mathbf{b}\) is consistent.)%
\item{}The \terminology{row space} of \(A\), denoted \(\operatorname{row}(A)\), is the span of the rows of \(A\), viewed as column vectors in \(\R^n\).%
\item{}The \terminology{null space} of \(A\) is the space of solutions to the homogeneous system \(A\xx=\zer\). This is, of course, equal the kernel of the associated transformation \(T_A\).%
\end{enumerate}
%
\par
There are some interesting relationships among these spaces, which are left as an exercise. (Solutions are in Nicholson if you get stumped.)%
\begin{inlineexercise}{}{g:exercise:idp116}%
Let \(A\) be an \(m\times n\) matrix. Prove the following:%
\begin{enumerate}
\item{}\(\displaystyle (\operatorname{row}(A))^\bot = \nll(A)\)%
\item{}\(\displaystyle (\csp(A))^\bot = \nll(A^T)\)%
\end{enumerate}
%
\end{inlineexercise}%
Here's the cool thing about the \initialism{SVD}. Let \(\sigma_1\geq \sigma_2\geq \cdots \geq \sigma_r\gt 0\) be the positive singular values of \(A\). Let \(\vecq_1,\ldots, \vecq_r,\ldots, \vecq_n\) be the orthonormal basis of eigenvectors for \(A^TA\), and let \(\vecp_1,\ldots, \vecp_r,\ldots, \vecp_m\) be the orthonormal basis of \(\R^m\) constructed in the \initialism{SVD} algorithm. Then:%
\par
%
\begin{enumerate}
\item{}\(\displaystyle \rank(A)=r\)%
\item{}\(\vecq_1,\ldots, \vecq_r\) form a basis for \(\operatorname{row}(A)\).%
\item{}\(\vecp_1,\ldots, \vecp_r\) form a basis for \(\csp(A)\) (and thus, the ``row rank'' and ``column rank'' of \(A\) are the same).%
\item{}\(\vecq_{r+1},\ldots, \vecq_n\) form a basis for \(\nll(A)\). (And these are therefore the basis solutions of \(A\xx=\zer\)!)%
\item{}\(\vecp_{r+1},\ldots, \vecp_m\) form a basis for \(\nll(A^T)\).%
\end{enumerate}
%
\par
If you want to explore this further, have a look at the excellent \href{https://www.juanklopper.com/wp-content/uploads/2015/03/III_05_Singular_value_decomposition.html}{notebook by Dr. Juan H Klopper}\footnote{\nolinkurl{www.juanklopper.com/wp-content/uploads/2015/03/III_05_Singular_value_decomposition.html}\label{g:fn:idp117}}. The \mono{ipynb} file can be found \href{https://github.com/juanklopper/MIT_OCW_Linear_Algebra_18_06}{on his GitHub page}\footnote{\nolinkurl{github.com/juanklopper/MIT_OCW_Linear_Algebra_18_06}\label{g:fn:idp118}}. In it, he takes you through various approaches to finding the singular value decomposition, using the method above, as well as using NumPy and SciPy (which, for industrial applications, are superior to SymPy and mpmath).%
\end{subsubsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsubsection 4.5.1.3 Polar Decomposition}
\typeout{************************************************}
%
\begin{subsubsectionptx}{Polar Decomposition}{}{Polar Decomposition}{}{}{x:subsubsection:pars-polar-decomp}
For any \(n\times n\) matrix \(A\), there exists an orthogonal (or unitary) matrix \(P\) such that%
\begin{equation*}
A = P\sqrt{A^TA}\text{.}
\end{equation*}
This is meant to remind you of the polar decomposition%
\begin{equation*}
z = e^{i\theta}\sqrt{\bar{z}z}
\end{equation*}
for a complex number.%
\par
One way to compute the polar decomposition is using the Singular Value Decomposition (see Nicholson's text). Note that both \(P\) and \(\sqrt{A^TA}\) can be diagonalized, but usually not with the same orthonormal basis.%
\end{subsubsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsubsection 4.5.1.4 QR Factorization}
\typeout{************************************************}
%
\begin{subsubsectionptx}{QR Factorization}{}{QR Factorization}{}{}{x:subsubsection:pars-qr-factor}
Suppose \(A\) is an \(m\times n\) matrix with independent columns. (Question: for this to happen, which is true \textemdash{} \(m\geq n\), or \(n\geq m\)?)%
\par
A \(QR\)-factorization of \(A\) is a factorization of the form \(A=QR\), where \(Q\) is \(m\times n\), with orthonormal columns, and \(R\) is an invertible upper-triangular (\(n\times n\)) matrix with positive diagonal entries. If \(A\) is a square matrix, \(Q\) will be orthogonal.%
\par
A lot of the methods we're looking at here involve more sophisticated numerical techniques than SymPy is designed to handle. If we wanted to spend time on these topics, we'd have to learn a bit about the NumPy package, which has built in tools for finding things like polar decomposition and singular value decomposition. However, SymPy does know how to do \(QR\) factorization. After defining a matrix \mono{A}, we can use the command%
\begin{codedisplay}

          Q, R = A.QRdecomposition()
        
\end{codedisplay}
.%
\begin{sageinput}
from sympy import Matrix,init_printing
init_printing()
A = Matrix(3,3,[1,-2,3,3,-1,2,4,2,5])
Q, R = A.QRdecomposition()
A, Q, R
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 1\amp -2\amp 3\\ 3\amp -1\amp 2\\ 4\amp 2\amp 5\ebm, \bbm \frac{\sqrt{26}}{6} \amp -\frac{11\sqrt{26}}{78} \amp \frac23\\ \frac{3\sqrt{26}}{26} \amp -\frac{7\sqrt{26}}{78} \amp -\frac23\\ \frac{2\sqrt{26}}{13} \amp \frac{4\sqrt{26}}{39} \amp \frac13\ebm, \bbm \sqrt{26}\amp \frac{3\sqrt{26}}{26} \amp \frac{29\sqrt{26}}{26}\\ 0\amp \frac{15\sqrt{26}}{26} \amp -\frac{7\sqrt{26}}{78}\\0\amp 0\amp \frac73\ebm\right)\]
\end{sageoutput}
Let's check that the matrix \(Q\) really is orthogonal:%
\begin{sageinput}
Q**(-1) == Q.T
\end{sageinput}
\begin{sageoutput}
True
\end{sageoutput}
Details of how to perform the QR factorization can be found in Nicholson's textbook. It's essentially a consequence of performing the Gram-Schmidt algorithm on the columns of \(A\), and keeping track of our work.%
\par
The calculation above is a symbolic computation, which is nice for understanding what's going on. The reason why the \(QR\) factorization is useful in practice is that there are efficient numerical methods for doing it (with good control over rounding errors). Our next topic looks at a useful application of the \(QR\) factorization.%
\end{subsubsectionptx}
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection 4.5.2 Computing Eigenvalues}
\typeout{************************************************}
%
\begin{subsectionptx}{Computing Eigenvalues}{}{Computing Eigenvalues}{}{}{x:subsection:subsec-compute-eigen}
\begin{introduction}{}%
Our first method focuses on the dominant eigenvalue of a matrix. An eigenvalue is dominant if it is larger in absolute value than all other eigenvalues. For example, if \(A\) has eigenvalues \(1,3,-2,-5\), then \(-5\) is the dominant eigenvalue.%
\par
If \(A\) has eigenvalues \(1,3,0,-4,4\) then there is no dominant eigenvalue. Any eigenvector corresponding to a dominant eigenvalue is called a dominant eigenvector.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsubsection 4.5.2.1 The Power Method}
\typeout{************************************************}
%
\begin{subsubsectionptx}{The Power Method}{}{The Power Method}{}{}{x:subsubsection:pars-power-method}
If a matrix \(A\) has a dominant eigenvalue, there is a method for finding it (approximately) that does not involve finding and factoring the characteristic polynomial of \(A\).%
\par
We start with some initial guess \(x_0\) for a dominant eigenvector. We then set \(x_{k+1} = Ax_k\) for each \(k\geq 0\), giving a sequence%
\begin{equation*}
x_0, Ax_0, A^2x_0, A^3x_0,\ldots\text{.}
\end{equation*}
We expect (for reasons we'll explain) that \(\lVert x_k-x\rVert \to 0\) as \(k\to\infty\), where \(x\) is a dominant eigenvector. Let's try an example.%
\begin{sageinput}
A = Matrix(2,2,[1,-4,-3,5])
A,A.eigenvects()
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 1\amp -4\\-3\amp 5\ebm, \left[\left(-1, 1, \left[\bbm 2\\1\ebm\right]\right),\left(7, 1, \left[\bbm -\frac23\\1\ebm\right]\right)\right]\right)\]
\end{sageoutput}
The dominant eigenvalue is \(\lambda = 7\). Let's try an initial guess of \(x_0=\begin{bmatrix}1\\0\end{bmatrix}\) and see what happens.%
\begin{sageinput}
x0 = Matrix(2,1,[1,0])
L = list()
for k in range(10):
    L.append(A**k*x0)
L
\end{sageinput}
\begin{sageoutput}
\begin{align*}
\left[\bbm 1\\0\ebm, \bbm 1\\-3\ebm, \bbm 13\\-18\ebm,\right. \amp \bbm 85\\-129\ebm, \bbm 601\\-900\ebm, \bbm 4201\\-6303\ebm,\\ \amp \left.\bbm 29413\\ -44118\ebm, \bbm 205885\\-308829\ebm, \bbm 1441201\\-2161800\ebm, \bbm 10088401\\-15132603\ebm\right]
\end{align*}
\end{sageoutput}
We might want to confirm whether that rather large fraction is close to \(\frac23\). To do so, we can get the computer to divide the numerator by the denominator.%
\begin{sageinput}
L[9][0]/L[9][1]
\end{sageinput}
\begin{sageoutput}
\[-\frac{10088401}{15132603}, \text{ or } -0.666666600584182\]
\end{sageoutput}
The above might show you the fraction rather than its decimal approximation. (This may depend on whether you're on Sage or Jupyter.) To get the decimal, try wrapping the above in \mono{float()} (or \mono{N}, or append with \mono{.evalf()}).%
\par
For the eigenvalue, we note that if \(Ax=\lambda x\), then%
\begin{equation*}
\frac{x\cdot Ax}{\lVert x\rVert^2} = \frac{x\cdot (\lambda x)}{\lVert x\rVert^2} = \lambda\text{.}
\end{equation*}
This leads us to consider the Rayleigh quotients%
\begin{equation*}
r_k = \frac{x_k\cdot x_{k+1}}{\lVert x_k\rVert^2}\text{.}
\end{equation*}
%
\begin{sageinput}
M = list()
for k in range(9):
    M.append((L[k].dot(L[k+1]))/(L[k].dot(L[k])))
M
\end{sageinput}
\begin{sageoutput}
\begin{align*}
\left[1,\frac{67}{10}, \frac{3427}{493}, \frac{167185}{23866}\right.,\amp\frac{8197501}{1171201},\frac{401639767}{57376210},\\ \amp \left.\frac{19680613327}{2811522493},\frac{964348200085}{137763984466},\frac{47253074775001}{6750439562401}\right]
\end{align*}
\end{sageoutput}
We can convert a rational number r to a float using either \mono{N(r)} or \mono{r.evalf()}. (The latter seems to be the better bet when working with a list.)%
\begin{sageinput}
M2 = list()
for k in range(9):
    M2.append((M[k]).evalf())
M2
\end{sageinput}
\begin{sageoutput}
\begin{align*}1.0,\right.\amp 6.7,6.95131845841785,\\ \amp 7.00515377524512, 6.99922643508672, 7.00010974931945,\\ \amp \qquad \left. 6.9999843060121, 7.00000224168168, 6.9999996797533\right]\end{align*}
\end{sageoutput}
\end{subsubsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsubsection 4.5.2.2 The QR Algorithm}
\typeout{************************************************}
%
\begin{subsubsectionptx}{The QR Algorithm}{}{The QR Algorithm}{}{}{x:subsubsection:pars-qr-algorithm}
Given an \(n\times n\) matrix \(A\), we know we can write \(A=QR\), with \(Q\) orthogonal and \(R\) upper-triangular. The \(QR\)-algorithm exploits this fact. We set \(A_1=A\), and write \(A_1=Q_1R_1\).%
\par
Then we set \(A_2 = R_1Q_1\), and factor: \(A_2=Q_2R_2\). Notice \(A_2 = R_1Q_1 = Q_1^TA_1Q_1\). Since \(A_2\) is similar to \(A_1\), \(A_2\) has the same eigenvalues as \(A_1=A\).%
\par
Next, set \(A_3 = R_2Q_2\), and factor as \(A_3 = Q_3R_3\). Since \(A_3 = Q_2^TA_2Q_2\), \(A_3\) has the same eigenvalues as \(A_2\). In fact, \(A_3 = Q_2^T(Q_1^TAQ_1)Q_2 = (Q_1Q_2)^TA(Q_1Q_2)\).%
\par
After \(k\) steps we have \(A_{k+1} = (Q_1\cdots Q_k)^TA(Q_1\cdots Q_k)\), which still has the same eigenvalues as \(A\). By some sort of dark magic, this sequence of matrices converges to an upper triangular matrix with eigenvalues on the diagonal!%
\par
Consider the matrix \(A = \begin{bmatrix}5&-2&3\\0&4&0\\0&-1&3\end{bmatrix}\)%
\begin{sageinput}
A = Matrix(3,3,[5,-2,3,0,4,0,0,-1,3])
A.eigenvals()
\end{sageinput}
\begin{sageoutput}
\[\{3:1, 4:1, 5:1\}\]
\end{sageoutput}
\begin{sageinput}
Q1,R1 = A.QRdecomposition()
A2=R1*Q1
A2,Q1,R1
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 5\amp -\frac{11\sqrt{17}}{17}\amp \frac{10\sqrt{17}}{17}\\ 0\amp \frac{71}{17} \amp \frac{5}{17}\\ 0\amp -\frac{12}{17} \amp \frac{48}{17}\ebm, \bbm 1\amp 0\amp 0\\ 0\amp \frac{4\sqrt{17}}{17} \amp \frac{\sqrt{17}}{17}\\ 0\amp -\frac{\sqrt{17}}{17}, \frac{4\sqrt{17}}{17}\ebm, \bbm 5\amp -2\amp 3\\ 0\amp \sqrt{17}\amp -\frac{3\sqrt{17}}{17}\\ 0\amp 0\amp \frac{12\sqrt{17}}{17}\ebm\right)\]
\end{sageoutput}
Now we repeat the process:%
\begin{sageinput}
Q2,R2 = A2.QRdecomposition()
A3=R2*Q2
A3.evalf()
\end{sageinput}
\begin{sageoutput}
\[\bbm 5.0\amp -3.0347711718635 \amp 1.94683433666715\\ 0\amp 4.20655737704918 \amp 0.527868852459016\\ 0 \amp -0.472131147540984 \amp 2.79344262295082\ebm\]
\end{sageoutput}
Do this a few more times, and see what results! (If someone can come up with a way to code this as a loop, let me know!) The diagonal entries should get closer to \(5,4,3\), respectively, and the \((3,2)\) entry should get closer to \(0\).%
\end{subsubsectionptx}
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Worksheet 4.6 Worksheet: Singular Value Decomposition}
\typeout{************************************************}
%
\newgeometry{left=1.25cm, right=1.25cm, top=1.25cm, bottom=1.25cm}
\begin{worksheet-section}{Worksheet: Singular Value Decomposition}{}{Worksheet: Singular Value Decomposition}{}{}{x:worksheet:worksheet-svd}
For this worksheet, the reader is directed to Section 8.6 of \emph{Linear Algebra with Applications}, by Keith Nicholson, and, of course, to \hyperref[x:section:section-matrix-factor]{Section~{\xreffont\ref{x:section:section-matrix-factor}}}. (See also \href{https://www.juanklopper.com/wp-content/uploads/2015/03/III_05_Singular_value_decomposition.html}{notebook by Dr. Juan H Klopper}\footnote{\nolinkurl{www.juanklopper.com/wp-content/uploads/2015/03/III_05_Singular_value_decomposition.html}\label{g:fn:idp119}}.)%
\par
In \hyperref[x:section:section-matrix-factor]{Section~{\xreffont\ref{x:section:section-matrix-factor}}} we saw that the \mono{svd} algorithm in the mpmath library does things a little bit differently than Nicholson. If we start with a square matrix \(A\), the results are the same, but if \(A\) is not square, the decomposition \(A = P\Sigma_A Q^T\) looks a little different. In particular, if \(A\) is \(m\times n\), the matrix \(\Sigma_A\) defined in Nicholson will also be \(m\times n\), but it will contain some rows or columns of zeros that are added to get the desired size. The matrix \(Q\) is an orthogonal \(n\times n\) matrix whose columns are an orthonormal basis of eigenvectors for \(A^TA\). The matrix \(P\) is an orthogonal \(m\times m\) matrix whose columns are an orthonormal basis of \(\R^m\). (The first \(r\) columns of \(P\) are given by \(A\vecq_i\), where \(\vecq_i\) is the eigenvector of \(A^TA\) corresponding to the positive singular value \(\sigma_i\).)%
\par
The \mono{svd} algorithm provided by mpmath replaces \(\Sigma_A\) by the \(m\times m\) diagonal matrix of singular values. The matrix \(Q\) is replaced by the \(m\times n\) matrix whose columns are the first \(m\) eigenvectors of \(A^TA\). (Note that the rank of \(A^TA\) is equal to the rank of \(A\), which is equal to the number of nonzero eigenvectors of \(A^TA\) (counted with multiplicity).) So we will have \(m\geq r\), where \(r\) is the number of nonzero singular values.%
\par
The product \(\Sigma_A Q^T\) will be the same in both cases, and the matrix \(P\) is the same as well.%
This time, rather than using the mpmath algorithm, we will work through the process as outlined in Nicholson step-by-step. First, we will work through (again) Example 8.6.1 in Nicholson. Let \(A = \bbm 1\amp 0\amp 1\\-1\amp 1\amp 0\ebm\). First, we get the singular values:%
\begin{sageinput}
from sympy import Matrix,init_printing
init_printing()
A = Matrix([[1,0,1],[-1,1,0]])
L0=A.singular_values()
L0
\end{sageinput}
Next, we get the eigenvalues and eigenvectors of \(A^TA\):%
\begin{sageinput}
B = (A.T)*A
L1=B.eigenvects()
L1
\end{sageinput}
Now we need to normalize the eigenvectors, in the correct order. Note that the eigenvectors were listed in \emph{increasing} order of eigenvalue, so we need to reverse the order. Note that \mono{L1} is a list of lists. The eigenvector is the third entry (index 2) in the list (eigenvalue, multiplicity, eigenvector). We also need to turn list elements into matrices. So, for example the second eigenvector is \mono{Matrix(L1[1][2])}.%
\begin{sageinput}
R1=Matrix(L1[2][2])
R2=Matrix(L1[1][2])
R3=Matrix(L1[0][2])
Q1 = (1/R1.norm())*R1
Q2 = (1/R2.norm())*R2
Q3 = (1/R3.norm())*R3
Q1,Q2,Q3
\end{sageinput}
\clearpage
Next, we can assemble these vectors into a matrix, and confirm that it's orthogonal.%
\begin{sageinput}
from sympy import BlockMatrix
Q = Matrix(BlockMatrix([Q1,Q2,Q3]))
Q,Q*Q.T
\end{sageinput}
We've made the matrix \(Q\)! Next, we construct \(\Sigma_A\). This we will do by hand.%
\begin{sageinput}
SigA = Matrix([[L0[0],0,0],[0,L0[1],0]])
SigA
\end{sageinput}
Alternatively, you could do \mono{SigA = diag(L0[0],L0[1]).row\_join(Matrix([0,0]))}. Finally, we need to make the matrix \(P\). First, we find the vectors \(A\vecq_1, A\vecq_2\) and normalize. (Note that \(A\vecq_3=\zer\), so this vector is unneeded, as expected.)%
\begin{sageinput}
S1 = A*Q1
S2 = A*Q2
P1 = (1/S1.norm())*S1
P2 = (1/S2.norm())*S2
P = Matrix(BlockMatrix([P1,P2]))
P
\end{sageinput}
Note that the matrix \(P\) is already the correct size, because \(\rank(A)=2\dim(\R^2)\). In general, for an \(m\times n\) matrix \(A\), if \(\rank(A)=r\lt m\), we would have to extend the set \(\{\vecp_1,\ldots, \vecp_r\}\) to a basis for \(\R^m\). Finally, we check that our matrices work as advertised.%
\begin{sageinput}
P*SigA*(Q.T)
\end{sageinput}
For convenience, here is all of the above code, with all print commands (except the last one) removed. This can be run as a single code cell.%
\begin{listingptx}{SymPy code for a singular value decomposition example}{x:listing:listing-svd}{}%
\begin{program}{none}{0}{1}{0}
from sympy import Matrix,BlockMatrix,init_printing
init_printing()
A = Matrix([[1,0,1],[-1,1,0]])
B=(A.T)*A
L0=A.singular_values()
L1=B.eigenvects()
R1=Matrix(L1[2][2])
R2=Matrix(L1[1][2])
R3=Matrix(L1[0][2])
Q1 = (1/R1.norm())*R1
Q2 = (1/R2.norm())*R2
Q3 = (1/R3.norm())*R3
Q = Matrix(BlockMatrix([Q1,Q2,Q3]))
SigA = diag(L0[0],L0[1]).row_join(Matrix([0,0]))
S1 = A*Q1
S2 = A*Q2
P1 = (1/S1.norm())*S1
P2 = (1/S2.norm())*S2
P = Matrix(BlockMatrix([P1,P2]))
P,SigA,Q,P*SigA*Q.T
\end{program}
\tcblower
\end{listingptx}%
\clearpage
\begin{divisionexercise}{1}{}{7in}{g:exercise:idp120}%
Do Exercise 8.6.9 in Nicholson: compute the SVD for the matrices%
\begin{equation*}
\bbm 1\amp -1\\1\amp 0\\0\amp 1\ebm \quad \quad \bbm 1\amp 1\amp 1\\-1\amp 0\amp 2 \\1\amp 2\amp 0\ebm\text{.}
\end{equation*}
Note that for these matrices, you will need to do some additional work to extend the \(\vecp_i\) vectors to an orthonormal basis. You can adapt the code above, but you will have to think about how to implement additional code to construct any extra vectors you need.%
\end{divisionexercise}%
\clearpage
\begin{divisionexercise}{2}{}{3.5in}{g:exercise:idp121}%
Either by reading Nicholson or by searching online (or both), come up with a couple of answers to the question: ``Why are people interested in the singular value decomposition?''%
\end{divisionexercise}%
\begin{divisionexercise}{3}{}{3.5in}{g:exercise:idp122}%
(Optional) If you are interested, learn how to compute the \initialism{SVD} using tools from the NumPy and SciPy libraries instead.%
\end{divisionexercise}%
\end{worksheet-section}
\restoregeometry
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 5 Change of Basis}
\typeout{************************************************}
%
\begin{chapterptx}{Change of Basis}{}{Change of Basis}{}{}{x:chapter:ch-change-basis}
%
%
\typeout{************************************************}
\typeout{Section 5.1 The matrix of a linear transformation}
\typeout{************************************************}
%
\begin{sectionptx}{The matrix of a linear transformation}{}{The matrix of a linear transformation}{}{}{x:section:sec-matrix-of-transformation}
Recall from \hyperref[x:example:ex-matrix-trans]{Example~{\xreffont\ref{x:example:ex-matrix-trans}}} in \hyperref[x:chapter:ch-linear-trans]{Chapter~{\xreffont\ref{x:chapter:ch-linear-trans}}} that given any \(m\times n\) matrix \(A\), we can define the matrix transformation \(T_A:\R^n\to \R^m\) by \(T_A(\xx)=A\xx\), where we view \(\xx\in\R^n\) as an \(n\times 1\) column vector.%
\par
Conversely, given any linear map \(T:\R^n\to \R^m\), if we let \(\basis{e}{n}\) denote the standard basis of \(\R^n\), then the matrix%
\begin{equation*}
A = \bbm T(\mathbf{e}_1) \amp T(\mathbf{e}_2) \amp \cdots \amp T(\mathbf{e}_n)\ebm
\end{equation*}
is such that \(T=T_A\).%
\par
We have already discussed the fact that this idea generalizes: given a linear transformation \(T:V\to W\), where \(V\) and \(W\) are finite-dimensional vector spaces, it is possible to represent \(T\) as a matrix transformation.%
\par
The representation depends on choices of bases for both \(V\) and \(W\). Recall the definition of the coefficient isomorphism, from \hyperref[x:definition:def-coefficient-iso]{Definition~{\xreffont\ref{x:definition:def-coefficient-iso}}} in \hyperref[x:section:sec-isomorphism]{Section~{\xreffont\ref{x:section:sec-isomorphism}}}. If \(\dim V=n\) and \(\dim W=m\), this gives us isomorphisms \(C_B:V\to \R^n\) and \(C_D:W\to \R^m\) depending on the choice of a basis \(B\) for \(V\) and a basis \(D\) for \(W\). These isomorphisms define a matrix transformation \(T_A:\R^n\to \R^m\) according to the diagram we gave in \hyperref[x:figure:fig_transformation_matrix]{Figure~{\xreffont\ref{x:figure:fig_transformation_matrix}}}.%
\par
We should stress one important point about the coefficient isomorphism, however. It depends on the choice of basis, but also on the \emph{order} of the basis elements. Thus, we generally will work with an \emph{ordered basis} in this chapter. That is, rather than simply thinking of our basis as a set, we will think of it as an ordered list. Order matters, since given a basis \(B=\basis{e}{n}\), we rely on the fact that we can write any vector \(\vv\) uniquely as%
\begin{equation*}
\vv = c_1\mathbf{e}_1+\cdots +c_n\mathbf{e}_n
\end{equation*}
in order to make the assignment \(C_B(\vv) = \bbm c_1\\\vdots \\c_n\ebm\).%
\begin{inlineexercise}{}{g:exercise:idp123}%
Show that the coefficient isomorphism is, indeed, a linear isomorphism from \(V\) to \(\R^n\).%
\end{inlineexercise}%
Given \(T:V\to W\) and coefficient isomorphisms \(C_B:V\to \R^n, C_D:W\to \R^m\), the map \(C_DTC_B^{-1}:\R^n\to \R^m\) is a linear transformation, and the matrix of this transformation gives a representation of \(T\). Explicitly, let \(B = \basis{v}{n}\) be an ordered basis for \(V\), and let \(D=\basis{w}{m}\) be an ordered basis for \(W\). Since \(T(\vv_i)\in W\) for each \(\vv_i\in B\), there exist unique scalars \(a_{ij}\), with \(1\leq i\leq m\) and \(1\leq j\leq n\) such that%
\begin{equation*}
T(\vv_j) = a_{1j}\ww_1+a_{2j}\ww_2+\cdots + a_{mj}\ww_m
\end{equation*}
for \(j=1,\ldots, n\). This gives us the \(m\times n\) matrix \(A = [a_{ij}]\). Notice that the first column of \(A\) is \(C_D(T(\vv_1))\), the second column is \(C_D(T(\vv_2))\), and so on.%
\par
Given \(\xx\in V\), write \(\xx = c_1\vv_1+\cdots + c_n\vv_n\), so that \(C_B(\xx) = \bbm c_1\\\vdots \\c_n\ebm\). Then%
\begin{equation*}
T_A(C_B(\xx)) = \bbm a_{11}\amp a_{12} \amp \cdots \amp a_{1n}\\
a_{21}\amp a_{22} \amp \cdots \amp a_{2n}\\
\vdots \amp \vdots \amp \ddots \amp \vdots\\
a_{m1}\amp a_{m2} \amp \cdots \amp a_{mn}\ebm\bbm c_1\\c_2\\ \vdots \\c_n\ebm
= \bbm a_{11}c_1+a_{12}c_2+\cdots +a_{1n}c_n\\
a_{21}c_1+a_{22}c_2+\cdots +a_{2n}c_n\\
\vdots\\
a_{m1}c_1+a_{m2}c_2+\cdots +a_{mn}c_n\ebm\text{.}
\end{equation*}
%
\par
On the other hand,%
\begin{align*}
T(\xx) \amp = T(c_1\vv_1+\cdots + c_n\vv_n) \\
\amp = c_1T(\vv_1)+\cdots + c_nT(\vv_n)\\
\amp = c_1(a_{11}\ww_1+\cdots + a_{m1}\ww_m)+\cdots c_n(a_{1n}\ww_1+\cdots + a_{mn}\ww_m)\\
\amp = (c_1a_{11}+\cdots + c_na_{1n})\ww_1 + \cdots + (c_1a_{m1}+\cdots + c_na_{mn})\ww_m\text{.}
\end{align*}
Therefore,%
\begin{equation*}
C_D(T(\xx)) = \bbm c_1a_{11}+\cdots + c_na_{1n}\\ \vdots \\ c_1a_{m1}+\cdots + c_na_{mn}\ebm = T_A(C_B(\xx))\text{.}
\end{equation*}
Thus, we see that \(C_DT = T_AC_B\), or \(T_A = C_DTC_B^{-1}\), as expected.%
\begin{definition}{The matrix \(M_{DB}(T)\) of a linear map.}{x:definition:def-transformation-matrix}%
Let \(V\) and \(W\) be finite-dimensional vector spaces, and let \(T:V\to W\) be a linear map. Let \(B=\basis{v}{n}\) and \(D=\basis{w}{m}\) be ordered bases for \(V\) and \(W\), respectively. Then the \terminology{matrix} \(M_{DB}(T)\) of \(T\) with respect to the bases \(B\) and \(D\) is defined by%
\begin{equation*}
M_{DB}(T) = \bbm C_D(T(\vv_1)) \amp C_D(T(\vv_2)) \amp \cdots \amp C_D(T(\vv_n))\ebm\text{.}
\end{equation*}
%
\end{definition}
In other words, \(A=M_{DB}(T)\) is the unique \(m\times n\) matrix such that \(C_DT = T_AC_B\). This gives the defining property%
\begin{equation*}
C_D(T(\vv)) = M_{DB}(T)C_B(\vv)  \text{ for all } \vv\in V\text{,}
\end{equation*}
as was demonstrated above.%
\begin{inlineexercise}{}{g:exercise:idp124}%
Suppose \(T:P_2(\R)\to \R^2\) is given by%
\begin{equation*}
T(a+bx+cx^2) = (a+c,2b)\text{.}
\end{equation*}
Compute the matrix of \(T\) with respect to the bases \(B = \{1,1-x,(1-x)^2\}\) of \(P_2(\R)\) and \(D = \{(1,0),(1,-1)\}\) of \(\R^2\).%
\end{inlineexercise}%
When we compute the matrix of a transformation with respect to a non-standard basis, we don't have to worry about how to write vectors in the domain in terms of that basis. Instead, we simply plug the basis vectors into the transformation, and then determine how to write the output in terms of the basis of the codomain. However, if we want to \emph{use} this matrix to compute values of \(T:V\to W\), then we need a systematic way of writing elements of \(V\) in terms of the given basis.%
\begin{example}{Working with the matrix of a transformation.}{g:example:idp125}%
Let \(T:P_2(\R)\to \R^2\) be a linear transformation whose matrix is given by%
\begin{equation*}
M(T) = \bbm 3\amp 0 \amp 3\\-1\amp -2\amp 2\ebm
\end{equation*}
with respect to the ordered bases \(B = \{1+x, 2-x, 2x+x^2\}\) of  \(P_2(\R)\) and \(D = \{(0,1),(-1,1)\}\) of \(\R^2\). Find the value of \(T(2+3x-4x^2)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp126}{}\quad{}We need to write the input \(2+3x-4x^2\) in terms of the basis \(B\). This amounts to solving the system of equations given by%
\begin{equation*}
a(1+x)+b(2-x)+c(2x+x^2)=2+3x-4x^2\text{.}
\end{equation*}
Of course, we can easily set up and solve this system, but let's try to be systematic, and obtain a more useful result for future problems. Since we can easily determine how to write any polynomial in terms of the standard basis \(\{1,x,x^2\}\), it suffices to know how to write these three polynomials in terms of our basis.%
\par
At first, this seems like more work. After all, we now have three systems to solve:%
\begin{align*}
a_1(x+1)+b_1(2-x)+c_1(2x+x^2) \amp =1\\
a_2(x+1)+b_2(2-x)+c_2(2x+x^2) \amp =x\\
a_3(x+1)+b_3(2-x)+c_3(2x+x^2) \amp =x^2\text{.}
\end{align*}
However, all three systems have the same coefficient matrix, so we can solve them simultaneously, by adding three ``constants'' columns to our augmented matrix.%
\par
We get the matrix%
\begin{equation*}
\left[\begin{matrix}1\amp 2\amp 0\\1\amp -1\amp 2\\0\amp 0\amp 1\end{matrix}
\right\rvert\left.\begin{matrix}1\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp 1\end{matrix}\right]\text{.}
\end{equation*}
But this is exactly the augmented matrix we'd right down if we were trying to find the inverse of the matrix%
\begin{equation*}
P=\bbm 1\amp 2\amp 0\\1\amp -1\amp 2\\0\amp 0\amp 1\ebm
\end{equation*}
whose columns are the coefficient representations of our given basis vectors in terms of the standard basis.%
\par
To compute \(P^{-1}\), we use the computer:%
\begin{sageinput}
from sympy import Matrix, init_printing
init_printing()
P = Matrix(3,3,[1,2,0,1,-1,2,0,0,1])
P**-1
\end{sageinput}
\begin{sageoutput}
\[\bbm \frac13\amp \frac23\amp -\frac43\\ \frac13\amp -\frac13 \amp \frac23\\0\amp 0\amp 1\ebm\]
\end{sageoutput}
Next, we find \(M(T)P^{-1}\):%
\begin{sageinput}
M = Matrix(2,3,[3,0,3,-1,-2,2])
v = Matrix(3,1,[2,3,-4])
M*P**-1
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp 2\amp -1\\-1\amp 0\amp 2\ebm\]
\end{sageoutput}
This matrix first converts the coefficient vector for a polynomial \(p(x)\) with respect to the standard basis into the coefficient vector for our given basis \(B\), and then multiplies by the matrix representing our transformation. The result will be the coefficient vector for \(T(p(x))\) with respect to the basis \(D\).%
\par
The polynomial \(p(x) = 2+3x-4x^2\) has coefficient vector \(\bbm 2\\3\\-4\ebm\) with respect to the standard basis. We find that \(M(T)P^{-1}\bbm 2\\3\\-4\ebm = \bbm 12\\-10\ebm\):%
\begin{sageinput}
M = Matrix(2,3,[3,0,3,-1,-2,2])
v = Matrix(3,1,[2,3,-4])
(M*P**-1)*v
\end{sageinput}
\begin{sageoutput}
\[\bbm 12\\-10\ebm\]
\end{sageoutput}
The coefficients \(12\) and \(-10\) are the coefficients of \(T(p(x))\) with repsect to the basis \(D\). Thus,%
\begin{equation*}
T(2+3x-4x^2) = 12(0,1)-10(-1,1) = (10,2)\text{.}
\end{equation*}
Note that in the last step we gave the ``simplified'' answer \((10,2)\), which is simplified primarily in that it is expressed with respect to the standard basis.%
\par
Note that we can also introduce the matrix \(Q = \bbm 0\amp -1\\1\amp 1\ebm\) whose columns are the coefficient vectors of the vectors in the basis \(D\) with respect to the standard basis. The effect of multiplying by \(Q\) is to convert from coefficients with respect to \(D\) into a coefficient vector with respect to the standard basis. We can then write a new matrix \(\hat{M}(T) = QM(T)P^{-1}\); this new matrix is now the matrix representation of \(T\) with respect to the \emph{standard} bases of \(P_2(\R)\) and \(\R^2\).%
\begin{sageinput}
Q = Matrix(2,2,[0,-1,1,1])
Q*M*P**-1
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp 0\amp -2\\0\amp 2\amp 1\ebm\]
\end{sageoutput}
We check that%
\begin{equation*}
\hat{M}(T)\bbm 2\\3\\-4\ebm = \bbm 10\\2\ebm\text{,}
\end{equation*}
as before.%
\begin{sageinput}
(Q*M*P**-1)*v
\end{sageinput}
\begin{sageoutput}
\[\bbm 10\\2 \ebm\]
\end{sageoutput}
We find that \(\tilde{M}(T) = \bbm 1\amp 0\amp -2\\0\amp 2\amp 1\ebm\). This lets us determine that for a general polynomial \(p(x) = a+bx+cx^2\),%
\begin{equation*}
\hat{M}(T)\bbm a\\b\\c\ebm = \bbm a-2c\\2b+c\ebm\text{,}
\end{equation*}
and therefore, our original transformation must have been%
\begin{equation*}
T(a+bx+cx^2)=(a-2c,2b+c)\text{.}
\end{equation*}
%
\end{example}
The previous example illustrated some important observations that are true in general. We won't give the general proof, but we sum up the results in a theorem.%
\begin{theorem}{}{}{x:theorem:thm-change-basis-transformation}%
Suppose \(T:V\to W\) is a linear transformation, and suppose \(M_0 = M_{D_0B_0}(T)\) is the matrix of \(T\) with respect to bases \(B_0\) of \(V\) and \(D_0\) of \(W\). Let \(B_1=\basis{v}{n}\) and \(D_1=\basis{w}{m}\) be any other choice of basis for \(V\) and \(W\), respectively. Let%
\begin{align*}
P \amp =\bbm C_{B_0}(\vv_1) \amp C_{B_0}(\vv_2) \amp \cdots \amp C_{B_0}(\vv_n)\ebm\\
Q \amp =\bbm C_{D_0}(\ww_1) \amp C_{D_0}(\ww_2) \amp \cdots \amp C_{DB_0}(\ww_n)\ebm
\end{align*}
be matrices whose columns are the coefficient vectors of the vectors in \(B_1,D_1\) with respect to \(B_0,D_0\). Then the matrix of \(T\) with respect to the bases \(B_1\) and \(D_1\) is%
\begin{equation*}
M_{D_0B_0}(T) = QM_{D_1B_1}(T)P^{-1}\text{.}
\end{equation*}
%
\end{theorem}
The relationship between the different maps is illustrated in \hyperref[x:figure:fig-basis-cube]{Figure~{\xreffont\ref{x:figure:fig-basis-cube}}} below. In this figure, the maps \(V\to V\) and \(W\to W\) are the identity maps, corresponding to representing the same vector with respect to two different bases. The vertical arrows are the coefficient isomorphisms \(C_{B_0},C_{B_1},C_{D_0},C_{D_1}\).%
\par
In the \initialism{HTML} version of the book, you can click and drag to rotate the figure below.%
\begin{figureptx}{Diagramming matrix of a transformation with respect to two different choices of basis}{x:figure:fig-basis-cube}{}%
\centering
\begin{image}{0.165}{0.67}{0.165}%
\includegraphics[width=\linewidth]{external/images/basis_cube.pdf}
\end{image}%
\tcblower
\end{figureptx}%
We generally apply \hyperref[x:theorem:thm-change-basis-transformation]{Theorem~{\xreffont\ref{x:theorem:thm-change-basis-transformation}}} in the case that \(B_0,D_0\) are the \emph{standard} bases for \(V,W\), since in this case, the matrices \(M_0, P, Q\) are easy to determine, and we can use a computer to calculate \(P^{-1}\) and the product \(QM_0P^{-1}\).%
\begin{inlineexercise}{}{g:exercise:idp127}%
Suppose \(T:M_{22}(\R)\to P_2(\R)\) has the matrix%
\begin{equation*}
M_{DB}(T) = \bbm 2\amp -1\amp 0\amp 3\\0\amp 4\amp -5\amp 1\\-1\amp 0\amp 3\amp -2\ebm
\end{equation*}
with respect to the bases%
\begin{equation*}
B = \left\{\bbm 1\amp 0\\0\amp 0\ebm, \bbm 0\amp 1\\0\amp 1\ebm, \bbm 0\amp 1\\1\amp 0\ebm, \bbm 1\amp 0\\0\amp 1\ebm\right\}
\end{equation*}
of \(M_{22}(\R)\) and \(D=\{1,x,x^2\}\) of \(P_2(\R)\). Determine a formula for \(T\) in terms of a general input \(X=\bbm a\amp b\\c\amp d\ebm\).%
\end{inlineexercise}%
In textbooks such as Sheldon Axler's \emph{Linear Algebra Done Right} that focus primarily on linear transformations, the above construction of the matrix of a transformation with respect to choices of bases can be used as a primary motivation for introducing matrices, and determining their algebraic properties. In particular, the rule for matrix multiplication, which can seem peculiar at first, can be seen as a consequence of the composition of linear maps.%
\begin{theorem}{}{}{x:theorem:thm-matrix-multiplication}%
Let \(U,V,W\) be finite-dimensional vectors spaces, with ordered bases \(B_1,B_2,B_3\), respectively. Let \(T:U\to V\) and \(S:V\to W\) be linear maps. Then%
\begin{equation*}
M_{B_3B_1}(ST) = M_{B_3B_2}(S)M_{B_2B_1}(T)\text{.}
\end{equation*}
%
\end{theorem}
\begin{proof}{}{g:proof:idp128}
Let \(\xx\in U\). Then \(C_{B_3}(ST(\xx)) = M_{B_3B_1}(ST)C_{B_1}(\xx)\). On the other hand,%
\begin{align*}
M_{B_3B_2}(S)M_{B_2B_1}(T)C_{B_1}(\xx) \amp  = M_{B_3B_2}(S)(C_{B_2}(T(\xx)))\\
\amp = C_{B_3}(S(T(\xx))) = C_{B_3}(ST(\xx))\text{.}
\end{align*}
Since \(C_{B_3}\) is invertible, the result follows.%
\end{proof}
Being able to express a general linear transformation in terms of a matrix is useful, since questions about linear transformations can be converted into questions about matrices that we already know how to solve. In particular,%
\begin{itemize}[label=\textbullet]
\item{}\(T:V\to W\) is an isomorphism if and only if \(M_{DB}(T)\) is invertible for some (and hence, all) choice of bases \(B\) of \(V\) and \(D\) of \(W\).%
\item{}The rank of \(T\)  is equal to the rank of \(M_{DB}(T)\) (and this does not depend on the choice of basis).%
\item{}The kernel of \(T\) is isomorphic to the nullspace of \(M_{DB}(T)\).%
\end{itemize}
%
\par
Next, we will want to look at two topics in particular. First, if \(T:V\to V\) is a linear operator, then it makes sense to consider the matrix \(M_B(T)=M_{BB}(T)\) obtained by using the same basis for both domain and codomain. Second, we will want to know how this matrix changes if we change the choice of basis.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 5.2 The matrix of a linear operator}
\typeout{************************************************}
%
\begin{sectionptx}{The matrix of a linear operator}{}{The matrix of a linear operator}{}{}{x:section:sec-matrix-operator}
Recall that a linear transformation \(T:V\to V\) is referred to as a \terminology{linear operator}. Recall also that two matrices \(A\) and \(B\) are \terminology{similar} if there exists an invertible matrix \(P\) such that \(B = PAP^{-1}\), and that similar matrices have a lot of properties in common. In particular, if \(A\) is similar to \(B\), then \(A\) and \(B\) have the same trace, determinant, and eigenvalues. One way to understand this is the realization that two matrices are similar if they are representations of the \emph{same} operator, with respect to \emph{different} bases.%
\par
Since the domain and codomain of a linear operator are the same, we can consider the matrix \(M_{DB}(T)\) where \(B\) and \(D\) are the \emph{same} ordered basis. This leads to the next definition.%
\begin{definition}{}{x:definition:def-operator-matrix}%
Let \(T:V\to V\) be a linear operator, and let \(B=\basis{b}{n}\) be an ordered basis of \(V\). The matrix \(M_B(T)=M_{BB}(T)\) is called the \terminology{\(B\)-matrix} of \(T\).%
\end{definition}
The following result collects several useful properties of the \(B\)-matrix of an operator. Most of these were already encountered for the matrix \(M_{DB}(T)\) of a transformation, although not all were stated formally. (Formal statements can be found in the textbook by Nicholson.)%
\begin{theorem}{}{}{x:theorem:thm-bmatrix-properties}%
Let \(T:V\to V\) be a linear operator, and let \(B=\basis{b}{n}\) be a basis for \(V\). Then%
\begin{enumerate}
\item{}\(C_B(T(\vv))=M_B(T)C_B(\vv)\) for all \(\vv\in V\).%
\item{}If \(S:V\to V\) is another operator, then \(M_B(ST)=M_B(S)M_B(T)\).%
\item{}\(T\) is an isomorphism if and only if \(M_B(T)\) is invertible.%
\item{}If \(T\) is an isomorphism, then \(M_B(T^{-1}) = [M_B(T)]^{-1}\).%
\item{}\(M_B(T)=\bbm C_B(T(\mathbf{b}_1)) \amp \cdots \amp C_B(T(\mathbf{b}_n))\ebm\).%
\end{enumerate}
%
\end{theorem}
\begin{example}{}{g:example:idp129}%
Find the \(B\)-matrix of the operator \(T:P_2(\R)\to P_2(\R)\) given by \(T(p(x))=p(0)(1+x^2)+p(1)x\), with respect to the ordered basis%
\begin{equation*}
B = \{1-x, x+3x^2, 2-x^2\}\text{.}
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp130}{}\quad{}We compute%
\begin{align*}
T(1-x) \amp = 1(1+x^2)+0(x) = 1+x^2\\
T(x+3x^2) \amp = 0(1+x^2)+4x=4x\\
T(2-x^2) \amp = 2(1+x^2)+1(x) = 2+x+2x^2\text{.}
\end{align*}
We now need to write each of these in terms of the basis \(B\). We can do this by working out how to write each polynomial above in terms of \(B\). Or we can be systematic.%
\par
Let \(P = \bbm 1\amp 0\amp 2\\-1\amp 1\amp 0\\0\amp 3\amp -1\ebm\) be the matrix whose columns are given by the coefficient representations of the polynomials in \(B\) with respect to the \emph{standard basis} \(B_0=\{1,x,x^2\}\). For \(T(1-x)=1+x^2\) we need to solve the equation%
\begin{equation*}
a(1-x)+b(x+3x^2)+c(2-x^2)=1+x^2
\end{equation*}
for scalars \(a,b,c\). But this is equivalent to the system%
\begin{align*}
a+2c \amp =1\\
-a+b \amp =0\\
3b-c \amp =1\text{,}
\end{align*}
which, in turn, is equivalent to the matrix equation%
\begin{equation*}
\bbm 1\amp 0\amp 2\\-1\amp 1\amp 0\\0\amp 3\amp -1\ebm\bbm a\\b\\c\ebm = \bbm 1\\0\\1\ebm\text{;}
\end{equation*}
that is, \(PC_B(1+x^2) = C_{B_0}(1+x^2)\). Thus,%
\begin{equation*}
C_B(1+x^2) = \bbm a\\b\\c\ebm = P^{-1}C_{B_0}(1+x^2) = P^{-1}\bbm 1\\0\\1\ebm\text{.}
\end{equation*}
Similarly, \(C_B(4x)=P^{-1}\bbm 0\\4\\0\ebm = P^{-1}C_{B_0}(4x)\), and \(C_B(2+x+2x^2)=P^{-1}\bbm 2\\1\\2\ebm = P^{-1}C_{B_0}(2+x+2x^2)\). Using the computer, we find:%
\begin{sageinput}
from sympy import Matrix, init_printing
init_printing()
P = Matrix(3,3,[1,0,2,-1,1,0,0,3,-1])
M = Matrix(3,3,[1,0,2,0,4,1,1,0,2])
P**-1, P**-1*M
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 1/7\amp -6/7\amp 3/7\\1/7\amp 1/7\amp 2/7\\3/7\amp 3/7\amp -1/7\ebm, \bbm 3/7\amp -24/7\amp 0\\3/7\amp 4/7\amp 1\\2/7\amp 12/7\amp 1\ebm\right)\]
\end{sageoutput}
That is,%
\begin{align*}
M_B(T) \amp =\bbm C_B(T(1-x)) \amp C_B(T(x+3x^2)) \amp C_B(T(2-x^2))\ebm\\
\amp =\bbm P^{-1}C_{B_0}T(1-x) \amp P^{-1} C_{B_0}T(x+3x^2) \amp P^{-1}C_{B_0}(T(2-x^2))\ebm\\
\amp =P^{-1}\bbm C_{B_0}(1+x^2) \amp C_{B_0}(4x)\amp C_{B_0}(2+x+2x^2)\ebm\\
\amp =\bbm 1/7\amp -6/7\amp 3/7\\1/7\amp 1/7\amp 2/7\\3/7\amp 3/7\amp -1/7\ebm
\bbm 1\amp 0\amp 2\\0\amp 4\amp 1\\1\amp 0\amp 2\ebm\\
\amp = \bbm 3/7\amp -24/7\amp 0\\3/7\amp 4/7\amp 1\\2/7\amp 12/7\amp 1\ebm\text{.}
\end{align*}
%
\par
Let's confirm that this works. Suppose we have%
\begin{align*}
p(x) \amp = C_B^{-1}\bbm a\\b\\c\ebm = a(1-x)+b(x+3x^2)+c(2-x^2)\\
\amp = (a+2c)+(-a+b)x+(3b-c)x^2\text{.}
\end{align*}
Then \(T(p(x))=(a+2c)(1+x^2)+(4b+c)x\), and we find%
\begin{equation*}
C_B(T(p(x))) = P^{-1}\bbm a+2c\\4b+c\\a+2c\ebm = \bbm \frac37 a-\frac{24}{7}b\\\frac37 a+\frac47 b+c\\\frac27 a+\frac{12}{7}b+c\ebm\text{.}
\end{equation*}
On the other hand,%
\begin{equation*}
M_B(T) = \bbm 3/7\amp -24/7\amp 0\\3/7\amp 4/7\amp 1\\2/7\amp 12/7\amp 1\ebm\bbm a\\b\\c\ebm = \bbm \frac37 a-\frac{24}{7}b\\\frac37 a+\frac47 b+c\\\frac27 a+\frac{12}{7}b+c\ebm\text{.}
\end{equation*}
The results agree, but possibly leave us a little confused.%
\end{example}
In general, given an ordered basis \(B=\basis{b}{n}\) for a vector space \(V\) with standard basis \(B_0 = \basis{e}{n}\), if we let%
\begin{equation*}
P = \bbm C_{B_0}(\mathbf{b}_1) \amp \cdots \amp C_{B_0}(\mathbf{b}_n)\ebm\text{,}
\end{equation*}
then%
\begin{align*}
M_B(T) \amp = \bbm C_B(T(\mathbf{b}_1)) \amp \cdots \amp C_B(T(\mathbf{b_n})\ebm\\
\amp = P^{-1}\bbm C_{B_0}(T(\mathbf{b}_1)) \amp \cdots \amp C_{B_0}(T(\mathbf{b_n})\ebm\text{.}
\end{align*}
%
\par
As we saw above, this gives us the result, but doesn't shed much light on the problem, unless we have an easy way to write vectors in terms of the basis \(B\). Let's revisit the problem. Instead of using the given basis \(B\), let's use the standard basis \(B_0 = \{1,x,x^2\}\). We quickly find%
\begin{equation*}
T(1)=1+x+x^2, T(x) = x, \text{ and } T(x^2)=x\text{,}
\end{equation*}
so with respect to the standard basis, \(M_{B_0}(T) = \bbm 1\amp 0\amp 0\\1\amp 1\amp 1\\1\amp 0\amp 0\ebm\). Now, recall that%
\begin{equation*}
M_{B}(T)=P^{-1}\bbm C_{B_0}T(1-x)\amp C_{B_0}(T(x+3x^2)\amp C_{B_0}(T(2-x^2))\ebm
\end{equation*}
and note that for any polynomial \(p(x)\), \(C_{B_0}T(p(x)) = M_{B_0}C_{B_0}(p(x))\). But%
\begin{equation*}
\bbm C_{B_0}(1-x) \amp C_{B_0}(x+3x^2)\amp C_{B_0}(2-x^2)\ebm = P\text{,}
\end{equation*}
so we get%
\begin{align*}
M_B(T) \amp = P^{-1}\bbm C_{B_0}T(1-x)\amp C_{B_0}(T(x+3x^2)\amp C_{B_0}(T(2-x^2))\ebm\\
\amp = P^{-1}\bbm M_{B_0}C_{B_0}(1-x)\amp M_{B_0}C_{B_0}(x+3x^2)\amp M_{B_0}C_{B_0}(2-x^2)\ebm \\
\amp = P^{-1}M_{B_0}\bbm C_{B_0}(1-x) \amp C_{B_0}(x+3x^2)\amp C_{B_0}(2-x^2)\ebm\\
\amp = P^{-1}M_{B_0}P\text{.}
\end{align*}
%
\par
Now we have a much more efficient method for arriving at the matrix \(M_B(T)\). The matrix \(M_{B_0}(T)\) is easy to determine, the matrix \(P\) is easy to determine, and with the help of the computer, it's easy to compute \(P^{-1}M_{B_0}P = M_B(T)\).%
\begin{sageinput}
from sympy import Matrix, init_printing
init_printing()
M0 = Matrix(3,3,[1,0,0,1,1,1,1,0,0])
P**-1*M0*P
\end{sageinput}
\begin{sageoutput}
\[\bbm 3/7\amp -24/7\amp 0\\3/7\amp 4/7\amp 1\\2/7\amp 12/7\amp 1\ebm\]
\end{sageoutput}
\begin{inlineexercise}{}{g:exercise:idp131}%
Determine the matrix of the operator \(T:\R^3\to \R^3\) given by%
\begin{equation*}
T(x,y,z) = (3x-2y+4z,x-5y,2y-7z)
\end{equation*}
with respect to the ordered basis%
\begin{equation*}
B = \{(1,2,0),(0,-1,2),(1,2,1)\}\text{.}
\end{equation*}
(You may want to use the Sage cell below for computational assistance.)%
\end{inlineexercise}%
The matrix \(P\) used in the above examples is known as a \emph{change} matrix. If the columns of \(P\) are the coefficient vectors of \(B=\basis{b}{n}\) with respect to another basis \(D\), then we have%
\begin{align*}
P \amp= \bbm C_D(\mathbf{b}_1)\amp\cdots \amp C_D(\mathbf{b}_n)\ebm\\
\amp = \bbm C_D(1_V(\mathbf{b}_1))\amp \cdots \amp C_D(1_V(\mathbf{b}_n))\ebm\\
\amp = M_{DB}(1_V)\text{.}
\end{align*}
In other words, \(P\) is the matrix of the identity transformation \(1_V:V\to V\), where we use the basis \(B\) for the domain, and the basis \(D\) for the codomain.%
\begin{definition}{}{x:definition:def-change-matrix}%
The \terminology{change matrix} with respect to ordered bases \(B,D\) of \(V\) is denoted \(P_{D\leftarrow B}\), and defined by%
\begin{equation*}
P_{D\leftarrow B} = M_{DB}(1_V)\text{.}
\end{equation*}
%
\end{definition}
\begin{theorem}{}{}{x:theorem:thm-change-matrix}%
Let \(B=\basis{b}{n}\) and \(D\) be two ordered bases of \(V\). Then%
\begin{equation*}
P_{D\leftarrow B} = \bbm C_D(\mathbf{b}_1)\amp \cdots \amp C_D(\mathbf{b}_n)\ebm\text{,}
\end{equation*}
and satisfies \(C_D(\vv) = P_{D\leftarrow B}C_B(\vv)\) for all \(\vv\in V\).%
\par
The matrix \(P_{D\leftarrow B}\) is invertible, and \((P_{D\leftarrow B})^{-1} = P_{B\leftarrow D}\). Moreover, if \(E\) is a third ordered basis, then%
\begin{equation*}
P_{E\leftarrow D}P_{D\leftarrow B} = P_{E\leftarrow B}\text{.}
\end{equation*}
%
\end{theorem}
\begin{example}{}{g:example:idp132}%
Let \(B = \{1,x,x^2\}\) and let \(D = \{1+x,x+x^2,2-3x+x^2\}\) be ordered bases of \(P_2(\R)\). Find the change matrix \(P_{D\leftarrow B}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp133}{}\quad{}Finding this matrix requires us to first write the vectors in \(B\) in terms of the vectors in \(D\). However, it's much easier to do this the other way around. We easily find%
\begin{equation*}
P_{B\leftarrow D} = \bbm 1\amp 0\amp 2\\1\amp 1\amp -3\\0\amp 1\amp 1\ebm\text{,}
\end{equation*}
and by \hyperref[x:theorem:thm-change-matrix]{Theorem~{\xreffont\ref{x:theorem:thm-change-matrix}}}, we have%
\begin{equation*}
P_{D\leftarrow B} = (P_{B\leftarrow D})^{-1} = \frac16\bbm 4\amp 2\amp -2\\-1\amp 1\amp 5\\1\amp -1\amp 1\ebm\text{.}
\end{equation*}
%
\end{example}
Note that the change matrix notation is useful for linear transformations between different vector spaces as well. Recall \hyperref[x:theorem:thm-change-basis-transformation]{Theorem~{\xreffont\ref{x:theorem:thm-change-basis-transformation}}}, which gave the result%
\begin{equation*}
M_{D_0B_0}(T) = QM_{D_1B_1}P^{-1}\text{,}
\end{equation*}
where (using our new notation) \(P=P_{B_0\leftarrow B_1}\) and \(Q=P_{D_0\leftarrow D_1}\). In this notation, we have%
\begin{equation*}
M_{D_0B_0}(T) = P_{D_0\leftarrow D_1}M_{D_1B_1}(T)P_{B_1\leftarrow B_0}\text{,}
\end{equation*}
which seems more intiutive.%
\par
The above results give a straightforward procedure for determining the matrix of any operator, with respect to any basis, if we let \(D\) be the standard basis. The importance of these results is not just their computational simplicity, however. The most important outcome of the above is that if \(M_B(T)\) and \(M_D(T)\) give the matrix of \(T\) with respect to two different bases, then%
\begin{equation*}
M_B(T) = (P_{D\leftarrow B})^{-1}M_D(T)P_{D\leftarrow B}\text{,}
\end{equation*}
so that the two matrices are \emph{similar}.%
\par
Recall from \hyperref[x:theorem:thm-similar-properties]{Theorem~{\xreffont\ref{x:theorem:thm-similar-properties}}} that similar matrices have the same determinant, trace, and eigenvalues. This means that we can unambiguously define the determinant and trace of an  \emph{operator}, and that we can compute eigenvalues of an operator using any matrix representation of that operator.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 5.3 Direct Sums and Invariant Subspaces}
\typeout{************************************************}
%
\begin{sectionptx}{Direct Sums and Invariant Subspaces}{}{Direct Sums and Invariant Subspaces}{}{}{x:section:sec-direct-sum}
\begin{introduction}{}%
Much of this section has been mentioned previously in the course (and these notes), but we will follow the organization of Nicholson's textbook, and reprise these concepts in more detail than previously.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection 5.3.1 Invariant subspaces}
\typeout{************************************************}
%
\begin{subsectionptx}{Invariant subspaces}{}{Invariant subspaces}{}{}{x:subsection:subsec-invariant}
\begin{definition}{}{x:definition:def-invariant-subspace}%
Given an operator \(T:V\to V\), we say that a subspace \(U\subseteq V\) is \(T\)-\terminology{invariant} if \(T(\uu)\in U\) for all \(\uu\in U\).%
\end{definition}
In other words, a subspace \(U\) is \(T\)-invariant if \(T\) does not map any vectors in \(U\) outside of \(U\). Notice that if we shrink the domain of \(T\) to \(U\), then we get an operator from \(U\) to \(U\), since the image \(T(U)\) is contained in \(U\).%
\par
Given a basis \(B=\basis{u}{k}\) of \(U\), note that \(U\) is \(T\)-invariant if and only if \(T(\uu_i)\in U\) for each \(i=1,2,\ldots, k\).%
\par
For any operator \(T:V\to V\), there are four subspaces that are always \(T\)-invariant:%
\begin{equation*}
\{\mathbf{0}\}, V, \ker T, \text{ and } \im T\text{.}
\end{equation*}
Of course, some of these subspaces might be the same; for example, if \(T\) is invertible, then \(\ker T = \{\mathbf{0}\}\) and \(\im T = V\). (We will skip the proof that \(\ker T\) and \(\im T\) are \(T\)-invariant, since this has been assigned as homework!)%
\begin{definition}{}{x:definition:def-restriction}%
Let \(T:V\to V\) be a linear operator, and let \(U\) be a \(T\)-invariant subspace. The \terminology{restriction} of \(T\) to \(U\), denoted \(T|_U\), is the operator \(T|_U:U\to U\) defined by \(T|_U(\uu)=T(\uu)\) for all \(\uu\in U\).%
\end{definition}
Notice that the restriction \(T|_U\) is defined by the same ``rule'' as \(T\), but its domain is the subspace \(U\) instead of the entire vector space \(V\).%
\par
A lot can be learned by studying the restrictions of an operator to invariant subspaces. Indeed, the textbook by Axler does almost everything from this point of view. One reason to study invariant subspaces is that they allow us to put the matrix of \(T\) into simpler forms.%
\begin{theorem}{}{}{x:theorem:thm-invariant-block-triangular}%
Let \(T:V\to V\) be a linear operator, and let \(U\) be a \(T\)-invariant subspace. Let \(B_U = \basis{u}{k}\) be a basis of \(U\), and extend this to a basis%
\begin{equation*}
B = \{\uu_1,\ldots, \uu_k,\ww_1,\ldots, \ww_{n-k}\}
\end{equation*}
of \(V\). Then the matrix \(M_B(T)\) with respect to this basis has the block-triangular form%
\begin{equation*}
M_B(T) = \bbm M_{B_U}(T_U) \amp P\\0 \amp Q\ebm
\end{equation*}
for some \((n-k)\times (n-k)\) matrix \(Z\).%
\end{theorem}
Reducing a matrix to block triangular form is useful, because it simplifies computations such as determinants and eigenvalues (and determinants and eigenvalues are computationally expensive). In particular, if a matrix \(A\) has the block form%
\begin{equation*}
A = \bbm A_{11} \amp A_{12} \amp \cdots \amp A_{1n}\\
0\amp A_{22} \amp \cdots \amp A_{2n}\\
\vdots \amp \vdots \amp \ddots \amp \vdots\\
0 \amp 0 \amp \cdots \amp A_{nn}\ebm\text{,}
\end{equation*}
where the diagonal blocks are square matrices, then \(\det(A) = \det(A_{11})\det(A_{22})\cdots \det(A_{nn})\) and \(c_A(x) = c_{A_{11}}(x)c_{A_{22}}(x)\cdots C_{A_{nn}}(x)\).%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection 5.3.2 Eigenspaces}
\typeout{************************************************}
%
\begin{subsectionptx}{Eigenspaces}{}{Eigenspaces}{}{}{x:subsection:subsec-eigenspace}
An important source of invariant subspaces is eigenspaces. Recall that for any real number \(\lambda\), and any operator \(T:V\to V\), we define%
\begin{equation*}
E_\lambda(T) = \ker(T-\lambda 1_V) = \{\vv\in V \,|\, T(\vv) = \lambda\vv\}\text{.}
\end{equation*}
For most values of \(\lambda\), we'll have \(E_\lambda(T)=\{\mathbf{0}\}\). The values of \(\lambda\) for which \(E_\lambda(T)\) is non-trivial are precisely the eigenvalues of \(T\). Note that since similar matrices have the same characteristic polynomial any matrix representation \(M_B(T)\) will have the same eigenvalues. They do \emph{not} generally have the same eigenspaces, but we do have the following.%
\begin{theorem}{}{}{x:theorem:thm-eigenspace-invariant}%
Let \(T:V\to V\) be a linear operator. For any scalar \(\lambda\), the eigenspace \(E_\lambda(T)\) is \(T\)-invariant. Moreover, for any ordered basis \(B\) of \(V\), the coefficient isomorphism \(C_B:V\to \R^n\) induces an isomorphism%
\begin{equation*}
C_B|_{E_\lambda(T)}:E_\lambda(T)\to E_{\lambda}(M_B(T))\text{.}
\end{equation*}
%
\end{theorem}
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection 5.3.3 Direct Sums}
\typeout{************************************************}
%
\begin{subsectionptx}{Direct Sums}{}{Direct Sums}{}{}{x:subsection:subsec-direct-sum}
Recall that for any subspaces \(U,W\) of a vector space \(V\), the sets%
\begin{align*}
U+W \amp =\{\uu+\ww \,|\, \uu\in U \text{ and } \ww\in W\}\\
U\cap W \amp = \{\vv \in V \,|\, \vv\in U \text{ and } \vv\in W\}
\end{align*}
are subspaces of \(V\). Saying that \(\vv\in U+W\) means that \(\vv\) can be written as a sum of a vector in \(U\) and a vector in \(W\). However, this sum may not be unique. If \(\vv\in U\cap W\), \(\uu\in U\) and \(\ww\in W\), then we can write \((\uu+\vv)+\ww = \uu + (\vv+\ww)\), giving two different representations of a vector as an element of \(U+W\).%
\begin{theorem}{}{}{x:theorem:thm-unique-sum}%
Let \(U\) and \(W\) be subspaces of a vector space \(V\). Let \(\vv\in U+W\). Then there exist \emph{unique} vectors \(\uu\in U, \ww\in W\) such that \(\vv = \uu+\ww\) if and only if \(U\cap W = \{\mathbf{0}\}\).%
\end{theorem}
\begin{proof}{}{g:proof:idp134}
Suppose \(U+W\) has the property that if \(\vv\in U+W\), then there exist unique \(\uu\in U,\ww\in W\) such that \(\vv=\uu+\ww\). Suppose \(\mathbf{x}\in U\cap W\). Then \(\xx\in U\) and \(\xx\in W\), which implies that \(-\xx\in W\), since \(W\) is a subspace. Then we can write%
\begin{equation*}
\mathbf{0} = \xx + (-\xx)\text{,}
\end{equation*}
with \(\xx\in U\) and \(-\xx\in W\). But we also know that \(\mathbf{0}\in U\) and \(\mathbf{0}\in W\), so we can also write \(\mathbf{0}=\mathbf{0}+\mathbf{0}\). Since we can only write \(\mathbf{0}\) in one way as a sum of a vector in \(U\) and a vector in \(W\), we must have \(\xx=\mathbf{0}\), showing that \(U\cap W = \{\mathbf{0}\}\).%
\par
Conversely, suppose that \(U\cap W = \{\zer\}\), and let \(\vv\in U+W\). Suppose that there exist vectors \(\uu_1,\uu_2\in U\) and \(\ww_1,\ww_2\in W\) such that \(\vv = \uu_1+\ww_1=\uu_2+\ww_2\). But then \(\uu_1-\uu_2=\ww_2-\ww_1\), and since \(\uu_1-\uu_2\in U\) and \(\ww_2-\ww_1\in W\), we have \(\uu_1-\uu_2=\ww_2-\ww_1\in U\cap W\). Since \(U\cap W=\{\zer\}\), we have \(\uu_1-\uu_2=\zer\) and \(\ww_2-\ww_1=\zer\), so \(\uu_1=\uu_2\) and \(\ww_1=\ww_2\).%
\end{proof}
\begin{definition}{}{x:definition:def-direct-sum}%
We say that a sum \(U+W\) is a \terminology{direct sum}, and write this sum as \(U\oplus W\), if \(U\cap W = \{\mathbf{0}\}\).%
\end{definition}
Typically we are interested in the case that the two subspaces sum to \(V\). If \(V = U\oplus W\), we say that \(W\) is a \terminology{complement} of \(U\), and that \(U\oplus W\) is a direct sum decomposition of \(V\). Of course, the orthogonal complement \(U^\bot\) of a subspace \(U\) is a complement in this sense, if \(V\) is equipped with an inner product. (Without an inner product we have no concept of ``orthogonal''.) But even if we don't have an inner product, finding a complement is not too difficult, as the next example shows.%
\begin{example}{Finding a complement by extending a basis.}{x:example:eg-direct-sum-basis}%
The easiest way to determine a direct sum decomposition (or equivalently, a complement) is through the use of a basis. Suppose \(U\) is a subspace of \(V\) with basis \(\basis{e}{k}\), and extend this to a basis%
\begin{equation*}
B = \{\mathbf{e}_1,\ldots, \mathbf{e}_k,\mathbf{e}_{k+1},\ldots, \mathbf{e}_n\}
\end{equation*}
of \(V\). Let \(W = \spn\{\mathbf{e}_{k+1},\ldots, \mathbf{e}_n\}\). Then clearly \(U+W=V\), and \(U\cap W=\{\zer\}\), since if \(\vv\in U\cap W\), then \(\vv\in U\) and \(\vv\in W\), so we have%
\begin{equation*}
\vv = a_1\mathbf{e}_1+\cdots + a_k\mathbf{e_k} = b_1\mathbf{e}_{k+1}+\cdots+b_{n-k}e_{n}\text{,}
\end{equation*}
which gives%
\begin{equation*}
a_1\mathbf{e}_1+\cdots + a_k\mathbf{e}_k-b_1\mathbf{e}_{k+1}-\cdots - b_{n-k}\mathbf{e}_n=\zer\text{,}
\end{equation*}
so \(a_1=\cdots b_{n-k}=0\) by the linear independence of \(B\), showing that \(\vv=\zer\).%
\par
Conversely, if \(V=U\oplus W\), and we have bases \(\basis{u}{k}\) of \(U\) and \(\basis{v}{l}\) of \(W\), then%
\begin{equation*}
B = \{\uu_1,\ldots, \uu_k,\ww_1,\ldots, \ww_l\}
\end{equation*}
is a basis for \(V\). Indeed, \(B\) spans \(V\), since every element of \(V\) can be written as \(\vv=\uu+\ww\) with \(\uu\in U,\ww\in W\). Independence follows by reversing the argument above: if%
\begin{equation*}
a_1\uu_1+\cdots + a_k\uu_k+b_1\ww_1+\cdots b_l\ww_l=\zer
\end{equation*}
then \(a_1\uu_1+\cdots + a_k\uu_k = -b_1\ww_1-\cdots -b_l\ww_l\), and equality is only possible if both sides belong to \(U\cap W = \{\zer\}\). Since \(\basis{u}{k}\) is independent, the \(a_i\) have to be zero, and since \(\basis{w}{l}\) is independent, the \(b_j\) have to be zero.%
\end{example}
The argument given in the second part of \hyperref[x:example:eg-direct-sum-basis]{Example~{\xreffont\ref{x:example:eg-direct-sum-basis}}} has an immediate, but important consequence.%
\begin{theorem}{}{}{x:theorem:thm-direct-sum-dimension}%
Suppose \(V=U\oplus W\), where \(\dim U = m\) and \(\dim W = n\). Then \(V\) is finite-dimensional, and \(\dim V = m+n\).%
\end{theorem}
\begin{example}{}{x:example:eg-invariant-block}%
Suppose \(V=U\oplus W\), where \(U\) and \(W\) are \(T\)-invariant subspaces for some operator \(T:V\to V\). Let \(B_U=\basis{u}{m}\) and let \(B_W = \basis{w}{n}\) be bases for \(U\) and \(W\), respectively. Determine the matrix of \(T\) with respect to the basis \(B=B_U\cup B_W\) of \(V\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp135}{}\quad{}Since we don't know the map \(T\) or anything about the bases \(B_U,B_W\), we're looking for a fairly general statement here. Since \(U\) is \(T\)-invariant, we must have \(T(\uu_i)\in U\) for each \(i=1,\ldots, m\). Similarly, \(T(\ww_j)\in W\) for each \(j=1,\ldots, n\). This means that we have%
\begin{align*}
T(\uu_1) \amp = a_{11}\uu_1 + \cdots + a_{m1}\uu_m + 0\ww_1+\cdots + 0\ww_n\\
\amp \vdots \\
T(\uu_m) \amp = a_{1m}\uu_1 + \cdots + a_{mm}\uu_m+0\ww_1+\cdots + 0\ww_n\\
T(\ww_1) \amp = 0\uu_1 + \cdots + 0\uu_m+b_{11}\ww_1 + \cdots + b_{n1}\ww_n \\
\amp \vdots \\
T(\ww_n) \amp = 0\uu_1 + \cdots + 0\uu_m+b_{1n}\ww_1 + \cdots + b_{nn}\ww_n
\end{align*}
for some scalars \(a_{ij},b_{ij}\). If we set \(A = [a_{ij}]_{m\times m}\) and \(B = [b_{ij}]_{n\times n}\), then we have%
\begin{equation*}
M_B(T) = \bbm A \amp 0\\0\amp B\ebm\text{.}
\end{equation*}
Moreover, we can also see that \(A = M_{B_U}(T|_U)\), and \(B = M_{B_W}(T|_W)\).%
\end{example}
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Worksheet 5.4 Worksheet: generalized eigenvectors}
\typeout{************************************************}
%
\newgeometry{left=1.25cm, right=1.25cm, top=1.25cm, bottom=1.25cm}
\begin{worksheet-section}{Worksheet: generalized eigenvectors}{}{Worksheet: generalized eigenvectors}{}{}{x:worksheet:worksheet-gen-eigen}
Let \(V\) be a finite-dimensional vector space, and let \(T:V\to V\) be a linear operator. Assume that \(T\) has all real eigenvalues (alternatively, assume we're working over the complex numbers). Let \(A\) be the matrix of \(T\) with respect to some standard basis \(B_0\) of \(V\).%
\par
Our goal will be to replace the basis \(B_0\) with a basis \(B\) such that the matrix of \(T\) with respect to \(B\) is as simple as possible. (Where we agree that the "simplest" possible matrix would be diagonal.)%
\par
Recall the following results that we've observed so far:%
\begin{itemize}[label=\textbullet]
\item{}The characteristic polynomial \(c_T(x)\) of \(T\) does not depend on the choice of basis.%
\item{}The eigenvalues of \(T\) are the roots of this polynomial.%
\item{}The eigenspaces \(E_\lambda(T)\) are \(T\)-invariant subspaces of \(V\).%
\item{}The matrix \(A\) can be diagonalized if and only if there is a basis of \(V\) consisting of eigenvectors of \(T\).%
\item{}Suppose%
\begin{equation*}
c_T(x) = (x-\lambda_1)^{m_1}(x-\lambda_2)^{m_2}\cdots (x-\lambda_k)^{m_k}\text{.}
\end{equation*}
Then \(A\) can be diagonalized if and only if \(\dim E_{\lambda_i}(T) = m_i\) for each \(i=1,\ldots, k\).%
\end{itemize}
%
\par
In the case where \(A\) can be diagonalized, we have the direct sum decomposition%
\begin{equation*}
V = E_{\lambda_1}(T)\oplus E_{\lambda_2}(T) \oplus \cdots \oplus E_{\lambda_k}(T)\text{.}
\end{equation*}
%
\par
The question is: what do we do if there aren't enough eigenvectors to form a basis of \(V\)? When that happens, the direct sum of all the eigenspaces will not give us all of \(V\).%
\par
The idea: replace \(E_{\lambda_j}(T)\) with a \terminology{generalized eigenspace} \(G_{\lambda_j}(T)\) whose dimension is \(m_i\).%
\par
Our candidate: instead of \(E_{\lambda}(T) = \ker(T-\lambda I)\), we use \(G_\lambda(T) = \ker((T-\lambda I)^m)\), where \(m\) is the multiplicity of \(\lambda\).%
\clearpage
\begin{divisionexercise}{1}{}{3in}{x:exercise:ws-ge-ex1}%
Recall that in class we proved that \(\ker(T)\) and \(\operatorname{im}(T)\) are \(T\)-invariant subspaces. Let \(p(x)\) be any polynomial, and prove that \(\ker (p(T))\) and \(\operatorname{im}(p(T))\) are also \(T\)-invariant.%
\par
\emph{Hint:} first show that \(p(T)T=Tp(T)\) for any polynomial \(T\).%
\end{divisionexercise}%
Applying the result of Problem 1 to the polynomial \(p(x) = (x-\lambda)^m\) shows that \(G_\lambda(T)\) is \(T\)-invariant. It is possible to show that \(\dim G_\lambda(T)=m\) but I won't ask you to do that. (A proof is in Nicholson if you really want to see it.)%
\par
Instead, we will try to understand what's going on by exploring an example.%
\par
Consider the following matrix.%
\begin{sageinput}
from sympy import *
init_printing()
A=Matrix([[2,0,0,1,0],[-1,0,1,2,3],[0,1,2,0,-1],[-2,-3,2,5,3],[0,-1,0,1,4]])
A
\end{sageinput}
\begin{divisionexercise}{2}{}{2in}{x:exercise:ws-ge-ex2}%
Find (and factor) the characteristic polynomial of \(A\). For the commands you might need, \href{https://opentext.uleth.ca/Math3410/sec-sympy.html}{refer to the textbook}\footnotemark{}.%
\end{divisionexercise}%
\footnotetext[1]{\nolinkurl{opentext.uleth.ca/Math3410/sec-sympy.html}\label{g:fn:idp136}}%
\clearpage
\begin{divisionexercise}{3}{}{3.25in}{x:exercise:ws-ge-ex3}%
Find the eigenvectors. What are the dimensions of the eigenspaces? Based on this observation, can \(A\) be diagonalized?%
\end{divisionexercise}%
\begin{divisionexercise}{4}{}{3.25in}{x:exercise:ws-ge-ex4}%
Prove that for any \(n\times n\) matrix \(A\), we have%
\begin{equation*}
\{0\}\subseteq \operatorname{null}(A)\subseteq \operatorname{null}(A^2) \subseteq \cdots \subseteq \operatorname{null}(A^n)\text{.}
\end{equation*}
%
\end{divisionexercise}%
\clearpage
It turns out that at some point, the null spaces stabilize. If \(\operatorname{null}(A^k)=\operatorname{null}A^{k+1}\) for some \(k\), then \(\operatorname{null}(A^k)=\operatorname{null}(A^{k+l})\) for all \(l\geq 0\).%
\begin{divisionexercise}{5}{}{2.5in}{x:exercise:ws-ge-ex5}%
For each eigenvalue found in \hyperlink{x:exercise:ws-ge-ex2}{Worksheet Exercise~{\xreffont 5.4.2}}, compute the nullspace of \(A-\lambda I\), \((A-\lambda I)^2\), \((A-\lambda I)^3\), etc.\@ until you find two consecutive nullspaces that are the same.%
\par
By \hyperlink{x:exercise:ws-ge-ex4}{Worksheet Exercise~{\xreffont 5.4.4}}, any vector in \(\operatorname{null}(A-\lambda I)^m\) will also be a vector in \(\operatorname{null}(A-\lambda I)^{m+1}\). In particular, at each step, we can find a basis for \(\operatorname{null}(A-\lambda I)^m\) that includes the basis for \(\operatorname{null}(A-\lambda I)^{m-1}\).%
\par
For each eigenvalue found in \hyperlink{x:exercise:ws-ge-ex2}{Worksheet Exercise~{\xreffont 5.4.2}}, determine such a basis for the corresponding generalized eigenspace. You will want to list your vectors so that the vectors from the basis of the nullspace for \(A-\lambda I\) come first, then the vectors for the basis of the nullspace for \((A-\lambda I)^2\), and so on.%
\end{divisionexercise}%
\begin{divisionexercise}{6}{}{2.5in}{x:exercise:ws-ge-ex6}%
Finally, let's see how all of this works. Let \(P\) be the matrix whose columns consist of the vectors found in Problem 4. What do you get when you compute the matrix \(P^{-1}AP\)?%
\end{divisionexercise}%
\end{worksheet-section}
\restoregeometry
%
%
\typeout{************************************************}
\typeout{Section 5.5 Generalized eigenspaces}
\typeout{************************************************}
%
\begin{sectionptx}{Generalized eigenspaces}{}{Generalized eigenspaces}{}{}{x:section:sec-gen-eigen}
Example \hyperref[x:example:eg-invariant-block]{Example~{\xreffont\ref{x:example:eg-invariant-block}}} showed us that if \(V=U\oplus W\), where \(U\) and \(W\) are \(T\)-invariant, then the matrix \(M_B(T)\) has block diagonal form \(\bbm A \amp 0\\0\amp B\ebm\), as long as the basis \(B\) is the union of bases of \(U\) and \(W\).%
\par
We want to take this idea further. If \(V = U_1\oplus U_2\oplus \cdots \oplus U_k\), where each subspace \(U_j\) is \(T\)-invariant, then with respect to a basis \(B\) consisting of basis vectors for each subspace, we will have%
\begin{equation*}
M_B(T)=\bbm A_1 \amp 0 \amp \cdots \amp 0\\
0 \amp A_2 \amp \cdots \amp 0\\
\vdots \amp \vdots \amp \ddots \amp \vdots\\
0 \amp 0 \amp \cdots \amp A_k\ebm\text{,}
\end{equation*}
where each \(A_j\) is the matrix of \(T|_{U_j}\) with respect to some basis of \(U_j\).%
\par
Our goal moving forward is twofold: one, to make the blocks as small as possible, so that \(M_B(T)\) is as close to diagonal as possible, and two, to make the blocks as simple as possible. Of course, if \(T\) is diagonalizable, then we can get all blocks down to size \(1\times 1\), but this is not always possible.%
\par
Recall from \hyperref[x:section:subsec-eigen-basics]{Section~{\xreffont\ref{x:section:subsec-eigen-basics}}} that if the characteristic polynomial of \(T\) (or equivalently, any matrix representation \(A\) of \(T\)) is%
\begin{equation*}
c_T(x) = (x-\lambda_1)^{m_1}(x-\lambda_2)^{m_2}\cdots (x-\lambda_k)^{m_k}\text{,}
\end{equation*}
then \(\dim E_{\lambda_j}(T)\leq m_j\) for each \(j=1,\ldots, k\), and \(T\) is diagonalizable if and only if we have equality for each \(j\). (This guarantees that we have sufficiently many independent eigenvectors to form a basis of \(V\).)%
\par
Since eigenspaces are \(T\)-invariant, we see that being able to diagonalize \(T\) is equivalent to having the direct sum decomposition%
\begin{equation*}
V = E_{\lambda_1}(T)\oplus E_{\lambda_2}(T)\oplus \cdots \oplus E_{\lambda_k}(T)\text{.}
\end{equation*}
If \(T\) cannot be diagonalized, it's because we came up short on the number of eigenvectors, and the direct sum of all eigenspaces only produces some subspace of \(V\) of lower dimension. We now consider how one might enlarge a set of independent eigenvectors in some standard, and ideally optimal, way.%
\par
First, we note that for any operator \(T\), the restriction of \(T\) to \(\ker T\) is the zero operator, since by definition, \(T(\vv)=\zer\) for all \(\vv\in\ker T\). Since we define \(E_{\lambda}(T)=\ker (T-\lambda I)\), it follows that \(T-\lambda I\) restricts to the zero operator on the eigenspace \(E_\lambda(T)\). The idea is to relax the condition ``identically zero'' to something that will allow us to potentially enlarge some of our eigenspaces, so that we end up with enough vectors to span \(V\).%
\par
It turns out that the correct replacement for ``identically zero'' is ``nilpotent''. What we would like to find is some subspace \(G_\lambda(T)\) such that the restriction of \(T-\lambda I\) to \(G_\lambda(T)\) will be nilpotent. (Recall that this means \((T-\lambda I)^k = 0\) for some integer \(k\) when restricted to \(G_\lambda(T)\).) The only problem is that we don't (yet) know what this subspace should be. To figure it out, we rely on some ideas you may have explored in your last assignment.%
\begin{theorem}{}{}{x:theorem:thm-nullspace-power}%
Let \(T:V\to V\) be a linear operator. Then:%
\begin{enumerate}
\item{}\(\displaystyle \{\zer\}\subseteq \ker T \subseteq \ker T^2 \subseteq \cdots \subseteq \ker T^k\subseteq \cdots\)%
\item{}If \(\ker T^{k+1}=\ker T^k\) for some \(k\), then \(\ker T^{k+m}=\ker T^k\) for all \(m\geq 0\).%
\item{}If \(n=\dim V\), then \(\ker T^{n+1} = \ker T^n\).%
\end{enumerate}
%
\end{theorem}
In other words, for any operator \(T\), the kernels of successive powers of \(T\) can get bigger, but the moment the kernel doesn't change for the next highest power, it stops changing for all further powers of \(T\). That is, we have a sequence of kernels of strictly greater dimension until we reach a maximum, at which point the kernels stop growing. And of course, the maximum dimension cannot be more than the dimension of \(V\).%
\begin{definition}{}{x:definition:def-generalized-eigenspace}%
Let \(T:V\to V\) be a linear operator, and let \(\lambda\) be an eigenvalue of \(T\). The \terminology{generalized eigenspace} of \(T\) associated to the eigenvalue \(\lambda\) is denoted \(G_\lambda(T)\), and defined as%
\begin{equation*}
G_\lambda(T) = \ker (T-\lambda I)^n\text{,}
\end{equation*}
where \(n=\dim V\).%
\end{definition}
Some remarks are in order. First, we can actually define \(G_\lambda(T)\) for any scalar \(\lambda\). But this space will be trivial if \(\lambda\) is not an eigenvalue. Second, it is possible to show (although we will not do so here) that if \(\lambda\) is an eigenvalue with multiplicity \(m\), then \(G_\lambda(T)=\ker (T-\lambda I)^m\). (The kernel will usually have stopped growing well before we hit \(n=\dim V\), but we know they're all eventually equal, so using \(n\) guarantees we have everything).%
\par
We will not prove it here (see Nicholson, or Axler), but the advantage of using generalized eigenspaces is that they're just big enough to cover all of \(V\).%
\begin{theorem}{}{}{x:theorem:thm-gen-eigen-decomp}%
Let \(V\) be a complex vector space, and let \(T:V\to V\) be a linear operator. (We can take \(V\) to be real if we assume that \(T\) has all real eigenvalues.) Let \(\lambda_1,\ldots, \lambda_k\) be the distinct eigenvalues of \(T\). Then each generalized eigenspace \(G_{\lambda_j}(T)\) is \(T\)-invariant, and we have the direct sum decomposition%
\begin{equation*}
V = G_{\lambda_1}(T)\oplus G_{\lambda_2}(T)\oplus \cdots \oplus G_{\lambda_k}(T)\text{.}
\end{equation*}
%
\end{theorem}
For each eigenvalue \(\lambda_j\) of \(T\), let \(l_j\) denote the \emph{smallest} integer power such that \(G_{\lambda_j}(T) = (T-\lambda_j I)^{l_j}\). Then certainly we have \(l_j\leq m_j\) for each \(j\). (Note also that if \(l_j=1\), then \(G_{\lambda_j}(T)=E_{\lambda_j}(T)\).)%
\par
The polynomial \(m_T(x) = (x-\lambda_1)^{l_1}(x-\lambda_2)^{l_2}\cdots (x-\lambda_k)^{l_k}\) is the polynomial of \emph{smallest degree} such that \(m_T(T)=0\). The polynomial \(m_T(x)\) is called the \terminology{minimal polynomial} of \(T\). Note that \(T\) is diagonalizable if and only if the minimal polynomial of \(T\) has no repeated roots.%
\par
In \hyperref[x:section:sec-jordan-form]{Section~{\xreffont\ref{x:section:sec-jordan-form}}}, we'll explore a systematic method for determining the generalized eigenspaces of a matrix, and in particular, for computing a basis for each generalized eigenspace, with respect to which the corresponding block in the block-diagonal form is especially simple.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 5.6 Jordan Canonical Form}
\typeout{************************************************}
%
\begin{sectionptx}{Jordan Canonical Form}{}{Jordan Canonical Form}{}{}{x:section:sec-jordan-form}
The results of \hyperref[x:theorem:thm-nullspace-power]{Theorem~{\xreffont\ref{x:theorem:thm-nullspace-power}}} and \hyperref[x:theorem:thm-gen-eigen-decomp]{Theorem~{\xreffont\ref{x:theorem:thm-gen-eigen-decomp}}} tell us that for an eigenvalue \(\lambda\) of \(T:V\to V\) with multiplicity \(m\), we have a sequence of subspace inclusions%
\begin{equation*}
E_\lambda(T) = \ker (T-\lambda I)\subseteq \ker (T-\lambda I)^2 \subseteq \cdots \subseteq \ker (T-\lambda I)^m = G_\lambda(T)\text{.}
\end{equation*}
Not all subspaces in this sequence are necessarily distinct. Indeed, it is entirely possible that \(\dim E_\lambda(T)=m\), in which case \(E_\lambda(T)=G_\lambda(T)\). In geeral there will be some \(l\leq m\) such that \(\ker (T-\lambda I)^l=G_\lambda(T)\).%
\par
Our goal in this section is to determine a basis for \(G_\lambda(T)\) in a standard way. We begin with a couple of important results, which we state without proof. The first can be found in Axler's book; the second in Nicholson's.%
\begin{theorem}{}{}{x:theorem:thm-gen-eigen-props}%
Suppose \(V\) is a complex vector space, and \(T:V\to V\) is a linear operator. Let \(\lambda_1,\ldots, \lambda_k\) denote the distinct eigenvalues of \(T\). (We can assume \(V\) is real if we also assume that all eigenvalues of \(V\) are real.) Then:%
\begin{enumerate}
\item{}Generalized eigenvectors corresponding to \emph{distinct} eigenvalues are linearly independent.%
\item{}\(\displaystyle V = G_{\lambda_1}(T)\oplus G_{\lambda_2}(T)\oplus \cdots \oplus G_{\lambda_k}(T)\)%
\item{}Each generalize eigenspace \(G_{\lambda_j}(T)\) is \(T\)-invariant.%
\item{}Each restriction \((T-\lambda_j)|_{G_{\lambda_j}(T)}\) is nilpotent.%
\end{enumerate}
%
\end{theorem}
\begin{theorem}{}{}{x:theorem:thm-block-eigen}%
Let \(T:V\to V\) be a linear operator. If the characteristic polynomial of \(T\) is given by%
\begin{equation*}
c_T(x) = (x-\lambda_1)^{m_1}(x-\lambda_2)^{m_2}\cdots (x-\lambda_k)^{m_k}\text{,}
\end{equation*}
then \(\dim G_{\lambda_j}(T)=m_j\) for each \(j=1,\ldots, k\).%
\par
Moreover, if we let \(B=B_1\cup B_2\cup\cdots \cup B_k\), where \(B_j\) is any basis for \(G_{\lambda_j}(T)\) for \(j=1,\ldots, k\), then \(B\) is a basis for \(V\) (this follows immediately from \hyperref[x:theorem:thm-gen-eigen-props]{Theorem~{\xreffont\ref{x:theorem:thm-gen-eigen-props}}}) and the matrix of \(T\) with respect to this basis has the block-diagonal form%
\begin{equation*}
M_B(T) = \bbm A_1 \amp 0 \amp \cdots \amp 0\\
0 \amp A_2 \amp \cdots \amp 0\\
\vdots\amp\vdots\amp\ddots\amp\vdots\\
0 \amp 0 \amp \cdots \amp A_k\ebm\text{,}
\end{equation*}
where each \(A_j\) has size \(m_j\times m_j\).%
\end{theorem}
A few remarks are called for here.%
\begin{itemize}[label=\textbullet]
\item{}One of the ways to see that \(\dim G_{\lambda_j}(T)=m_j\) is to consider \((M_B(T)-\lambda_j I_n)^{m_j}\). This will have the form \(\diag(U_1^{m_j}, U_2^{m_j},\ldots, U_k^{m_j})\), where \(U_i\) is the matrix of \((T-\lambda_j)^{m_j}\), restricted to \(G_{\lambda_i}(T)\). If \(i\neq j\), \(T-\lambda_j I\) restricts to an invertible operator on \(G_{\lambda_i}(T)\), but its restriction to \(G_{\lambda_j}(T)\) is nilpotent, by \hyperref[x:theorem:thm-gen-eigen-props]{Theorem~{\xreffont\ref{x:theorem:thm-gen-eigen-props}}}. So \(U_j\) is nilpotent (with \(U_j^{m_j}=0\)), and has size \(m_j\times m_j\), while \(U_i\) is invertible if \(i\neq j\). The matrix \((M_B(T)-\lambda_j I)^{m_j}\) thus ends up with a \(m_j\times m_j\) block of zeros, so \(\dim \ker (T-\lambda_j I)^{m_j}=m_j\).%
\item{}If the previous point wasn't clear, note that with an appropriate choice of basis, the block \(A_i\) in \hyperref[x:theorem:thm-block-eigen]{Theorem~{\xreffont\ref{x:theorem:thm-block-eigen}}} has the form%
\begin{equation*}
A_i = \bbm \lambda_i \amp \ast \amp \cdots \amp \ast\\
0 \amp \lambda_i \amp \cdots \amp \ast\\
\vdots \amp \vdots \amp \ddots\amp \vdots\\
0 \amp 0 \amp \cdots \amp \lambda_i\ebm\text{.}
\end{equation*}
Thus, \(M_B(T)-\lambda_j I\) will have blocks that are upper triangular, with diagonal entries \(\lambda_i-\lambda_j\neq 0\) when \(i\neq j\), but when \(i=j\) we get a matrix that is strictly upper triangular, and therefore nilpotent, since its diagonal entries will be \(\lambda_j-\lambda_j=0\).%
\item{}if \(l_j\) is the \emph{least} integer such that \(\ker (A-\lambda_j I)^{l_j}=G_{\lambda_j}(T)\), then it is possible to choose the basis of \(G_{\lambda_j}(T)\) so that \(A_j\) is itself block-diagonal, with the largest block having size \(l_j\times l_j\). The remainder of this section is devoted to determining how to choose such a basis.%
\end{itemize}
%
\par
The basic principle for choosing a basis for each generalized eigenspace is as follows. We know that \(E_{\lambda}(T)\subseteq G_\lambda(T)\) for each eigenvalue \(\lambda\). So we start with a basis for \(E_\lambda(T)\), by finding eigenvectors as usual. If \(\ker (T-\lambda I)^2 = \ker (T-\lambda I)\), then we're done: \(E_\lambda(T)=G_\lambda(T)\). Otherwise, we enlarge the basis for \(E_\lambda(T)\) to a basis of \(\ker T(-\lambda I)^2\). If \(\ker T(-\lambda I)^3=\ker (T-\lambda I)^2\), then we're done, and \(G_\lambda(T) = \ker (T-\lambda I)^2\). If not, we enlarge our existing basis to a basis of \(\ker (T-\lambda I)^3\). We continue this process until we reach some power \(l\) such that \(\ker (T-\lambda I)^l = \ker (T-\lambda I)^{l+1}\). (This is guaranteed to happen by \hyperref[x:theorem:thm-nullspace-power]{Theorem~{\xreffont\ref{x:theorem:thm-nullspace-power}}}.) We then conclude that \(G_\lambda(T) = \ker (T-\lambda I)^l\).%
\par
The above produces \emph{a} basis for \(G_\lambda(T)\), but we want what is, in some sense, the ``best'' basis. For our purposes, the best basis is the one in which the matrix of \(T\) restricted to each generalized eigenspace is block diagonal, where each block is a \emph{Jordan block}.%
\begin{definition}{}{x:definition:def-jordan-block}%
Let \(\lambda\) be a scalar. A \terminology{Jordan block} is an \(m\times m\) matrix of the form%
\begin{equation*}
J(m,\lambda) = \bbm \lambda \amp 1 \amp 0 \amp \cdots \amp 0\\
0 \amp \lambda \amp 1 \amp \cdots \amp 0\\
\vdots \amp \vdots \amp \ddots \amp \ddots \amp \vdots\\
0 \amp 0 \amp \cdots \amp \lambda \amp 1\\
0 \amp 0 \amp 0 \amp \cdots \amp \lambda\ebm\text{.}
\end{equation*}
That is \(J(m,\lambda)\) has each diagonal entry equal to \(\lambda\), and each ``superdiagonal'' entry (those just above the diagonal) equal to 1, with all other entries equal to zero.%
\end{definition}
\begin{example}{}{g:example:idp137}%
The following are examples of Jordan blocks:%
\begin{equation*}
J(2,4)=\bbm 4 \amp 1\\ 0\amp 4\ebm, J(3,\sqrt{2})=\bbm \sqrt{2} \amp 1\amp 0\\0\amp \sqrt{2}\amp 1\\0\amp 0\amp \sqrt{2}\ebm,
J(4,2i)=\bbm 2i \amp 1\amp 0\amp 0\\0\amp 2i\amp 1\amp 0\\0\amp 0\amp 2i\amp 1\\0\amp 0\amp 0\amp 2i\ebm\text{.}
\end{equation*}
%
\end{example}
\begin{insight}{Finding a chain basis.}{x:insight:jordan-chain-basis}%
A Jordan block corresponds to basis vectors \(\vv_1,\vv_2,\ldots, \vv_m\) with the following properties:%
\begin{align*}
T(\vv_1) \amp = \lambda \vv_1\\
T(\vv_2) \amp = \vv_1+\lambda \vv_2\\
T(\vv_3) \amp = \vv_2+\lambda \vv_3\text{,}
\end{align*}
and so on. Notice that \(\vv_1\) is an eigenvector, and for each \(j=2,\ldots, m\),%
\begin{equation*}
(T-\lambda I)\vv_{j} = \vv_{j-1}\text{.}
\end{equation*}
%
\par
Notice also that if we set \(N=T-\lambda I\), then%
\begin{equation*}
\vv_1 = N\vv_2, \vv_2 = N\vv_3, \ldots, \vv_{m-1} = N\vv_m
\end{equation*}
so our basis for \(G_\lambda(T)\) is of the form%
\begin{equation*}
\vv, N\vv, N^2\vv, \ldots, N^{m-1}\vv\text{,}
\end{equation*}
where \(\vv = \vv_m\), and \(\vv_1=N^{m-1}\vv\) is an eigenvector. (Note that \(N^m\vv = (T-\lambda I)\vv_1=\zer\), and indeed \(N^m\vv_j=\zer\) for each \(j=1,\ldots, m\).) Such a basis is known as a \terminology{chain basis}.%
\par
If \(\dim E_\lambda(T)\gt 1\) we might have to repeat this process for each eigenvector in a basis for the eigenspace. The full matrix of \(T\) might have several Jordan blocks of possibly different sizes for each eigenvalue.%
\end{insight}
\begin{example}{}{x:example:ex-jordan-form1}%
Determine a Jordan basis for the operator \(T:\R^5\to \R^5\) whose matrix with respect to the standard basis is given by%
\begin{equation*}
A = \bbm 7\amp 1 \amp -3\amp 2\amp 1\\
-6\amp 2\amp 4\amp -2\amp -2\\
0 \amp 1\amp 3 \amp 1\amp -1\\
-8\amp -1\amp 6\amp 0 \amp-3\\
-4\amp 0\amp 3\amp -1\amp 1\ebm
\end{equation*}
%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp138}{}\quad{}First, we need the characteristic polynomial.%
\begin{sageinput}
from sympy import Matrix,init_printing,factor
init_printing()
A = Matrix([[7,1,-3,2,1],
            [-6,2,4,-2,-2],
            [0,1,3,1,-1],
            [-8,-1,6,0,-3],
            [-4,0,3,-1,1]])
p = A.charpoly().as_expr()
factor(p)
\end{sageinput}
\begin{sageoutput}
\[(\lambda-3)^3(\lambda-2)^2\]
\end{sageoutput}
The characteristic polynomial of \(A\) is given by%
\begin{equation*}
c_A(x)=(x-2)^2(x-3)^3\text{.}
\end{equation*}
We thus have two eigenvalues: \(2\), of multiplicity \(2\), and \(3\), of multiplicity \(3\). We next find the \(E_2(A)\) eigenspace.%
\begin{sageinput}
from sympy import eye
N2 = A-2*eye(5)
E2 = N2.nullspace()
E2
\end{sageinput}
\begin{sageoutput}
\[\bbm -1\\0\\-1\\1\\0\ebm\]
\end{sageoutput}
The computer gives us%
\begin{equation*}
E_2(A)=\nll(A-2I) = \spn\{\xx_1\}, \text{ where } \xx_1=\bbm -1\\0\\-1\\1\\0\ebm\text{,}
\end{equation*}
so we have only one independent eigenvector, which means that \(G_2(A)=\nll(A-2I)^2\).%
\par
Following \hyperref[x:insight:jordan-chain-basis]{Insight~{\xreffont\ref{x:insight:jordan-chain-basis}}}, we extend \(\{\xx_1\}\) to a basis of \(G_2(A)\) by solving the system%
\begin{equation*}
(A-2I)\xx = \xx_1\text{.}
\end{equation*}
%
\begin{sageinput}
B2 = N2.col_insert(5,E2[0])
B2.rref()
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 1\amp 0\amp 0 \amp 1\amp 0\amp 0\\0\amp 1\amp 0\amp 0\amp 0\amp -1\\0\amp 0\amp 1\amp 1\amp 0\amp 0\\
0\amp 0\amp 0\amp 0\amp 1\amp 0\\0\amp 0\amp 0\amp 0\amp 0\amp 0\ebm,(0,1,2,4)\right)\]
\end{sageoutput}
Using the results above from the computer (or Gaussian elimination), we find a general solution%
\begin{equation*}
\xx = \bbm -t\\-1\\-t\\t\\0\ebm = t\bbm -1\\0\\-1\\1\\0\ebm + \bbm 0\\-1\\0\\0\\0\ebm\text{.}
\end{equation*}
Note that our solution is of the form \(\xx = t\xx_1+\xx_2\). We set \(t=0\), and get \(\xx_2 = \bbm 0\amp -1\amp 0\amp 0\amp 0\ebm^T\).%
\par
Next, we consider the eigenvalue \(\lambda=3\). The computer gives us the following:%
\begin{sageinput}
N3 = A-3*eye(5)
E3 = N3.nullspace()
E3
\end{sageinput}
\begin{sageoutput}
\[\left[\bbm \frac12\\-1\\1\\1\\0\ebm, \bbm -\frac12\\1\\0\\0\\1\ebm\right]\]
\end{sageoutput}
Rescaling to remove fractions, we find%
\begin{equation*}
E_3(A) = \nll(A-3I) = \spn\{\yy_1,\yy_2\}, \text{ where } \yy_1 = \bbm 1\\-2\\2\\2\\0\ebm, \yy_2 = \bbm -1\\2\\0\\0\\2\ebm\text{.}
\end{equation*}
Again, we're one eigenvector short of the multiplicity, so we need to consider \(G_3(A)=\nll(A-3I)^3\).%
\par
In the next cell, note that we doubled the eigenvectors in \mono{E3} to avoid fractions. To follow the solution in our example, we append \mono{2*E3[0]}, and reduce the resulting matrix. You should find that using the eigenvector \(\yy_1\) corresponding to \mono{E3[0]} leads to an inconsistent system. Once you confirm this, replace \mono{E3[0]} with \mono{E3[1]} and re-run the cell to see that we get an inconsistent system using \(\yy_2\) as well!%
\begin{sageinput}
B3 = N3.col_insert(5,2*E3[0])
B3.rref()
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 1\amp 0\amp 0\amp -\frac12\amp \frac12\amp 0\\0\amp 1\amp 0\amp 1\amp -1\amp 0\\0\amp 0\amp 1\amp -1\amp 0\amp 0\\
0\amp 0\amp 0\amp 0\amp 0\amp 1\\0\amp 0\amp 0\amp 0\amp 0\amp 0\ebm, (0,1,2,5)\right)\]
\end{sageoutput}
The systems \((A-3I)\yy = \yy_1\) and \((A-3I)\yy=\yy_2\) are both inconsistent, but we can salvage the situation by replacing the eigenvector \(\yy_2\) by some linear combination \(\zz_2 = a\yy_1+b\yy_2\). We row-reduce, and look for values of \(a\) and \(b\) that give a consistent system.%
\par
The \mono{rref} command takes things a bit farther than we'd like, so we use the command \mono{echelon\_form()} instead. Note that if \(a=b\), the system is inconsistent.%
\begin{sageinput}
from sympy import Symbol
a = Symbol('a')
b = Symbol('b')
C3 = N3.col_insert(5,a*E3[0]+b*E3[1])
C3.echelon_form()
\end{sageinput}
\begin{sageoutput}
\[\bbm 4\amp 1\amp -3\amp 2\amp 1\amp \frac{a}{2}-\frac{b}{2}\\
0\amp 2\amp -2\amp 4\amp -2\amp-a+b\\
0\amp 0\amp 2\amp -2\amp 0\amp 3a-b\\
0\amp 0\amp 0\amp 0\amp 0\amp 16a-16b\\
0\amp 0\amp 0\amp 0\amp 0\amp 0\ebm\]
\end{sageoutput}
We find that \(a=b\) does the job, so we set%
\begin{equation*}
\zz_2 = \yy_1+\yy_2 = \bbm 0\\0\\2\\2\\2\ebm\text{.}
\end{equation*}
%
\begin{sageinput}
D3 = N3.col_insert(5,E3[0]+E3[1])
D3.rref()
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 1\amp 0\amp 0\amp -\frac12\amp \frac12\amp \frac12\\
0\amp 1\amp 0\amp 1\amp -1 \amp 1\\0\amp 0\amp 1\amp -1\amp 0\amp 1\\
0\amp 0\amp 0\amp 0\amp 0\amp 0\\0\amp 0\amp 0\amp 0\amp 0\amp 0\ebm,(0,1,2)\right)\]
\end{sageoutput}
Solving the system \((A-3I)\zz = \yy_1+\yy_2\), using the code above, we find%
\begin{align*}
\zz \amp = \bbm \frac12 +\frac12 s-\frac12 t\\1-s+t\\1+s\\s\\t\ebm\\
\amp = \bbm \frac12\\1\\1\\0\\0\ebm + s\bbm\frac12\\-1\\1\\1\\0\ebm+t\bbm -\frac12\\1\\0\\0\\1\ebm\\
\amp = \bbm \frac12\\1\\1\\0\\0\ebm \frac{s}{2}\yy_1+\frac{t}{2}\yy_2\text{.}
\end{align*}
%
\par
We let \(\zz_3 = \bbm 1 \\ 2 \\ 2 \\ 0 \\ 0\ebm\), and check that%
\begin{equation*}
A\zz_3 = 3\zz_3+\zz_2\text{,}
\end{equation*}
as required:%
\begin{sageinput}
Z3 = Matrix(5,1,[1,2,2,0,0])
A*Z3-3*Z3-2*(E3[0]+E3[1])
\end{sageinput}
\begin{sageoutput}
\[\bbm 0\\0\\0\\0\\0\ebm\]
\end{sageoutput}
This gives us the basis \(B = \{\xx_1,\xx_2,\yy_1,\zz_2,\zz_3\}\) for \(\R^5\), and with respect to this basis, we have the Jordan canonical form%
\begin{equation*}
M_B(T) = \bbm 2 \amp 1\amp 0\amp 0 \amp 0\\
0 \amp 2\amp 0\amp 0 \amp 0\\
0 \amp 0\amp 3\amp 0 \amp 0\\
0 \amp 0\amp 0\amp 3 \amp 1\\
0 \amp 0\amp 0\amp 0 \amp 3\ebm\text{.}
\end{equation*}
%
\par
Now that we've done all the work required for \hyperref[x:example:ex-jordan-form1]{Example~{\xreffont\ref{x:example:ex-jordan-form1}}}, we should confess that there was an easier way all along:%
\begin{sageinput}
A.jordan_form()
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 1\amp 0\amp 0\amp \frac12\amp \frac12\\ 0\amp 1\amp 0\amp 1\amp -1\\
1\amp 0\amp 1\amp 1\amp 1\\-1\amp 0\amp 1\amp 0\amp 1\\0\amp 0\amp 1\amp 0\amp 0\ebm,
\bbm 2\amp 1\amp 0\amp 0\amp 0\\0\amp 2\amp 0\amp 0\amp 0\\
0\amp 0\amp 3\amp 1\amp 0\\0\amp 0\amp 0\amp 3\amp 0\\0\amp 0\amp 0\amp 0\amp 3\ebm\right)\]
\end{sageoutput}
The \mono{jordan\_form()} command returns a pair \(P,J\), where \(J\) is the Jordan canonical form of \(A\), and \(P\) is an invertible matrix such that \(P^{-1}AP=J\). You might find that the computer's answer is not quite the same as ours. This is because the Jordan canonical form is only unique up to permutation of the Jordan blocks. Changing the order of the blocks amounts to changing the order of the columns of \(P\), which are given by a basis of (generalized eigenvectors).%
\end{example}
\begin{inlineexercise}{}{x:exercise:ex-jordan-form2}%
Determine a Jordan basis for the linear operator \(T:\R^4\to\R^4\) given by%
\begin{equation*}
T(w,x,y,z)=(w+x,x,-x+2y,w-x+y+z)\text{.}
\end{equation*}
A code cell is given below in case you want to try performing the operations demonstrated in \hyperref[x:example:ex-jordan-form1]{Example~{\xreffont\ref{x:example:ex-jordan-form1}}}.%
\end{inlineexercise}%
One final note: we mentioned above that the minimal polynomial of an operator has the form%
\begin{equation*}
m_T(x)=(x-\lambda_1)^{l_1}(x-\lambda_2)^{l_2}\cdots (x-\lambda_k)^{l_k}\text{,}
\end{equation*}
where for each \(j=1,2,\ldots, k\), \(l_j\) is the size of the largest Jordan block corresponding to \(\lambda_j\). Knowing the minimal polynomial therefore tells as a lot about the Jordan canonical form, but not everything. Of course, if \(l_j=1\) for all \(j\), then our operator can be diagaonalized. If \(\dim V\leq 4\), the minimal polynomial tells us everything, except for the order of the Jordan blocks.%
\par
In \hyperref[x:exercise:ex-jordan-form2]{Exercise~{\xreffont\ref{x:exercise:ex-jordan-form2}}}, the minimal polynomial is \(m_T(x)=(x-1)^3(x-2)\), the same as the characteristic polynomial. If we knew this in advance, then the only possible Jordan canonical forms would be%
\begin{equation*}
\bbm 1\amp 1\amp 0\amp 0\\
0\amp 1\amp 1\amp 0\\
0\amp 0\amp 1\amp 0\\
0\amp 0\amp 0\amp 2\ebm \text{ or } \bbm 2\amp 0\amp 0\amp 0\\
0\amp 1\amp 1\amp 0\\
0\amp 0\amp 1\amp 1\\
0\amp 0\amp 0\amp 1\ebm\text{.}
\end{equation*}
If instead the minimal polynomial had turned out to be \((x-1)^2(x-2)\) (with the same characteristic polynomial), then, up to permutation of the Jordan blocks, our Jordan canonical form would be%
\begin{equation*}
\bbm 1\amp 0\amp 0\amp 0\\0\amp 1\amp 1\amp 0\\0\amp 0\amp 1\amp 0\\0\amp 0\amp 0\amp 2\ebm\text{.}
\end{equation*}
%
\par
However, once we hit matrices of size \(5\times 5\) or larger, some ambiguity creeps in. For example, suppose \(c_A(x) = (x-2)^5\) with \(m_A(x)=(x-2)^2\). Then the largest Jordan block is \(2\times 2\), but we could have two \(2\times 2\) blocks and a \(1\times 1\), or three \(1\times 1\) blocks and one \(2\times 2\).%
\end{sectionptx}
\end{chapterptx}
%
\appendix%
%
\clearpage\phantomsection%
\addcontentsline{toc}{part}{Appendices}%
%
%
\typeout{************************************************}
\typeout{Appendix A Computational Tools}
\typeout{************************************************}
%
\begin{appendixptx}{Computational Tools}{}{Computational Tools}{}{}{x:appendix:ch-computation}
%
%
\typeout{************************************************}
\typeout{Section A.1 Jupyter}
\typeout{************************************************}
%
\begin{sectionptx}{Jupyter}{}{Jupyter}{}{}{x:section:section-jupyter}
The first thing you need to know about doing linear algebra in Python is how to access a Python environment. Fortunately, you do not need to install any software for this. The University of Lethbridge has access to the \terminology{Syzygy Jupyter Hub} service, provided by \initialism{PIMS} (the Pacific Institute for Mathematical Sciences), Cybera, and Compute Canada. To access Syzygy, go to \href{https://uleth.syzygy.ca}{\nolinkurl{uleth.syzygy.ca}} and log in with your ULeth credentials. Below is a video explaining some of the features of our Jupyter hub.%
\setlength{\qrsize}{9em}
\setlength{\previewwidth}{\linewidth}
\addtolength{\previewwidth}{-\qrsize}
\begin{tcbraster}[raster columns=2, raster column skip=1pt, raster halign=center, raster force size=false, raster left skip=0pt, raster right skip=0pt]%
\begin{tcolorbox}[previewstyle, width=\previewwidth]%
\includegraphics[width=0.80\linewidth,height=\qrsize,keepaspectratio]{generated/youtube/video-1.jpg}%
\end{tcolorbox}%
\begin{tcolorbox}[qrstyle]%
{\hypersetup{urlcolor=black}\qrcode[height=\qrsize]{https://www.youtube.com/watch?v=VUfp7AQdxhk}}%
\end{tcolorbox}%
\begin{tcolorbox}[captionstyle]%
\small YouTube: \mono{https://www.youtube.com/watch?v=VUfp7AQdxhk}\end{tcolorbox}%
\end{tcbraster}%
Note: if you click the login button and nothing happens, click the back button and try again. Sometimes there's a problem with our single sign-on service.%
\par
The primary type of document you'll encounter on Syzygy is the \terminology{Jupyter notebook}. Content in a Juypter notebook is organized into \terminology{cells}. Some cells contain text, which can be in either \initialism{HTML} or \terminology{Markdown}. Markdown is a simple markup language. It's not as versatile as HTML, but it's easier to use. On Jupyter, markdown supports the LaTeX language for mathematical expressions. Use single dollar signs for inline math: \mono{\$\textbackslash{}frac\{d\}\{dx\}\textbackslash{}sin(x)=\textbackslash{}cos(x)\$} produces \(\frac{d}{dx}\sin(x)=\cos(x)\), for example.%
\par
If you want ``display math'', use double dollar signs. Unfortunately, entering matrices is a bit tedious. For example, \mono{\$\$A = \textbackslash{}begin\{bmatrix\}1 \& 2 \& 3\textbackslash{}\textbackslash{}4 \& 5 \& 6 \&\textbackslash{}end\{bmatrix\}\$\$} produces%
\begin{equation*}
A = \begin{bmatrix}1\amp 2\amp 3\\4\amp 5\amp 6\end{bmatrix}\text{.}
\end{equation*}
Later we'll see how to enter things like matrices in Python.%
\par
It's also possible to use markdown to add \emph{emphasis}, images, URLs, etc.\@. For details, see the following \href{https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet}{Markdown cheatsheet}\footnote{\nolinkurl{github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet}\label{g:fn:idp139}}, or this \href{https://callysto.ca/wp-content/uploads/2018/12/Callysto-Cheatsheet_12.19.18_web.pdf}{quick reference}\footnote{\nolinkurl{callysto.ca/wp-content/uploads/2018/12/Callysto-Cheatsheet_12.19.18_web.pdf}\label{g:fn:idp140}} from \href{https://callysto.ca/}{\nolinkurl{callysto.ca}}.%
\par
What's cool about a Jupyter notebook is that in addition to markdown cells, which can present content and provide explanation, we can also include \emph{code cells}. Jupyter supports many different programming languages, but we will stick mainly to Python.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section A.2 Python basics}
\typeout{************************************************}
%
\begin{sectionptx}{Python basics}{}{Python basics}{}{}{x:section:sec-python-basics}
OK, so you've logged into Syzygy and you're ready to write some code. What does basic code look like in Python? The good news is that you don't need to be a programmer to do linear algebra in Python. Python includes many different \emph{libraries} that keep most of the code under the hood, so all you have to remember is what command you need to use to accomplish a task. That said, it won't hurt to learn a little bit of basic coding.%
\par
Basic arithmetic operations are understood, and you can simply type them in. Hit \mono{shift+enter} in a code cell to execute the code and see the result.%
\begin{sageinput}
3+4
\end{sageinput}
\begin{sageinput}
3*4
\end{sageinput}
\begin{sageinput}
3**4
\end{sageinput}
OK, great. But sometimes we want to do calculations with more than one step. For that, we can assign variables.%
\begin{sageinput}
a = 14
b = -9
c = a+b
print(a, b, c)
\end{sageinput}
Sometimes you might need input that's a string, rather than a number. We can do that, too.%
\begin{sageinput}
string_var = "Hey, look at my string!"
print(string_var)
\end{sageinput}
Another basic construction is a list. Getting the hang of lists is useful, because in a sense, matrices are just really fancy lists.%
\begin{sageinput}
empty_list = list()
this_too = []
list_of_zeros = [0]*7
print(list_of_zeros)
\end{sageinput}
Once you have an empty list, you might want to add something to it. This can be done with the \mono{append} command.%
\begin{sageinput}
empty_list.append(3)
print(empty_list)
print(len(empty_list))
\end{sageinput}
Go back and re-run the above code cell two or three more times. What happens? Probably you can guess what the \mono{len} command is for. Now let's get really carried away and do some ``for real'' coding, like loops!%
\begin{sageinput}
for i in range(10):
    empty_list.append(i)
print(empty_list)
\end{sageinput}
Notice the indentation in the second line. This is how Python handles things like for loops, with indentation rather than bracketing. We could say more about lists but perhaps it's time to talk about matrices. For further reading, you can \href{https://developers.google.com/edu/python/lists}{start here}\footnote{\nolinkurl{developers.google.com/edu/python/lists}\label{g:fn:idp141}}.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section A.3 SymPy for linear algebra}
\typeout{************************************************}
%
\begin{sectionptx}{SymPy for linear algebra}{}{SymPy for linear algebra}{}{}{x:section:sec-sympy}
\begin{introduction}{}%
\terminology{SymPy} is a Python library for symbolic algebra. On its own, it's not as powerful as programs like Maple, but it handles a lot of basic manipulations in a fairly simple fashion, and when we need more power, it can interface with other Python libraries.%
\par
Another advantage of SymPy is sophisticated ``pretty-printing''. In fact, we can enable MathJax within SymPy, so that output is rendered in the same way as when LaTeX is entered in a markdown cell.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection A.3.1 SymPy basics}
\typeout{************************************************}
%
\begin{subsectionptx}{SymPy basics}{}{SymPy basics}{}{}{x:subsection:subsec-sympy-basics}
Running the following Sage cell will load the SymPy library and turn on MathJax.%
\begin{sageinput}
from sympy import *
init_printing()
\end{sageinput}
\alert{Note:} the command \mono{from sympy import *} given above is \emph{not} best practice. It can be convenient when you want to do a quick calculation (for example, on a test), but can have unintended consequences. It is better to only load those parts of a library you are going to use; for example, \mono{from sympy import Matrix, init\_printing}.%
\par
If you are going to be working with multiple libraries, and more than one of them defines a certain command, you can use prefixes to indicate what library you want to use. For example, if you enter \mono{import sympy as sy}, each SymPy command will need to be appended with \mono{sy}; for example, you might write \mono{sy.Matrix} instead of simply \mono{Matrix}. Let's use SymPy to create a \(2\times 3\) matrix.%
\begin{sageinput}
from sympy import Matrix, init_printing
init_printing()
A = Matrix(2,3,[1,2,3,4,5,6])
A
\end{sageinput}
The \mono{A} on the second line asks Python to print the matrix using SymPy's printing support. If we use Python's \mono{print} command, we get something different; note that the next Sage cell remembers the values from the previous one, if you are using the \initialism{HTML} version of the book.%
\begin{sageinput}
print(A)
\end{sageinput}
We'll have more on matrices in \hyperref[x:subsection:subsec-sympy-matrix]{Subsection~{\xreffont\ref{x:subsection:subsec-sympy-matrix}}}. For now, let's look at some more basic constructions. One basic thing to be mindful of is the type of numbers we're working with. For example, if we enter \mono{2/7} in a code cell, Python will interpret this as a floating point number (essentially, a division).%
\par
(If you are using Sage cells in HTML rather than Jupyter, this will automatically be interpreted as a fraction.)%
\begin{sageinput}
2/7
\end{sageinput}
But we often do linear algebra over the rational numbers, and so SymPy will let you specify this. First, you'll need to load the \mono{Rational} function.%
\begin{sageinput}
from sympy import Rational
Rational(2,7)
\end{sageinput}
You might not think to add the comma above, because you're used to writing fractions like \(2/7\). Fortunately, the SymPy authors thought of that:%
\begin{sageinput}
Rational(2/7)
\end{sageinput}
Hmm... You might have got the output you expected in the cell above, but maybe not. If you got a much worse looking fraction, read on.%
\par
Another cool command is the \mono{sympify} command, which can be called with the shortcut \mono{S}. The input \mono{2} is interpreted as an \mono{int} by Python, but \mono{S(2)} is a ``SymPy \mono{Integer}'':%
\begin{sageinput}
from sympy import S
S(2)/7
\end{sageinput}
Of course, sometimes you \emph{do} want to use floating point, and you can specify this, too:%
\begin{sageinput}
2.5
\end{sageinput}
\begin{sageinput}
from sympy import Float
Float(2.5)
\end{sageinput}
\begin{sageinput}
Float(2.5e10)
\end{sageinput}
One note of caution: \mono{Float} is part of SymPy, and not the same as the core Python \mono{float} command. You can also put decimals into the Rational command and get the corresponding fraction:%
\begin{sageinput}
Rational(0.75)
\end{sageinput}
The only thing to beware of is that computers convert from decimal to binary and then back again, and sometimes weird things can happen:%
\begin{sageinput}
Rational(0.2)
\end{sageinput}
Of course, there are workarounds. One way is to enter \(0.2\) as a string:%
\begin{sageinput}
Rational('0.2')
\end{sageinput}
Another is to limit the size of the denominator:%
\begin{sageinput}
Rational(0.2).limit_denominator(10**12)
\end{sageinput}
Try some other examples above. Some inputs to try are \mono{1.23} and \mono{23e-10}%
\par
We can also deal with repeating decimals. These are entered as strings, with square brackets around the repeating part. Then we can ``sympify'':%
\begin{sageinput}
S('0.[1]')
\end{sageinput}
Finally, SymPy knows about mathematical constants like \(e, \pi, i\), which you'll need from time to time in linear algebra. If you ever need \(\infty\), this is entered as \mono{oo}.%
\begin{sageinput}
I*I
\end{sageinput}
\begin{sageinput}
I-sqrt(-1)
\end{sageinput}
\begin{sageinput}
from sympy import pi
pi.is_irrational
\end{sageinput}
Finally, from time to time you may need to include parameters (variables) in an expression. Typical code for this is of the form \mono{a, b, c = symbols(\textquotesingle{}a b c\textquotesingle{}, real = True, constant = True)}. Here, we introduce the symbols \mono{a,b,c} with the specification that they represent real-valued constants.%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection A.3.2 Matrices in SymPy}
\typeout{************************************************}
%
\begin{subsectionptx}{Matrices in SymPy}{}{Matrices in SymPy}{}{}{x:subsection:subsec-sympy-matrix}
Here we collect some of the SymPy commands used throughout this text, for ease of reference. For further details, please consult the \href{https://docs.sympy.org/latest/modules/matrices/matrices.html}{online documentation}\footnote{\nolinkurl{docs.sympy.org/latest/modules/matrices/matrices.html}\label{g:fn:idp142}}.%
\par
To create a \(2\times 3\) matrix, we can write either \mono{A=Matrix(2,3,[1,2,3,4,5,6])} or \mono{A=Matrix([[1,2,3],[4,5,6]])}, where of course the size and entries can be changed to whatever you want. The former method is a bit faster, but once your matrices get a bit bigger, the latter method is less prone to typos.%
\begin{sageinput}
A=Matrix(2,3,[1,2,3,4,5,6])
B=Matrix([[1,2,3],[4,5,6]])
A,B
\end{sageinput}
Also of note: a column vector \(\bbm 1\\2\\3\ebm\) can be entered using \mono{Matrix([1,2,3])}. There are also certain built in special matrices. To get an \(n\times n\) identity matrix, use \mono{eye(n)}. To get an \(m\times n\) zero matrix, use \mono{zeros(m,n)}, or \mono{zeros(n)} for a square matrix. There is also syntax for diagonal matrices, such as \mono{diag(1,2,3)}. What's cool is that you can even use this for block diagonal matrices:%
\begin{sageinput}
A=Matrix(2,2,[1,2,3,4])
B=Matrix(2,2,[5,6,7,8])
D=diag(A,B)
D
\end{sageinput}
To get the reduced row-echelon form of the matrix \(A\), simply use \mono{A.rref()}. Addition, subtraction, and multiplication use the obvious syntax: \mono{A+B}, \mono{A*B}, etc.\@. The determinant of a square matrix is given by \mono{A.det()}. Inverses can be computed using \mono{A.inv()} or \mono{A**-1}. The latter is rather natural, since powers in general are entered as \mono{A**n} for \(A^n\).%
\par
In most cases where you want to reduce a matrix, you're going to want to simply use the \mono{rref} function. But there are times where this can be overzealous; for example, if you have a matrix with one or more symbols. One option is to replace \mono{A.rref()} with \mono{A.echelon\_form()}. The \mono{echelon\_form} function creates zeros in the pivot columns, but does not create leading on ones.%
\par
For example, let's take the matrix \(A = \bbm a \amp 2\amp b\\2\amp 1\amp a\\2a\amp b\amp 3\ebm\). Note the difference in output between \mono{rref} and \mono{echelon\_form}.%
\begin{sageinput}
from sympy import Symbol
a = Symbol('a')
b = Symbol('b')
A = Matrix(3,3,[a,2,b,2,1,a,2*a,b,3])
A, A.rref(), A.echelon_form()
\end{sageinput}
It is possible to manually perform row operations when you need additional control. This is achieved using the function \mono{A.elementary\_row\_op(<arguments>)}, with arguments \mono{op,row,k,row1,row2}.%
\par
We have the following general syntax:%
\begin{itemize}[label=\textbullet]
\item{}To swap two rows:%
\begin{itemize}[label=$\circ$]
\item{}\mono{op=\textquotesingle{}n<->m\textquotesingle{}}%
\item{}\mono{row1=i}, where \mono{i} is the index of the first row being swapped (remembering that rows are indexed starting with \(0\) for the first row).%
\item{}\mono{row2=j}, where \mono{j} is the index of the second row being swapped.%
\end{itemize}
%
\item{}To rescale a row:%
\begin{itemize}[label=$\circ$]
\item{}\mono{op=\textquotesingle{}n->kn\textquotesingle{}}%
\item{}\mono{row=i}, where \mono{i} is the index of the row being rescaled.%
\item{}\mono{k=c}, where \mono{c} is the value of the scalar you want to multiply by.%
\end{itemize}
%
\item{}To add a multiple of one row to another:%
\begin{itemize}[label=$\circ$]
\item{}\mono{op=\textquotesingle{}n->n+km\textquotesingle{}}%
\item{}\mono{row=i}, where \mono{i} is the index of the row you want to change.%
\item{}\mono{k=c}, where \mono{c} is the multiple of the other row.%
\item{}\mono{row2=j}, where \mono{j} is the index of the other row.%
\end{itemize}
%
\end{itemize}
%
\par
When studying matrix transformations, we are often interested in the \emph{null space} and \emph{column space}, since these correspond to the kernel and image of a linear transformation. This is achieved, simply enough, using \mono{A.nullspace()} and \mono{A.colspace()}. The output will be a basis of column vectors for these spaces, and these are exactly the ones you'd find doing Gaussian elimination by hand.%
\par
Once you get to orthogonality, you'll want to be able to compute things like dot products, and transpose. These are simple enough. The dot product of vectors \mono{X,Y} is simply \mono{X.dot(Y)}. The transpose of a matrix \mono{A} is \mono{A.T}. As we should expect, \mono{X\textbackslash{}dotp Y = X\textasciicircum{}TY}.%
\begin{sageinput}
X=Matrix(3,1,[1,2,3])
Y=Matrix(3,1,[4,5,6])
X.dot(Y),(X.T)*Y
\end{sageinput}
Of course, nobody wants to do things like the Gram Schmidt algorithm by hand. Fortunately, there's a function for that. If we have vectors \mono{X,Y,Z}, we can make a list \mono{L=[X,Y,Z]}, and perform Gram Schmidt with \mono{GramSchmidt(L)}. If you want your output to be an orthonormal basis (and not merely orthogonal), then you can use \mono{GramSchmidt(L,true)}.%
\par
It's useful to note that the output from functions like \mono{nullspace()} are automatically treated as lists. So one can use simple code like the following:%
\begin{sageinput}
from sympy import GramSchmidt
A=Matrix(2,3,[1,0,3,2,-1,4])
L=A.nullspace()
GramSchmidt(L)
\end{sageinput}
If for some reason you need to reference particular vectors in a list, this can be done by specifying the index. If \mono{L=[X,Y,Z]}, then \mono{L[0]==X}, \mono{L[1]==Y}, and \mono{L[2]==Z}.%
\par
Next up is eigenvalues and eigenvectors. Given an \(n\times n\) matrix \(A\), we have the following:%
\begin{itemize}[label=\textbullet]
\item{}For the characteristic polynomial, use \mono{A.charpoly()}. However, the result will give you something SymPy calls a ``PurePoly'', and the \mono{factor} command will have no effect. Instead, use \mono{A.charpoly().as\_expr()}.%
\item{}If we know that \(3\) is an eigenvalue of a \(4\times 4\) matrix \(A\), one way to get a basis for the eigenspace \(E_3(A)\) is to do:%
\begin{codedisplay}
B=A-3*eye(4)
B.nullspace()
\end{codedisplay}
If you just want all the eigenvalues and eigenvectors without going through the steps, then you can simply execute \mono{A.eigenvects()}. The result is a list of lists \textemdash{} each list in the list is of the form: eigenvalue, multiplicity, basis for the eigenspace.%
\par
For diagonalization, one can do \mono{A.diagonalize()}. But this will not necessarily produce orthogonal diagonalization for a symmetric matrix.%
\end{itemize}
%
\par
For complex vectors and matrices, the main additional operation we need is the \terminology{hermitian conjugate}. The hermitian conjugate of a matrix \mono{A} is called using \mono{A.H}, which is simple enough. Unfortunately, there is no built-in complex inner product, perhaps because mathematicians and physicists cannot agree on which of the two vectors in the inner product should have the complex conjugate applied to it. Since we define the complex inner product by \(\langle \zz,\ww\rangle = \zz\dotp\bar{\ww}\), we can execute the inner product in SymPy using \mono{Z.dot(W.H)}, or \mono{(W.H)*Z}, although the latter gives the output as a \(1\times 1\) matrix rather than a number.%
\par
Don't forget that when entering complex matrices, the complex unit is entered as \mono{I}. Also, complex expressions are not simplified by default, so you will often need to wrap your output line in \mono{simplify()}. The Sage Cell below contains complete code for the unitary diagonalization of a \(2\times 2\) hermitian matrix with distinct eigenvalues. When doing a problem like this in a Sage cell, it's a good idea to execute each line of code (and display output) before moving on to the next. In this case, printing the output for the list \mono{L} given by \mono{A.eigenvects()} helps explain the complicated-looking definitions of the vectors \mono{v,w}. Of course, if we had a matrix with repeated eigenvalues, we'd need to add steps involving Gram Schmidt.%
\begin{sageinput}
from sympy import Matrix,init_printing,simplify
init_printing()
A = Matrix(2,2,[4,3-I,3+I,1])
L = A.eigenvects()
v = ((L[0])[2])[0]
w = ((L[1])[2])[0]
u1 = (1/v.norm())*v
u2 = (1/w.norm())*w
U = u1.row_join(u2)
u1, u2, U, simplify(U.H*A*U)
\end{sageinput}
There are a few other commands that might come in handy as you work through this material:%
\begin{itemize}[label=\textbullet]
\item{}Two matrices can be glued together. If matrices \mono{A,B} have the same number of rows, the command \mono{A.row\_join(B)} will glue the matrices together, left-to-right. If they have the same number of columns, \mono{A.col\_join(B)} will glue them together top-to-bottom.%
\item{}To insert a column \mono{C} into a matrix \mono{M} (of appropriate size) as the \(j\)th column, you can do \mono{M.col\_insert(j,C)}. Just remember that columns are indexed starting at zero, so you might want \mono{j-1} instead of \mono{j}. This can be useful for things like solving a system \(A\xx=B\), where you want to append the column \(B\) to the matrix \(A\).%
\item{}A \(QR\)-factorization can be performed using \mono{Q,R=A.QRdecomposition()}%
\item{}The Jordan canonical form \(M\) of a matrix \(A\) can be obtained (along with the matrix \(P\) whose columns are a Jordan basis) using \mono{P,M=A.jordan\_form()}.%
\end{itemize}
%
\end{subsectionptx}
\end{sectionptx}
\end{appendixptx}
%
%
\typeout{************************************************}
\typeout{Appendix B Solutions to Selected Exercises}
\typeout{************************************************}
%
\begin{solutions-chapter}{Solutions to Selected Exercises}{}{Solutions to Selected Exercises}{}{}{g:solutions:idp143}
\par\medskip
\noindent\textbf{\Large{}1\space\textperiodcentered\space{}Vector spaces\\
1.1\space\textperiodcentered\space{}Abstract vector spaces}
\begin{inlinesolution}{1.1.4}{}{x:exercise:ex-more-props}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp144-back}{}\quad{}%
\begin{enumerate}
\item{}Suppose \(\uu+\vv=\uu+\ww\). By adding \(-\uu\) on the left of each side, we obtain:%
\begin{align*}
-\uu+(\uu+\vv) \amp =-\uu+(\uu+\ww)\\
(-\uu+\uu)+\vv \amp =(-\uu+\uu)+\ww \quad \text{ by A3}\\
\zer+\vv \amp =\zer+\ww \quad \text{ by A5}\\
\vv \amp =\ww \quad \text{ by A4}\text{.}
\end{align*}
%
\item{}We have \(c\zer = c(\zer+\zer) = c\zer +c\zer\), by A4 and S2, respectively. Adding \(-c\zer\) to both sides (and using axioms A3, A4, A5 as in the proof of \hyperref[x:theorem:thm-zero-mult]{Theorem~{\xreffont\ref{x:theorem:thm-zero-mult}}}) we get \(\zer = c\zer\).%
\item{}The main difficulty with this problem is getting the logic of the statement correct, and making sure you know what it is you're trying to prove. The desired conclusion is an \emph{or} statement, which means it suffices to establish one part or the other. Typically, the way to proceed in these cases is to argue that if the first part is false, then the second must be true. This is how we proceed here.%
\par
Suppose that \(c\vv=\zer\). If \(c=0\), then it's true that \(c=0\) or \(\vv=\zer\), so there's nothing to prove. Suppose then that \(c\neq 0\). Then there exists a real number \(\frac1c\) such that \(c\bigl(\frac1c\bigr)=1\). Multiplying both sides of \(c\vv=\zer\) by \(\frac1c\), we get%
\begin{equation*}
\frac1c(c\vv)=\frac1c\zer\text{.}
\end{equation*}
By the previous problem, we know that \(\frac1c\zer = \zer\), and by axioms S4 and S5, we have%
\begin{equation*}
\frac1c(c\vv)=\bigl(\frac1c\cdot c\bigr)\vv = 1\vv = \vv\text{,}
\end{equation*}
showing that \(\vv=\zer\), and thus \(c=0\) or \(\vv=\zer\). (This is a proof by cases, relying on the tautology \(c=0\) or \(c\neq 0\).)%
\item{}Suppose there are two vectors \(\zer_1,\zer_2\) that act as additive identities. Then%
\begin{align*}
\zer_1 \amp = \zer_1+\zer_2 \quad \text{ since } \vv+\zer_2=\vv \text{ for any } \vv\\
\amp =\zer_2+\zer_1 \quad \text{ by axiom A2}\\
\amp \zer_2 \quad \text{ since } \vv+\zer_1=\vv \text{ for any } \vv
\end{align*}
So any two vectors satisfying the property in A4 must, in fact, be the same.%
\item{}Let \(\vv\in V\), and suppose there are vectors \(\ww_1,\ww_2\in V\) such that \(\vv+\ww_1=\zer\) and \(\vv+\ww_2=\zer\). Then%
\begin{align*}
\ww_1 \amp = \ww_1+\zer \quad  \text{ by A4}\\
\amp = \ww_1+(\vv+\ww_2) \quad \text{ by assumption}\\
\amp = (\ww_1+\vv)+\ww_2 \quad \text{ by A3}\\
\amp = (\vv+\ww_1)+\ww_2 \quad \text{ by A2}\\
\amp = \zer+\ww_2 \quad \text{ by assumption}\\
\amp \ww_2 \quad \text{ by A4}\text{.}
\end{align*}
%
\end{enumerate}
%
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}1.3\space\textperiodcentered\space{}Span}
\begin{inlinesolution}{1.3.1}{}{g:exercise:idp19}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp145-back}{}\quad{}This is really asking: are there scalars \(s,t\) such that%
\begin{equation*}
s\bbm 1\\1\ebm + t\bbm -1\\2\ebm = \bbm 2\\3\ebm\text{?}
\end{equation*}
And this, in turn, is equivalent to the system%
\begin{align*}
s -t \amp=2 \\
s+2t \amp=3 \text{,}
\end{align*}
which is the same as the matrix equation%
\begin{equation*}
\bbm 1\amp -1\\1\amp 2\ebm\bbm s\\t\ebm = \bbm 2\\3\ebm.
\end{equation*}
Solving the system confirms that there is indeed a solution, so the answer to our original question is yes.%
\par
To confirm the above example (and see what the solution is), we can use the computer. This first code cell loads the \mono{sympy} Python library, and then configures the output to look nice. For details on the code used below, see \hyperref[x:appendix:ch-computation]{the Appendix}.%
\begin{sageinput}
from sympy import Matrix, init_printing
init_printing()
A = Matrix(2,3,[1,-1,2,1,2,3])
A.rref()
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp 0\amp \frac73\\0\amp 1\amp \frac13\ebm, (0,1)\]
\end{sageoutput}
The above code produces the reduced row-echelon form of the augmented matrix for our system. (The tuple \((0,1)\) lists the pivot columns \textemdash{} note that Python indexes the columns starting at \(0\) rather than \(1\).) Do you remember how to get the answer from here? Here's another approach.%
\begin{sageinput}
B = Matrix(2,2,[1,-1,1,2])
B
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp -1\\1\amp 2\ebm\]
\end{sageoutput}
\begin{sageinput}
C = Matrix(2,1,[2,3])
X = (B**-1)*C
X
\end{sageinput}
\begin{sageoutput}
\[\bbm \frac 73\\\frac13\ebm\]
\end{sageoutput}
\end{inlinesolution}%
\begin{inlinesolution}{1.3.2}{}{g:exercise:idp20}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp146-back}{}\quad{}We seek scalars \(s,t\) such that%
\begin{equation*}
s(1+2x-2x^2)+t(3+5x+2x^2)=1+x+4x^2\text{.}
\end{equation*}
On the left-hand side, we expand and gather terms:%
\begin{equation*}
(s+3t)+(2s+5t)x+(-2s+2t)x^2 = 1+x+4x^2\text{.}
\end{equation*}
These two polynomials are equal if and only if we can solve the system%
\begin{align*}
s+3t \amp = 1 \\
2s+5t \amp =1\\
-2s+2t \amp =4\text{.}
\end{align*}
%
\par
We can solve this computationally using matrices again.%
\begin{sageinput}
from sympy import Matrix, init_printing
init_printing()
M = Matrix(3,3,[1,3,1,2,5,1,-2,2,4])
M.rref()
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp 1\ebm, (0,1,2)\]
\end{sageoutput}
So, what's the answer? Is \(p(x)\) in the span? Can we determine what polynomials are in the span? Let's consider a general polynomial \(q(x)=a+bx+cx^2\). A bit of thought tells us that the coefficients \(a,b,c\) should replace the constants \(1,1,4\) above.%
\begin{sageinput}
from sympy import symbols
a, b, c = symbols('a b c', real = True, constant = True)
N = Matrix(3,3,[1,3,a,2,5,b,-2,2,c])
N
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp 3\amp a\\2\amp 5\amp b\\-2\amp 2\amp c\ebm\]
\end{sageoutput}
Asking the computer to reduce this matrix to \initialism{RREF} won't produce the desired result. But we can always specify row operations.%
\begin{sageinput}
N1 = N.elementary_row_op(op='n->n+km',row=1,k=-2,row2=0)
N1
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp 3\amp a\\0\amp -1\amp -2a+b\\-2\amp 2\amp c\ebm\]
\end{sageoutput}
In the \mono{elementary\_row\_op} function called above, we are asking the computer to change row \(1\) (the second row) by adding \(-2\) times row \(0\) )the first row). See \hyperref[x:subsection:subsec-sympy-matrix]{Section~{\xreffont\ref{x:subsection:subsec-sympy-matrix}}} for complete details on this syntax.%
\par
Now we repeat. Here is another cell to work with:%
Another option is to replace the \mono{rref()} function with the \mono{echelon\_form()} function, which doesn't simplify quite as far:%
\begin{sageinput}
a, b, c = symbols('a b c', real = True, constant = True)
N = Matrix(3,3,[1,3,a,2,5,b,-2,2,c])
N.echelon_form()
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp 3\amp a\\0\amp -1\amp -2a+b\\0\amp 0\amp 14a-8b-c\ebm\]
\end{sageoutput}
For a consistent system, we need \(c=14a-8b\).%
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}1.5\space\textperiodcentered\space{}Linear Independence}
\begin{inlinesolution}{1.5.2}{}{g:exercise:idp31}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp147-back}{}\quad{}Again, we set up a matrix and reduce:%
\begin{sageinput}
from sympy import Matrix,init_printing
init_printing()
A = Matrix(3,3,[1,-1,-1,2,0,4,0,3,9])
A.rref()
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp 0\amp 2\\0\amp 1\amp 3\\0\amp 0\amp 0\ebm, (0,1)\]
\end{sageoutput}
Notice that this time we don't get a unique solution, so we can conclude that these vectors are \emph{not} independent. Furthermore, you can probably deduce from the above that we have \(2\vv_1+3\vv_2-\vv_3=\mathbf{0}\). Now suppose that \(\ww\in\spn\{\vv_1,\vv_2,\vv_3\}\). In how many ways can we write \(\ww\) as a linear combination of these vectors?%
\end{inlinesolution}%
\begin{inlinesolution}{1.5.3}{}{g:exercise:idp32}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp148-back}{}\quad{}In each case, we set up the defining equation for independence, collect terms, and then analyze the resulting system of equations. (If you work with polynomials often enough, you can probably jump straight to the matrix. For now, let's work out the details.)%
\par
Suppose%
\begin{equation*}
r(x^2+1)+s(x+1)+tx = 0\text{.}
\end{equation*}
Then \(rx^2+(s+t)x+(r+s)=0=0x^2+0x+0\), so%
\begin{align*}
r \amp =0\\
s+t \amp =0\\
r+s\amp =0\text{.}
\end{align*}
%
\par
And in this case, we don't even need to ask the computer. The first equation gives \(r=0\) right away, and putting that into the third equation gives \(s=0\), and the second equation then gives \(t=0\).%
\par
Since \(r=s=t=0\) is the only solution, the set is independent.%
\par
Repeating for \(S_2\) leads to the equation%
\begin{equation*}
(r+2s+t)x^2+(-r+s+5t)x+(3r+5s+t)1=0.
\end{equation*}
This gives us:%
\begin{sageinput}
from sympy import Matrix,init_printing
init_printing()
A = Matrix(3,3,[1,2,1,-1,1,5,3,5,1])
A.rref()
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp 0\amp -3\\0\amp 1\amp 2\\0\amp 0\amp 0\ebm, (0,1)\]
\end{sageoutput}
\end{inlinesolution}%
\begin{inlinesolution}{1.5.4}{}{g:exercise:idp33}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp149-back}{}\quad{}Again, we set a linear combination equal to the zero vector, and combine:%
\begin{align*}
a\bbm -1\amp 0\\0\amp -1\ebm +b\bbm 1\amp -1\\ -1\amp 1\ebm
+c\bbm 1\amp 1\\1\amp 1\ebm +d \bbm 0\amp -1\\-1\amp 0\ebm = \bbm 0\amp 0\\ 0\amp 0\ebm\\
\bbm -a+b+c\amp -b+c-d\\-b+c-d\amp -a+b+c\ebm = \bbm 0\amp 0\\0\amp 0\ebm\text{.}
\end{align*}
%
\par
We could proceed, but we might instead notice right away that equations 1 and 4 are identical, and so are equations 2 and 3. With only two distinct equations and 4 unknowns, we're certain to find nontrivial solutions.%
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}1.6\space\textperiodcentered\space{}Basis and dimension}
\begin{inlinesolution}{1.6.5}{}{g:exercise:idp34}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp150-back}{}\quad{}Let \(X=\bbm a\amp b\\c\amp d\ebm\). Then \(AX = \bbm a+c\amp b+d\\0\amp 0\ebm\), and \(XA = \bbm a\amp a\\c\amp c\ebm\), so the condition \(AX=XA\) requires:%
\begin{align*}
a+c \amp = a\\
b+d \amp = a\\
0 \amp = c\\
0 \amp =c\text{.}
\end{align*}
%
\par
So \(c=0\), in which case the first equation \(a=a\) is trivial, and we are left with the single equation \(a=b+d\). Thus, our matrix \(X\) must be of the form%
\begin{equation*}
X = \bbm b+d \amp b\\0\amp d\ebm = b\bbm 1\amp 1\\0\amp 0\ebm + d\bbm 1\amp 0\\0\amp 1\ebm\text{.}
\end{equation*}
Since the matrices \(\bbm 1\amp 1\\0\amp 0\ebm\) and \(\bbm 1\amp 0\\0\amp 1\ebm\) are not scalar multiples, they must be independent, and therefore, they form a basis for \(U\).%
\end{inlinesolution}%
\begin{inlinesolution}{1.6.7}{}{g:exercise:idp37}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp151-back}{}\quad{}We need to show that the set is independent, and that it spans.%
\par
The set is independent if the equation%
\begin{equation*}
x(1,1,0)+y(1,0,1)+z(0,1,1)=(0,0,0)
\end{equation*}
has \(x=y=z=0\) as its only solution. This equation is equivalent to the system%
\begin{align*}
x+y \amp =0\\
x+z \amp =0\\
y+z \amp =0\text{.}
\end{align*}
%
\par
We know that the solution to this system is unique if the coefficient matrix \(A = \bbm 1\amp 1\amp 0\\1\amp 0\amp 1\\0\amp 1\amp 1\ebm\) is invertible. Note that the columns of this matrix are vectors in our set.%
\par
We can determine invertibility either by showing that the \initialism{RREF} of \(A\) is the identity, or by showing that the determinant of \(A\) is nonzero. Either way, this is most easily done by the computer:%
\begin{sageinput}
from sympy import Matrix,init_printing
init_printing()
A = Matrix([[1,1,0],[1,0,1],[0,1,1]])
A.rref(), A.det()
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 1\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp 1\ebm, (0,1,2)\right), -2\]
\end{sageoutput}
Our set of vectors is therefore linearly independent. Now, to show that it spans, we need to show that for any vector \((a,b,c)\), the equation%
\begin{equation*}
x(1,1,0)+y(1,0,1)+z(0,1,1)=(a,b,c)
\end{equation*}
has a solution. But we know that this system has the same coefficient matrix as the one above, and that existence of a solution again follows from invertibility of \(A\), which we have already established.%
\par
Note that for three vectors in \(\R^3\), once independence has been confirmed, span is automatic. We will soon see that this is not a coincidence.%
\item[(b)]\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp152-back}{}\quad{}Based on what we learned from the first set, determining whether or not this set is a basis is equivalent to determining whether or not the matrix \(A\) whose columns consist of the vectors in the set is invertible. We form the matrix%
\begin{equation*}
A = \bbm -1\amp 1\amp 1\\1\amp -1\amp 1\\1\amp 1\amp -1\ebm
\end{equation*}
and then check invertibility using the computer.%
\begin{sageinput}
A = Matrix([[-1,1,1],[1,-1,1],[1,1,-1]])
A.det()
\end{sageinput}
\begin{sageoutput}
\[4\]
\end{sageoutput}
Since the determinant is nonzero, our set is a basis.%
\end{enumerate}
\end{inlinesolution}%
\begin{inlinesolution}{1.6.10}{}{g:exercise:idp40}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp153-back}{}\quad{}%
\begin{enumerate}[label=\alph*]
\item{}By definition, \(U_1 = \spn \{1+x,x+x^2\}\), and these vectors are independent, since neither is a scalar multiple of the other. Since there are two vectors in this basis, \(\dim U_1 = 2\).%
\item{}If \(p(1)=0\), then \(p(x)=(x-1)q(x)\) for some polynomial \(q\). Since \(U_2\) is a subspace of \(P_2\), the degree of \(q\) is at most 2. Therefore, \(q(x)=ax+b\) for some \(a,b\in\R\), and%
\begin{equation*}
p(x) = (x-1)(ax+b) = a(x^2-x)+b(x-1)\text{.}
\end{equation*}
Since \(p\) was arbitrary, this shows that \(U_2 = \spn\{x^2-x,x-1\}\).%
\par
The set \(\{x^2-x,x-1\}\) is also independent, since neither vector is a scalar multiple of the other. Therefore, this set is a basis, and \(\dim U_2=2\).%
\item{}If \(p(x)=p(-x)\), then \(p(x)\) is an even polynomial, and therefore \(p(x)=a+bx^2\) for \(a,b\in\R\). (If you didn't know this it's easily verified: if%
\begin{equation*}
a+bx+cx^2 = a+b(-x)+c(-x)^2\text{,}
\end{equation*}
we can immediately cancel \(a\) from each side, and since \((-x)^2=x^2\), we can cancel \(cx^2\) as well. This leaves \(bx=-bx\), or \(2bx=0\), which implies that \(b=0\).)%
\par
It follows that the set \(\{1,x^2\}\) spans \(U_3\), and since this is a subset of the standard basis \(\{1,x,x^2\}\) of \(P_2\), it must be independent, and is therefore a basis of \(U_3\), letting us conclude that \(\dim U_3=2\).%
\end{enumerate}
%
\end{inlinesolution}%
\begin{inlinesolution}{1.6.14}{}{g:exercise:idp43}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp154-back}{}\quad{}By the previous theorem, we can form a basis by adding vectors from the standard basis%
\begin{equation*}
\left\{\bbm 1\amp 0\\0\amp 0\ebm, \bbm 0\amp 1\\0\amp 0\ebm, \bbm 0\amp 0\\1\amp 0\ebm, \bbm 0\amp 0\\0\amp 1\ebm\right\}\text{.}
\end{equation*}
It's easy to check that \(\bbm 1\amp 0\\0\amp 0\ebm\) is not in the span of \(\{\vv,\ww\}\). To get a basis, we need one more vector. Observe that all three of our vectors so far have a zero in the \((2,1)\)-entry. Thus, \(\bbm 0\amp 0\\1\amp 0\ebm\) cannot be in the span of the first three vectors, and adding it gives us our basis.%
\end{inlinesolution}%
\begin{inlinesolution}{1.6.15}{}{g:exercise:idp44}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp155-back}{}\quad{}Again, we only need to add one vector from the standard basis \(\{1,x,x^2,x^3\}\), and it's not too hard to check that any of them will do.%
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}2\space\textperiodcentered\space{}Linear Transformations\\
2.1\space\textperiodcentered\space{}Definition and examples}
\begin{inlinesolution}{2.1.6}{}{g:exercise:idp51}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp156-back}{}\quad{}Since we know the value of \(T\) on the standard basis, we can use properties of linear transformations to immediately obtain the answer:%
\begin{align*}
T\bbm -2\\4\ebm \amp= T\left(-2\bbm 1\\0\ebm +4\bbm 0\\1\ebm\right)\\
\amp = -2T\bbm1\\0\ebm+4T\bbm 0\\1\ebm\\
\amp = -2\bbm 3\\-4\ebm +4\bbm 5\\2\ebm\\
\amp = \bbm 14\\16\ebm\text{.}
\end{align*}
%
\end{inlinesolution}%
\begin{inlinesolution}{2.1.7}{}{g:exercise:idp52}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp157-back}{}\quad{}At first, this example looks the same as the one above, and to some extent, it is. The difference is that this time, we're given the values of \(T\) on a basis that is not the standard one. This means we first have to do some work to determine how to write the given vector in terms of the given basis.%
\par
Suppose we have \(a\bbm 3\\1\ebm+b\bbm 2\\-5\ebm = \bbm 4\\3\ebm\) for scalars \(a,b\). This is equivalent to the matrix equation%
\begin{equation*}
\bbm 3\amp 2\\1\amp -5\ebm\bbm a\\b\ebm = \bbm 4\\3\ebm.
\end{equation*}
Solving (perhaps using the code cell below), we get \(a=\frac{26}{17}, b = -\frac{5}{17}\).%
\begin{sageinput}
from sympy import Matrix,init_printing
init_printing()
A = Matrix(2,2,[3,2,1,-5])
B = Matrix(2,1,[4,3])
(A**-1)*B
\end{sageinput}
\begin{sageoutput}
\[\bbm \frac{26}{17}\\-\frac{5}{17}\ebm\]
\end{sageoutput}
Therefore,%
\begin{equation*}
T\bbm 4\\3\ebm = \frac{26}{17}\bbm 1\\4\ebm -\frac{5}{17}\bbm 2\\-1\ebm = \bbm 16/17\\109/17\ebm\text{.}
\end{equation*}
%
\end{inlinesolution}%
\begin{inlinesolution}{2.1.8}{}{g:exercise:idp53}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp158-back}{}\quad{}We need to find scalars \(a,b,c\) such that%
\begin{equation*}
2-x+3x^2 = a(x+2)+b(1)+c(x^2+x)\text{.}
\end{equation*}
We could set up a system and solve, but this time it's easy enough to just work our way through. We must have \(c=3\), to get the correct coefficient for \(x^2\). This gives%
\begin{equation*}
2-x+3x^2=a(x+2)+b(1)+3x^2+3x\text{.}
\end{equation*}
Now, we have to have \(3x+ax=-x\), so \(a=-4\). Putting this in, we get%
\begin{equation*}
2-x+3x^2=-4x-8+b+3x^2+3x\text{.}
\end{equation*}
Simiplifying this leaves us with \(b=10\). Finally, we find:%
\begin{align*}
T(2-x+3x^2) \amp = T(-4(x+2)+10(1)+3(x^2+x)) \\
\amp = -4T(x+2)+10T(1)+3T(x^2+x)\\
\amp = -4(1)+10(5)+3(0) = 46\text{.}
\end{align*}
%
\end{inlinesolution}%
\begin{inlinesolution}{2.1.9}{}{g:exercise:idp54}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp159-back}{}\quad{}Since \(\{(1,2),(-1,1)\}\) forms a basis of \(\R^2\) (the vectors are not parallel and there are two of them), it suffices to determine how to write a general vector in terms of this basis. Suppose%
\begin{equation*}
x(1,2)+y(-1,1)=(a,b)
\end{equation*}
for a general element \((a,b)\in \R^2\). This is equivalent to the matrix equation \(\bbm 1\amp -1\\2\amp 1\ebm\bbm x\\y\ebm = \bbm a\\b\ebm\), which we can solve as \(\bbm x\\y\ebm = \bbm 1\amp -1\\2\amp 1\ebm^{-1}\bbm a\\b\ebm\):%
\begin{sageinput}
from sympy import Matrix, init_printing, symbols
init_printing()
a, b = symbols('a b', real = True, constant = True)
A = Matrix(2,2,[1,-1,2,1])
B = Matrix(2,1,[a,b])
(A**-1)*B
\end{sageinput}
\begin{sageoutput}
\[\bbm \frac{a}{3}+\frac{b}{3}\\-\frac{2a}{3}+\frac{b}{3}\ebm\]
\end{sageoutput}
This gives us the result%
\begin{equation*}
(a,b) = \frac13(a+b)\cdot (1,2)+\frac13(-2a+b)\cdot (-1,1).
\end{equation*}
Thus,%
\begin{align*}
T(a,b) \amp = \frac13(a+b)\cdot T(1,2)+\frac13(-2a+b)\cdot T(-1,1) \\
\amp = \frac13(a+b)\cdot (1,1,0)+\frac13(-2a+b)\cdot (0,2,-1)\\
\amp = \left(\frac{a+b}{3}, -a+b, \frac{2a-b}{3}\right)\text{.}
\end{align*}
%
\par
We conclude that%
\begin{equation*}
T(3,2) = \left(\frac53, -1, \frac43\right)\text{.}
\end{equation*}
%
\end{inlinesolution}%
\begin{inlinesolution}{2.1.10}{}{x:exercise:ex_lintrans-indep}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp160-back}{}\quad{}Let us suppose that \(\{T(\vv_1),\ldots, T(\vv_n)\}\) is linearly independent. We want to show that the set \(\{\vv_1,\ldots, \vv_n\}\) is linearly independent. To that end, suppose that we have%
\begin{equation*}
c_1\vv_1+\cdots + c_n\vv_n=\mathbf{0}
\end{equation*}
for some scalars \(c_1,\ldots, c_n\) (that we want to show must all equal zero).%
\par
We want to make use of our hypothesis, so we need to bring the linear map \(T\) into the picture. We apply \(T\) to both sides of the equation above, giving us:%
\begin{equation*}
T(c_1\vv_1+\cdots + c_n\vv_n)=T(\mathbf{0})\text{.}
\end{equation*}
Now we make use of both parts of \hyperref[x:theorem:thm-lt-props]{Theorem~{\xreffont\ref{x:theorem:thm-lt-props}}} to get%
\begin{equation*}
c_1T(\vv_1)+\cdots +c_nT(\vv_n) = \mathbf{0}\text{.}
\end{equation*}
By our hypothesis that the \(T(\vv_i)\) are independent, we must conclude that all the scalars are zero, and we're done.%
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}2.2\space\textperiodcentered\space{}Kernel and Image}
\begin{inlinesolution}{2.2.8}{}{x:exercise:ex-dimension-injection-surjection}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp161-back}{}\quad{}%
\begin{enumerate}
\item{}Suppose \(T:V\to W\) is injective. Then \(\ker T = \{0\}\), so%
\begin{equation*}
\dim V = 0 + \dim \im T \leq \dim W\text{,}
\end{equation*}
since \(\im T\) is a subspace of \(W\).%
\par
Conversely, suppose \(\dim V\leq \dim W\). Choose a basis \(\{\vv_1,\ldots, \vv_m\}\) of \(V\), and a basis \(\{\ww_1,\ldots, \ww_n\}\) of \(W\), where \(m\leq n\). By \hyperref[x:theorem:thm-define-using-basis]{Theorem~{\xreffont\ref{x:theorem:thm-define-using-basis}}}, there exists a linear transformation \(T:V\to W\) with \(T(\vv_i)=\ww_i\) for \(i=1,\ldots, m\). (The main point here is that we run out of basis vectors for \(V\) before we run out of basis vectors for \(W\).) This map is injective: if \(T(\vv)=\mathbf{0}\), write \(\vv=c_1\vv_1+\cdots + c_m\vv_m\). Then%
\begin{align*}
\mathbf{0} \amp = T(\vv)\\
\amp = T(c_1\vv_1+\cdots + c_m\vv_m)\\
\amp = c_1T(\vv_1)+\cdots + c_mT(\vv_m)\\
\amp = c_1\ww_1+\cdots +c_m\ww_m\text{.}
\end{align*}
Since \(\{\ww_1,\ldots, \ww_m\}\) is a subset of a basis, it's independent. Therefore, the scalars \(c_i\) must all be zero, and therefore \(\vv=\mathbf{0}\).%
\item{}Suppose \(T:V\to W\) is surjective. Then \(\dim \im T = \dim W\), so%
\begin{equation*}
\dim V = \dim \ker T + \dim W \geq  \dim W\text{.}
\end{equation*}
Conversely, suppose \(\dim V\geq \dim W\). Again, choose a basis \(\{\vv_1,\ldots, \vv_m\}\) of \(V\), and a basis \(\{\ww_1,\ldots, \ww_n\}\) of \(W\), where this time, \(m\geq n\). We can define a linear transformation as follows:%
\begin{equation*}
T(\vv_1)=\ww_1,\ldots, T(\vv_n)=\ww_n, \text{ and } T(\vv_j) = \mathbf{0} \text{ for } j>n.
\end{equation*}
It's easy to check that this map is a surjection: given \(\ww\in W\), we can write it in terms of our basis as \(\ww=c_1\ww_1+\cdots + c_n\ww_n\). Using these same scalars, we can define \(\vv=c_1\vv_1+\cdots + c_n\vv_n\in V\) such that \(T(\vv)=\ww\).%
\par
Note that it's not important how we define \(T(\vv_j)\) when \(j>n\). The point is that this time, we run out of basis vectors for \(W\) before we run out of basis vectors for \(V\). Once each vector in the basis of \(W\) is in the image of \(T\), we're guaranteed that \(T\) is surjective, and we can define the value of \(T\) on any remaining basis vectors however we want.%
\end{enumerate}
%
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}2.3\space\textperiodcentered\space{}Isomorphisms (a.k.a. invertible linear maps)}
\begin{inlinesolution}{2.3.6}{}{g:exercise:idp61}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp162-back}{}\quad{}Suppose we have linear maps \(U\xrightarrow{T} V\xrightarrow{S} W\), and let \(\uu_1,\uu_2\in U\). Then%
\begin{align*}
ST(\uu_1+\uu_2) \amp = S(T(\uu_1+\uu_2)) \\
\amp = S(T(\uu_1)+T(\uu_2))\\
\amp = S(T(\uu_1))+S(T(\uu_2)) \\
\amp = ST(\uu_1)+ST(\uu_2)\text{,}
\end{align*}
and for any scalar \(c\),%
\begin{equation*}
ST(c\uu_1) = S(T(c\uu_1))=S(cT(\uu_1)) = cS(T(\uu_1))=c(ST(\uu_1))\text{.}
\end{equation*}
%
\end{inlinesolution}%
\begin{inlinesolution}{2.3.7}{}{g:exercise:idp62}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp163-back}{}\quad{}Let \(\ww_1,\ww_2\in W\). Then there exist \(\vv_1,\vv_2\in V\)  with \(\ww_1=T(\vv_1), \ww_2=T(\vv_2)\). We then have%
\begin{align*}
T^{-1}(\ww_1+\ww_2) \amp = T^{-1}(T(\vv_1)+T(\vv_2)) \\
\amp = T^{-1}(T(\vv_1+\vv_2))\\
\amp = \vv_1+\vv_2\\
\amp = T^{-1}(\ww_1)+T^{-1}(\ww_2)\text{.}
\end{align*}
For any scalar \(c\), we similarly have%
\begin{equation*}
T^{-1}(c\ww_1) = T^{-1}(cT(\vv_1))=T^{-1}(T(c\vv_1)) = c\vv_1 = cT^{-1}(\ww_1)\text{.}
\end{equation*}
%
\end{inlinesolution}%
\begin{inlinesolution}{2.3.8}{}{g:exercise:idp63}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp64-back}{}\quad{}This is really a Math 2000 problem.%
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}3\space\textperiodcentered\space{}Orthogonality and Applications\\
3.1\space\textperiodcentered\space{}Orthogonal sets of vectors\\
3.1.1\space\textperiodcentered\space{}Basic definitions and properties}
\begin{inlinesolution}{3.1.4}{}{g:exercise:idp76}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp164-back}{}\quad{}Note that the distributive property, together with symmetry, let us handle this dot product using what is essentially ``\initialism{FOIL}'':%
\begin{align*}
(4\xx-3\yy)\dotp (\xx+5\yy)\amp = (4\xx)\dotp \xx+(4\xx)\dotp(5\yy)+(-3\yy)\dotp \xx+(-3\yy)\dotp(5\yy)\\
\amp = 4(\xx\dotp\xx)+(4\cdot 5)(\xx\dotp \yy)-3(\yy\dotp \xx)+(-3\cdot 5)(\yy\dotp\yy)\\
\amp = 4\len{\xx}^2+20\xx\dotp\yy-3\xx\dotp\yy-15\len{\yy}^2\\
\amp = 4(9)+17(-2)-15(1) = -13\text{.}
\end{align*}
%
\end{inlinesolution}%
\begin{inlinesolution}{3.1.5}{}{x:exercise:ex-norm-sum-square}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp165-back}{}\quad{}This is simply an exercise in properties of the dot product. We have%
\begin{align*}
\len{\xx+\yy}^2 \amp = (\xx+\yy)\dotp (\xx+\yy) \\
\amp = \xx\dotp \xx+\xx\dotp\yy+\yy\dotp\xx+\yy\dotp\yy\\
\amp =\len{\xx}^2+2\xx\dotp\yy+\len{\yy}^2\text{.}
\end{align*}
%
\end{inlinesolution}%
\begin{inlinesolution}{3.1.6}{}{g:exercise:idp77}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp166-back}{}\quad{}If \(\xx=\mathbf{0}\), then the result follows immediately from the dot product formula in \hyperref[x:definition:def-dot-prod-norm]{Definition~{\xreffont\ref{x:definition:def-dot-prod-norm}}}. Conversely, suppose \(\xx\dotp \vv_i=0\) for each \(i\). Since the \(\vv_i\) span \(\R^n\), there must exist scalars \(c_1,c_2,\ldots, c_k\) such that \(\xx=c_1\vv_1+c_2\vv_2+\cdots+c_k\vv_k\). But then%
\begin{align*}
\xx\dotp\xx \amp = \xx\dotp (c_1\vv_1+c_2\vv_2+\cdots+c_k\vv_k) \\
\amp = c_1(\xx\dotp \vv_1)+ c_2(\xx\dotp \vv_2)+\cdots +c_k(\xx\dotp \vv_k)\\
\amp = c_1(0)+c_2(0)+\cdots + c_k(0)=0\text{.}
\end{align*}
%
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}3.1.2\space\textperiodcentered\space{}Orthogonal sets of vectors}
\begin{inlinesolution}{3.1.10}{}{x:exercise:ex-orthogonal-set}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp167-back}{}\quad{}Clearly, all three vectors are nonzero. To confirm the set is orthogonal, we simply compute dot products:%
\begin{align*}
(1,0,1,0)\dotp (-1,0,1,1)\amp =-1+0+1+0=0\\
(-1,0,1,1)\dotp (1,1,-1,2)\amp =-1+0-1+2=0\\
(1,0,1,0)\dotp (1,1,-1,2) \amp = 1+0-1+0=0\text{.}
\end{align*}
%
\par
To find a fourth vector, we proceed as follows. Let \(\xx=(a,b,c,d)\). We want \(\xx\) to be orthogonal to the three vectors in our set. Computing dot products, we must have:%
\begin{align*}
(a,b,c,d)\dotp (1,0,1,0) \amp = a+c=0 \\
(a,b,c,d)\dotp (-1,0,1,1) \amp = -a+c+d=0 \\
(a,b,c,d)\dotp (1,1,-1,2) \amp = a+b-c+2d=0\text{.}
\end{align*}
This is simply a homogeneous system of three equations in four variables. Using the Sage cell below, we find that our vector must satisfy \(a=\frac12 d, b = -3d, c=-\frac12 d\).%
\begin{sageinput}
from sympy import Matrix, init_printing
init_printing()
A=Matrix(3,4,[1,0,1,0,-1,0,1,1,1,1,-1,2])
A.rref()
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\amp 0\amp 0\amp -\frac12\\ 0\amp 1\amp 0\amp 3\\ 0\amp 0\amp 1\amp \frac12\ebm, (0,1,2)\]
\end{sageoutput}
One possible nonzero solution is to take \(d=2\), giving \(\xx=(1,-6,-1,2)\). We'll leave the verification that this vector works as an easy exercise.%
\end{inlinesolution}%
\begin{inlinesolution}{3.1.14}{}{x:exercise:ex-test-span}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp168-back}{}\quad{}We compute%
\begin{align*}
\left(\frac{\vv\dotp\xx_1}{\len{\xx_1}^2}\right)\xx_1
\amp +\left(\frac{\vv\dotp\xx_2}{\len{\xx_2}^2}\right)\xx_2
+\left(\frac{\vv\dotp\xx_3}{\len{\xx_3}^2}\right)\xx_3\\
\amp = \frac{4}{2}\xx_1+\frac{-9}{3}\xx_2+\frac{-28}{7}\xx_3\\
\amp = 2(1,0,1,0)-3(-1,0,1,1)-4(1,1,-1,2)\\
\amp = (1,-4,3,-11) = \vv\text{,}
\end{align*}
so \(\vv\in\spn\{\xx_1,\xx_2,\xx_3\}\).%
\par
On the other hand, repeating the same calculation with \(\ww\), we find%
\begin{align*}
\left(\frac{\vv\dotp\xx_1}{\len{\xx_1}^2}\right)\xx_1
\amp +\left(\frac{\vv\dotp\xx_2}{\len{\xx_2}^2}\right)\xx_2
+\left(\frac{\vv\dotp\xx_3}{\len{\xx_3}^2}\right)\xx_3\\
\amp =\frac12 (1,0,1,0)-\frac53 (-1,0,1,1) +\frac47 (1,1,-1,2)\\
\amp = \left(\frac{73}{42},\frac47,-\frac{115}{42},-\frac{11}{21}\right)\neq \ww\text{,}
\end{align*}
so \(\ww\notin\spn\{\xx_1,\xx_2,\xx_3\}\).%
\par
Soon, we'll see that the quantity we computed when showing that \(\ww\notin\spn\{\xx_1,\xx_2,\xx_3\}\) is, in fact, the \emph{orthogonal projection} of \(\ww\) onto the subspace \(\spn\{\xx_1,\xx_2,\xx_3\}\).%
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}3.2\space\textperiodcentered\space{}Orthogonal Projection\\
3.2.1\space\textperiodcentered\space{}The Gram-Schmidt Procedure}
\begin{inlinesolution}{3.2.3}{}{g:exercise:idp83}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp169-back}{}\quad{}First, note that we can actually jump right into the Gram-Schmidt procedure. If the set \(B\) is not a basis, then it won't be independent, and when we attempt to construct the third vector in our orthonormal basis, its projection on the the subspace spanned by the first two will be the same as the original vector, and we'll get zero when we subtract the two.%
\par
We let \(\xx_1=(1,-2,1), \xx_2=(3,0,-2), \xx_3=(-1,1,2)\), and set \(\vv_1=\xx_1\). Then we have%
\begin{align*}
\vv_2 \amp = \xx_2-\left(\frac{\xx_2\dotp \vv_1}{\len{\vv_1}^2}\right)\vv_1 \\
\amp = (3,0,-2)-\frac{1}{6}(1,-2,1)\\
\amp = \frac16(17,2,-3) \text{.}
\end{align*}
%
\par
Next, we compute \(\vv_3\).%
\begin{align*}
\vv_3 \amp = \xx_3-\left(\frac{\xx_3\dotp \vv_1}{\len{\vv_1}^2}\right)\vv_1 - \left(\frac{\xx_3\dotp \vv_2}{\len{\vv_2}^2}\right)\vv_2\\
\amp = (-1,1,2)-\frac{-1}{6}(1,-2,1)-\cdot \frac{-21}{303}(17,2,-3)\\
\amp = (-1,1,2)+\frac16(1,-2,1)+\frac{7}{101}(17,2,-3)\\
\amp = \frac{1}{606}\bigl((-606,606,1212)+(101,-202,101)+(782,84,-126)\bigr)\\
\amp = \frac{1}{606}(277,488,1187)\text{.}
\end{align*}
%
\par
OK. Now, given the frequency with which typos occur in this text, and the fact that I tried to do the above problem in my head while typing (with an occasional calculator check), there's a good chance there's a mistake somewhere. Let's check our work.%
\begin{sageinput}
from sympy import Matrix, init_printing, GramSchmidt
init_printing()
L=(Matrix([1,-2,1]),Matrix([3,0,-2]),Matrix([-1,1,2]))
GramSchmidt(L)
\end{sageinput}
\begin{sageoutput}
\[\left[\bbm 1\\-2\\1\ebm, \bbm \frac{17}{6}\\ \frac13\\ -\frac{13}{6}\ebm, \bbm \frac{52}{77}\\ \frac{65}{77}\\ \frac{78}{77}\ebm\right]\]
\end{sageoutput}
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}3.2.2\space\textperiodcentered\space{}Projections}
\begin{inlinesolution}{3.2.9}{}{g:exercise:idp90}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp91-back}{}\quad{}Suppose we find vectors \(\mathbf{p}\) and \(\mathbf{p}'\) using basis \(B\) and \(B'\). Note that \(\mathbf{p}-\mathbf{p}'\in U\), but also that%
\begin{equation*}
\mathbf{p}-\mathbf{p}' = (\mathbf{p}-\xx)-(\mathbf{p}'-\xx)
\end{equation*}
Now use \hyperref[x:theorem:thm-projection]{Theorem~{\xreffont\ref{x:theorem:thm-projection}}}%
\end{inlinesolution}%
\begin{inlinesolution}{3.2.11}{}{g:exercise:idp92}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp170-back}{}\quad{}First, we note that for a general element of \(U\), we have%
\begin{equation*}
(a-b+3c, 2a+b, 3c, 4a-b+3c,a-4c) = a(1,2,0,4,1)+b(-1,1,0,-1,0)+c(3,0,3,3,-4)\text{.}
\end{equation*}
If \(\ww = (x_1,x_2,x_3,x_4,x_5)\in U^\bot\), then we must have%
\begin{align*}
\ww\dotp (1,2,0,4,1) \amp = x_1+2x_2+4x_4+x_5=0 \\
\ww\dotp (-1,1,0,-1,0) \amp =-x_1+x_2-x_4=0\\
\ww\dotp (3,0,3,3,-4) \amp =3x_1+3x_3+3x_4-4x_5 = 0\text{.}
\end{align*}
To find a basis for \(U^\bot\), we simply need to find the nullspace of the coefficient matrix for this system, which we do below.%
\begin{sageinput}
from sympy import Matrix, init_printing
init_printing()
A = Matrix(3,5,[1,2,0,4,1,-1,1,0,-1,0,3,0,3,3,-4])
A.nullspace()
\end{sageinput}
\begin{sageoutput}
\[\left[\bbm -2\\-1\\1\\1\\0\ebm, \bbm -\frac13\\ -\frac13\\ \frac53\\0\\1\ebm\right]\]
\end{sageoutput}
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}4\space\textperiodcentered\space{}Diagonalization\\
4.2\space\textperiodcentered\space{}Diagonalization of symmetric matrices}
\begin{inlinesolution}{4.2.1}{}{g:exercise:idp102}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp171-back}{}\quad{}Take \(\xx=\mathbf{e}_i\) and \(\yy=\mathbf{e}_j\), where \(\{\mathbf{e}_1,\ldots, \mathbf{e}_n\}\) is the standard basis for \(\R^n\). Then with \(A = [a_{ij}]\) we have%
\begin{equation*}
a_{ij} =\mathbf{e}_i\dotp(A\mathbf{e}_j) = (A\mathbf{e}_i)\dotp \mathbf{e}_j = a_{ji}\text{,}
\end{equation*}
which shows that \(A^T=A\).%
\end{inlinesolution}%
\begin{inlinesolution}{4.2.7}{}{g:exercise:idp104}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp172-back}{}\quad{}We'll solve this problem with the help of the computer.%
\begin{sageinput}
from sympy import Matrix,init_printing,factor
init_printing()
A = Matrix(3,3,[5,-2,-4,-2,8,-2,-4,-2,5])
p=A.charpoly().as_expr()
factor(p)
\end{sageinput}
\begin{sageoutput}
\[\lambda(\lambda-9)^2\]
\end{sageoutput}
We get \(c_A(x)=x(x-9)^2\), so our eigenvalues are \(0\) and \(9\). For \(0\) we have \(E_0(A) = \nll(A)\):%
\begin{sageinput}
A.nullspace()
\end{sageinput}
\begin{sageoutput}
\[\bbm 1\\ \frac12\\1\ebm\]
\end{sageoutput}
For \(9\) we have \(E_9(A) = \nll(A-9I)\).%
\begin{sageinput}
from sympy import eye
B=A-9*eye(3)
B.nullspace()
\end{sageinput}
\begin{sageoutput}
\[\left[\bbm -\frac12\\1\\0\ebm, \bbm -1\\0\\1\ebm\right]\]
\end{sageoutput}
The approach above is useful as we're trying to remind ourselves how eigenvalues and eigenvectors are defined and computed. Eventually we might want to be more efficient. Fortunately, there's a command for that.%
\begin{sageinput}
A.eigenvects()
\end{sageinput}
\begin{sageoutput}
\[\left[\left(0,1,\bbm 1\\ \frac12\\1\ebm\right), \left(9,2,\left[\bbm -\frac12\\1\\0\ebm,\bbm -1\\0\\1\ebm\right]\right)\right]\]
\end{sageoutput}
Note that the output above lists each eigenvalue, followed by its multiplicity, and then the associated eigenvectors.%
\par
This gives us a basis for \(\R^3\) consisting of eigenvalues of \(A\), but we want an orthogonal basis. Note that the eigenvector corresponding to \(\lambda = 0\) is orthogonal to both of the eigenvectors corresponding to \(\lambda =9\). But these eigenvectors are not orthogonal to each other. To get an orthogonal basis for \(E_9(A)\), we apply the Gram-Schmidt algorithm.%
\begin{sageinput}
from sympy import GramSchmidt
L=B.nullspace()
GramSchmidt(L)
\end{sageinput}
\begin{sageoutput}
\[\left[\bbm -\frac12\\1\\0\ebm, \bbm -\frac45\\-\frac25\\1\ebm\right]\]
\end{sageoutput}
This gives us an orthogonal basis of eigenvectors. Scaling to clear fractions, we have%
\begin{equation*}
\left\{\bbm 2\\1\\2\ebm, \bbm -1\\2\\0\ebm, \bbm -4\\-2\\5\ebm\right\}
\end{equation*}
From here, we need to normalize each vector to get the matrix \(P\). But we might not like that the last vector has norm \(\sqrt{45}\). One option to consider is to apply Gram-Schmidt with the vectors in the other order.%
\begin{sageinput}
L=[Matrix(3,1,[-1,0,1]),Matrix(3,1,[-1,2,0])]
GramSchmidt(L)
\end{sageinput}
\begin{sageoutput}
\[\left[\bbm -1\\0\\1\ebm, \bbm -\frac12\\2\\-\frac12\ebm\right]\]
\end{sageoutput}
That gives us the (slightly nicer) basis%
\begin{equation*}
\left\{\bbm 2\\1\\2\ebm, \bbm -1\\0\\1\ebm, \bbm 1\\-4\\1\ebm\right\}\text{.}
\end{equation*}
The corresponding orthonormal basis is%
\begin{equation*}
B = \left\{\frac{1}{3}\bbm 2\\1\\2\ebm, \frac{1}{\sqrt{2}}\bbm -1\\0\\1\ebm, \frac{1}{\sqrt{18}}\bbm 1\\-4\\1\ebm\right\}\text{.}
\end{equation*}
This gives us the matrix \(P=\bbm 2/3\amp -1/\sqrt{2}\amp 1/\sqrt{18}\\1/3\amp 0 \amp -4/\sqrt{18}\\2/3\amp 1/\sqrt{2}\amp 1/\sqrt{18}\ebm\). Let's confirm that \(P\) is orthogonal.%
\begin{sageinput}
P=Matrix(3,3,[2/3, -1/sqrt(2),1/sqrt(18), 1/3,0,-4/sqrt(18),2/3,1/sqrt(2),1/sqrt(18)])
P,P*P.transpose()
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm \frac23 \amp -\frac{\sqrt{2}}{2}\amp \frac{\sqrt{2}}{6}\\ \frac13\amp 0\amp -\frac{2\sqrt{2}}{3}\\ \frac23 \amp \frac{\sqrt{2}}{2} \amp \frac{\sqrt{2}}{6}\ebm,
\bbm 1\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp 1\ebm\right)\]
\end{sageoutput}
Since \(PP^T=I_3\), we can conclude that \(P^T=P^{-1}\), so \(P\) is orthogonal, as required. Finally, we diagonalize \(A\).%
\begin{sageinput}
Q=P.transpose()
Q*A*P
\end{sageinput}
\begin{sageoutput}
\[\bbm 0\amp 0\amp 0\\0\amp 9\amp 0\\0\amp 0\amp 9\ebm\]
\end{sageoutput}
Incidentally, the SymPy library for Python does have a diagaonalization routine; however, it does not do orthogonal diagonalization by default. Here is what it provides for our matrix \(A\).%
\begin{sageinput}
A.diagonalize()
\end{sageinput}
\begin{sageoutput}
\[\left(\bbm 2\amp -1\amp -1\\1\amp 2\amp 0\\2\amp 0\amp 1\ebm, \bbm 0\amp 0\amp 0\\0\amp 9\amp 0\\0\amp 0\amp 9\ebm\right)\]
\end{sageoutput}
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}4.4\space\textperiodcentered\space{}Diagonalization of complex matrices\\
4.4.3\space\textperiodcentered\space{}Complex matrices}
\begin{inlinesolution}{4.4.7}{}{g:exercise:idp107}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp173-back}{}\quad{}We have \(\bar{A}=\bbm 4\amp 1+i\amp -2-3i\\1-i\amp 5 \amp -7i\\-2+3i\amp 7i\amp -4\ebm\), so%
\begin{equation*}
A^H = (\bar{A})^T = \bbm 4\amp 1-i\amp -2+3i\\1+i\amp 5\amp 7i\\-2-3i\amp -7i\amp -4\ebm = A\text{,}
\end{equation*}
and%
\begin{align*}
BB^H \amp =\frac14\bbm 1+i\amp \sqrt{2}\\1-i\amp\sqrt{2}i\ebm\bbm 1-i\amp 1+i\\\sqrt{2}\amp-\sqrt{2}i\ebm \\
\amp =\frac14\bbm (1+i)(1-i)+2\amp (1+i)(1+i)-2i\\(1-i)(1-i)+2i\amp (1-i)(1+i)+2\ebm\\
\amp =\frac14\bbm 4\amp 0\\0\amp 4\ebm = \bbm 1\amp 0\\0\amp 1\ebm\text{,}
\end{align*}
so that \(B^H = B^{-1}\).%
\end{inlinesolution}%
\begin{inlinesolution}{4.4.11}{}{g:exercise:idp110}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp174-back}{}\quad{}Confirming that \(A^H=A\) is almost immediate. We will use the computer below to compute the eigenvalues and eigenvectors of \(A\), but it's useful to attempt this at least once by hand. We have%
\begin{align*}
\det(zI-A) \amp = \det\bbm z-4 \amp -3+i\\-3-i\amp z-1\ebm\\
\amp (z-4)(z-1)-(-3-i)(-3+i)\\
\amp z^2-5z+4-10\\
\amp (z+1)(z-6)\text{,}
\end{align*}
so the eigenvalues are \(\lambda_1=-1\) and \(\lambda_2=6\), which are both real, as expected.%
\par
Finding eigenvectors can seem trickier than with real numbers, mostly because it is no longer immediately apparent when one row or a matrix is a multiple of another. But we know that the rows of \(A-\lambda I\) must be parallel for a \(2\times 2\) matrix, which lets proceed nonetheless.%
\par
For \(\lambda_1=-1\), we have%
\begin{equation*}
A + I =\bbm 5 \amp 3-i\\3+i\amp 2\ebm\text{.}
\end{equation*}
There are two ways one can proceed from here. We could use row operations to get to the reduced row-echelon form of \(A\). If we take this approach, we multiply row 1 by \(\frac15\), and then take \(-3-i\) times the new row 1 and add it to row 2, to create a zero, and so on.%
\par
Easier is to realize that if we haven't made a mistake calculating our eigenvalues, then the above matrix can't be invertible, so there must be some nonzero vector in the kernel. If \((A+I)\bbm a\\b\ebm=\bbm0\\0\ebm\), then we must have%
\begin{equation*}
5a+(3-i)b=0\text{,}
\end{equation*}
when we multiply by the first row of \(A\). This suggests that we take \(a=3-i\) and \(b=-5\), to get \(\zz = \bbm 3-i\\-5\ebm\) as our first eigenvector. To make sure we've done things correctly, we multiply by the second row of \(A+I\):%
\begin{equation*}
(3+i)(3-i)+2(-5) = 10-10 = 0\text{.}
\end{equation*}
Success! Now we move onto the second eigenvalue.%
\par
For \(\lambda_2=6\), we get%
\begin{equation*}
A-6I = \bbm -2\amp 3-i\\3+i\amp -5\ebm\text{.}
\end{equation*}
If we attempt to read off the answer like last time, the first row of \(A-6I\) suggests the vector \(\ww = \bbm 3-i\\2\ebm\). Checking the second row to confirm, we find:%
\begin{equation*}
(3+i)(3-i)-5(2) = 10-10=0\text{,}
\end{equation*}
as before.%
\par
Finally, we note that%
\begin{equation*}
\langle \zz, \ww\rangle = (3-i)\overline{(3-i)}+(-5)(2) = (3-i)(3+i)-10 = 0\text{,}
\end{equation*}
so the two eigenvectors are orthogonal, as expected. We have%
\begin{equation*}
\len{\zz}=\sqrt{10+25}=\sqrt{35} \quad \text{ and } \quad \len{\ww}=\sqrt{10+4}=\sqrt{14}\text{,}
\end{equation*}
so our orthogonal matrix is%
\begin{equation*}
U = \bbm \frac{3-i}{\sqrt{35}}\amp \frac{3-i}{\sqrt{14}}\\-\frac{5}{\sqrt{35}}\amp \frac{2}{\sqrt{14}}\ebm\text{.}
\end{equation*}
With a bit of effort, we can finally confirm that%
\begin{equation*}
U^HAU = \bbm -1\amp 0\\0\amp 6\ebm\text{,}
\end{equation*}
as expected.%
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}5\space\textperiodcentered\space{}Change of Basis\\
5.1\space\textperiodcentered\space{}The matrix of a linear transformation}
\begin{inlinesolution}{5.1.1}{}{g:exercise:idp123}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp175-back}{}\quad{}It's clear that \(C_B(\mathbf{0})=\mathbf{0}\), since the only way to write the zero vector in \(V\) in terms of \(B\) (or, indeed, any independent set) is to set all the scalars equal to zero.%
\par
If we have two vectors \(\vv,\ww\) given by%
\begin{align*}
\vv \amp = a_1\mathbf{e}_1+\cdots + a_n\mathbf{e}_n \\
\ww \amp = b_1\mathbf{e}_1+\cdots + b_n\mathbf{e}_n\text{,}
\end{align*}
then%
\begin{equation*}
\vv+\ww = (a_1+b_1)\mathbf{e}_1+\cdots + (a_n+b_n)\mathbf{e}_n\text{,}
\end{equation*}
so%
\begin{align*}
C_B(\vv+\ww) \amp = \bbm a_1+b_1\\\vdots \\ a_n+b_n\ebm \\
\amp = \bbm a_1\\\vdots\\a_n\ebm +\bbm b_1\\\vdots \\b_n\ebm\\
\amp = C_B(\vv)+C_B(\ww)\text{.}
\end{align*}
%
\par
Finally, for any scalar \(c\), we have%
\begin{align*}
C_B(c\vv) \amp = C_B((ca_1)\mathbf{e}_1+\cdots +(ca_n)\mathbf{e}_n)\\
\amp = \bbm ca_1\\\vdots \\ca_n\ebm\\
\amp =c\bbm a_1\\\vdots \\a_n\ebm\\
\amp =cC_B(\vv)\text{.}
\end{align*}
%
\par
This shows that \(C_B\) is linear. To see that \(C_B\) is an isomorphism, we can simply note that \(C_B\) takes the basis \(B\) to the standard basis of \(\R^n\). Alternatively, we can give the inverse: \(C_B^{-1}:\R^n\to V\) is given by%
\begin{equation*}
C_B^{-1}\bbm c_1\\\vdots \\c_n\ebm = c_1\mathbf{e}_1+\cdots +c_n\mathbf{e}_n\text{.}
\end{equation*}
%
\end{inlinesolution}%
\begin{inlinesolution}{5.1.3}{}{g:exercise:idp124}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp176-back}{}\quad{}We have%
\begin{align*}
T(1) \amp = (1,0) = 1(1,0)+0(1,-1) \\
T(1-x) \amp= (1,-2) = -1(1,0)+2(1,-1) \\
T((1-x)^2) = T(1-2+x^2) \amp = (2, -4) = -2(1,0)+4(1,-1)\text{.}
\end{align*}
Thus,%
\begin{align*}
M_{DB}(T) \amp = \bbm C_D(T(1))\amp C_D(T(1-x)) \amp C_D(T((1-x)^2))\ebm\\
\amp = \bbm 1\amp -1\amp -2\\0\amp 2\amp 4\ebm\text{.}
\end{align*}
To confirm, note that%
\begin{align*}
M_{DB}(T)C_B\amp (a+bx+cx^2)\\
\amp = M_{DB}(T)C_B((a+b+c)-(b+2c)(1-x)+c(1-x)^2)\\
\amp = \bbm 1\amp -1\amp -2\\0\amp 2\amp 4\ebm\bbm a+b+c\\ -b-2c\\c\ebm\\
\amp \bbm (a+b+c)+(b+2c)-2c\\0-2(b+2c)+4c\ebm = \bbm a+2b+c\\-2b\ebm\text{,}
\end{align*}
while on the other hand,%
\begin{align*}
C_D(T(a+bx+cx^2)) \amp= C_D(a+c,2b)\\
\amp = C_D((a+2b+c)(1,0)-2b(1,-1))\\
\amp = \bbm a+2b+c\\-2b\ebm\text{.}
\end{align*}
%
\end{inlinesolution}%
\begin{inlinesolution}{5.1.7}{}{g:exercise:idp127}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp177-back}{}\quad{}We must first write our general input in terms of the given basis. With respect to the standard basis%
\begin{equation*}
B_0 = \left\{\bbm 1\amp 0\\0\amp 0\ebm, \bbm 0\amp 1\\0\amp 0\ebm, \bbm 0\amp 0\\1\amp 0\ebm, \bbm 0\amp 0\\0\amp 1\ebm\right\}\text{,}
\end{equation*}
we have the matrix \(P = \bbm 1\amp 0\amp 0\amp 1\\0\amp 1\amp 1\amp 0\\0\amp 0\amp 1\amp 0\\0\amp 1\amp 0\amp 1\ebm\), representing the change from the basis \(B\) the basis \(B_0\). The basis \(D\) of \(P_2(\R)\) is already the standard basis, so we need the matrix \(M_{DB}(T)P^{-1}\):%
\begin{sageinput}
from sympy import Matrix, init_printing
init_printing()
M = Matrix(3,4,[2,-1,0,3,0,4,-5,1,-1,0,3,-2])
P = Matrix(4,4,[1,0,0,1,0,1,1,0,0,0,1,0,0,1,0,1])
M*P**-1
\end{sageinput}
\begin{sageoutput}
\[\bbm 2\amp -2\amp 2\amp 1\\0\amp 3\amp -8\amp 1\\-1\amp 1\amp 2\amp -1\ebm\]
\end{sageoutput}
For a matrix \(X = \bbm a\amp b\\c\amp d\ebm\) we find%
\begin{equation*}
M_{DB}(T)P^{-1}C_{B_0}(X)=\bbm 2\amp -2\amp 2\amp 1\\0\amp 3\amp -8\amp 1\\-1\amp 1\amp 2\amp -1\ebm\bbm a\\b\\c\\d\ebm =
\bbm 2a-2b+2c+d\\3b-8c+d\\-a+b+2c-d\ebm\text{.}
\end{equation*}
But this is equal to \(C_D(T(X))\), so%
\begin{align*}
T\left(\bbm a\amp b\\c\amp d\ebm\right) \amp = C_D^{-1}\bbm 2a-2b+2c+d\\3b-8c+d\\-a+b+2c-d\ebm\\
\amp = (2a-2b+2c+d)+(3b-8c+d)x+(-a+b+2c-d)x^2\text{.}
\end{align*}
%
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}5.2\space\textperiodcentered\space{}The matrix of a linear operator}
\begin{inlinesolution}{5.2.4}{}{g:exercise:idp131}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp178-back}{}\quad{}With respect to the standard basis, we have%
\begin{equation*}
M_0=M_{B_0}(T) = \bbm 3\amp -2\amp 4\\1\amp -5\amp 0\\0\amp 2\amp -7\ebm\text{,}
\end{equation*}
and the matrix \(P\) is given by \(P = \bbm 1\amp 3\amp 1\\2\amp -1\amp 2\\0\amp 2\amp-5\ebm\). Thus, we find%
\begin{equation*}
M_B(T)=P^{-1}M_0P=\bbm 9\amp 56\amp 36\\7\amp 15\amp 15\\-10\amp -46\amp -33\ebm\text{.}
\end{equation*}
%
\end{inlinesolution}%
\par\medskip
\noindent\textbf{\Large{}5.6\space\textperiodcentered\space{}Jordan Canonical Form}
\begin{inlinesolution}{5.6.7}{}{x:exercise:ex-jordan-form2}%
\par\smallskip%
\noindent\textbf{\blocktitlefont Solution}.\hypertarget{g:solution:idp179-back}{}\quad{}With respect to the standard basis of \(\R^4\), the matrix of \(T\) is%
\begin{equation*}
M = \bbm 1\amp 1\amp 0\amp 0\\0\amp 1\amp 0\amp 0\\0\amp -1\amp 2\amp 0\\1\amp -1\amp 1\amp 1\ebm\text{.}
\end{equation*}
We find (perhaps using the Sage cell provided below, and the code from the example above) that%
\begin{equation*}
c_T(x)=(x-1)^3(x-2)\text{,}
\end{equation*}
so \(T\) has eigenvalues \(1\) (of multiplicity \(3\)), and \(2\) (of multiplicity \(1\)).%
\par
We tackle the repeated eigenvalue first. The reduced row-echelon form of \(M-I\) is given by%
\begin{equation*}
R_1 = \bbm 1\amp 0\amp 0\amp 0\\0\amp 1\amp 0\amp 0\\0\amp 0\amp 1\amp 0\\0\amp 0\amp 0\amp 0\ebm\text{,}
\end{equation*}
so%
\begin{equation*}
E_1(M) = \spn\{\xx_1\}, \text{ where } \xx_1 = \bbm 0\\0\\0\\1\ebm\text{.}
\end{equation*}
We now attempt to solve \((M-I)\xx=\xx_1\). We find%
\begin{equation*}
\left(\begin{matrix}0\amp 1\amp 0\amp 0\\0\amp 0\amp 0\amp 0\\0\amp -1\amp 1\amp 0\\1\amp -1\amp 1\amp 0\end{matrix}\right|\left.\begin{matrix}0\\0\\0\\1\end{matrix}\right)
\xrightarrow{\text{RREF}}
\left(\begin{matrix} 1\amp 0\amp 0\amp 0\\0\amp 1\amp 0\amp 0\\0\amp 0\amp 1\amp 0\\0\amp 0\amp 0\amp 0\end{matrix}\right|\left.\begin{matrix}1\\0\\0\\0\end{matrix}\right)\text{,}
\end{equation*}
so \(\xx = t\xx_1+\xx_2\), where \(\xx_2 = \bbm 1\\0\\0\\0\ebm\). We take \(\xx_2\) as our first generalized eigenvector. Note that \((M-I)^2\xx_2 = (M-I)\xx_1=\zer\), so \(\xx_2\in \nll (M-I)^2\), as expected.%
\par
Finally, we look for an element of \(\nll (M-I)^3\) of the form \(\xx_3\), where \((M-I)\xx_3=\xx_2\). We set up and solve the system \((M-I)\xx=\xx_2\) as follows:%
\begin{equation*}
\left(\begin{matrix}0\amp 1\amp 0\amp 0\\0\amp 0\amp 0\amp 0\\0\amp -1\amp 1\amp 0\\1\amp -1\amp 1\amp 0\end{matrix}\right|\left.\begin{matrix}1\\0\\0\\0\end{matrix}\right)
\xrightarrow{\text{RREF}}
\left(\begin{matrix} 1\amp 0\amp 0\amp 0\\0\amp 1\amp 0\amp 0\\0\amp 0\amp 1\amp 0\\0\amp 0\amp 0\amp 0\end{matrix}\right|\left.\begin{matrix}0\\1\\1\\0\end{matrix}\right)\text{,}
\end{equation*}
so \(\xx = t\xx_1+\xx_3\), where \(\xx_3 =\bbm 0\\1\\1\\0\ebm\).%
\par
Finally, we deal with the eigenvalue \(2\). The reduced row-echelon form of \(M-2I\) is%
\begin{equation*}
R_2 = \bbm 1\amp 0\amp 0\amp 0\\0\amp 1\amp 0\amp 0\\0\amp 0\amp 1\amp -1\\0\amp 0\amp 0\amp 0\ebm\text{,}
\end{equation*}
so%
\begin{equation*}
E_2(M) = \spn\{\yy\}, \text{ where } \yy = \bbm 0\\0\\1\\1\ebm\text{.}
\end{equation*}
%
\par
Our basis of column vectors is therefore \(B=\{\xx_1,\xx_2,\xx_3,\yy\}\). Note that by design,%
\begin{align*}
M\xx_1 \amp =\xx_1\\
M\xx_2 \amp =\xx_1+\xx_2\\
M\xx_3 \amp= \xx_2+\xx_3\\
M\yy \amp = 2\yy\text{.}
\end{align*}
The corresponding Jordan basis for \(\R^4\) is%
\begin{equation*}
\{(0,0,0,1),(1,0,0,0),(0,1,1,0),(0,0,1,1)\}\text{,}
\end{equation*}
and with respect to this basis, we have%
\begin{equation*}
M_B(T) = \bbm 1\amp 1\amp 0\amp 0\\
0\amp 1\amp 1\amp 0\\
0\amp 0\amp 1\amp 0\\
0\amp 0\amp 0\amp 2\ebm\text{.}
\end{equation*}
%
\end{inlinesolution}%
\end{solutions-chapter}
\end{document}