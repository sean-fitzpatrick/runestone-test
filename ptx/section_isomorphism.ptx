<?xml version="1.0" encoding="UTF-8" ?>

<section xml:id="sec-isomorphism">
  <title>Isomorphisms, composition, and inverses</title>
  <introduction>
    <p>
      We ended the last section with an important result.
      <xref ref="ex-dimension-injection-surjection">Exercise</xref>
      showed that existence of an injective linear map <m>T:V\to W</m>
      is equivalent to <m>\dim V\leq \dim W</m>,
      and that existence of a surjective linear map is equivalent to <m>\dim V\geq \dim W</m>.
      It's probably not surprising than that existence of a <em>bijective</em> linear map <m>T:V\to W</m>
      is equivalent to <m>\dim V = \dim W</m>.
    </p>

    <p>
      In a certain sense that we will now try to make preceise,
      vectors spaces of the same dimension are equivalent:
      they may look very different, but in fact, they contain exactly the same information,
      presented in different ways.
    </p>
  </introduction>

  <subsection xml:id="subsec-isomorphism">
    <title>Isomorphisms</title>

    <definition xml:id="def-isomorphism">
      <statement>
        <p>
          A bijective linear transformation <m>T:V\to W</m> is called an <term>isomorphism</term>.
          If such a map exists, we say that <m>V</m> and <m>W</m> are <term>isomorphic</term>, and write <m>V\cong W</m>.
        </p>
      </statement>
    </definition>

    <theorem xml:id="thm-iso-dimension">
      <statement>
        <p>
          For any finite-dimensional vector spaces <m>V</m> and <m>W</m>,
          <m>V\cong W</m> if any only if <m>\dim V = \dim W</m>.
        </p>
      </statement>
      <proof>
        <title>Strategy</title>
        <p>
          We again need to prove both directions of an <q>if and only if</q>.
          If an isomorphism exists, can you see how to use <xref ref="ex-dimension-injection-surjection"/>
          to show the dimensions are equal?
        </p>
        <p>
          If the dimensions are equal, you need to construct an isomorphism.
          Since <m>V</m> and <m>W</m> are finite-dimensional, you can choose a basis for each space.
          What can you say about the sizes of these bases?
          How can you use them to define a linear transformation?
          (You might want to remind yourself what <xref ref="thm-define-using-basis"/> says.)
        </p>
      </proof>

      <proof>
        <p>
          If <m>T:V\to W</m> is a bijection, then it is both injective and surjective.
          Since <m>T</m> is injective, <m>\dim V\leq \dim W</m>,
          by <xref ref="ex-dimension-injection-surjection">Exercise</xref>.
          By this same exercise,  since <m>T</m> is surjective, we must have <m>\dim V\geq \dim W</m>.
          It follows that <m>\dim V=\dim W</m>.
        </p>

        <p>
          Suppose now that <m>\dim V =\dim W</m>.
          Then we can choose bases <m>\{\vv_1,\ldots, \vv_n\}</m> of <m>V</m>,
          and <m>\{\ww_1,\ldots, \ww_n\}</m> of <m>W</m>.
          <xref ref="thm-define-using-basis"/> then guarantees the existence of a linear map <m>T:V\to W</m>
          such that <m>T(\vv_i)=\ww_i</m> for each <m>i=1,2,\ldots, n</m>.
          Repeating the arguments of <xref ref="ex-dimension-injection-surjection"/> shows that <m>T</m> is a bijection.
        </p>
      </proof>

    </theorem>

    <p>
      Buried in the theorem above is the following useful fact:
      <em>an isomorphism <m>T:V\to W</m> takes any basis of <m>V</m> to a basis of <m>W</m></em>.
      Another remarkable result of the above theorem is that <em>any two vector spaces of the same dimension are isomorphic</em>!
      In particular, we have the following theorem.
    </p>

    <theorem xml:id="thm-iso-rn">
      <statement>
        <p>
          If <m>\dim V=n</m>, then <m>V\cong \R^n</m>.
        </p>
      </statement>
    </theorem>

    <exercise>
      <statement>
        <p>
          Match each vector space on the left with an isomorphic vector space on the right.
        </p>
      </statement>
      <matches randomize="yes">
        <match order="2">
          <premise><m>P_3(\R)</m></premise>
          <response><m>\R^4</m></response>
        </match>
        <match order="1">
          <premise><m>M_{2\times 3}(\R)</m></premise>
          <response><m>\R^6</m></response>
        </match>
        <match order="4">
          <premise><m>P_4(\R)</m></premise>
          <response><m>\R^5</m></response>
        </match>
        <match order="3">
          <premise><m>M_{2\times 2}(\R)</m></premise>
          <response><m>R^4</m></response>
        </match>
      </matches>
    </exercise>

    <p>
      <xref ref="thm-iso-rn"/> is a direct consequence of <xref ref="thm-iso-dimension"/>.
      But it's useful to understand how it works in practice.
      Note that in the definition below, we use the term <term>ordered basis</term>.
      This just means that we fix the order in which the vectors in our basis are written.
    </p>

    <definition xml:id="def-coefficient-iso">
      <statement>
        <p>
          Let <m>V</m> be a finite-dimensional vector space,
          and let <m>B=\{\mathbf{e}_1,\ldots, \mathbf{e}_n\}</m> be an ordered basis for <m>V</m>.
          The <term>coefficient isomorphism</term> associated to <m>B</m>
          is the map <m>C_B:V\to \R^n</m> defined by
          <me>
            C_B(c_1\mathbf{e}_1+c_2\mathbf{e}_2+\cdots +c_n\mathbf{e}_n)=\bbm c_1\\c_2\\\vdots \\c_n\ebm
          </me>.

        </p>
      </statement>
    </definition>

    <p>
      Note that this is a well-defined map since every vector in <m>V</m> can be written uniquely in terms of the basis <m>B</m>.
      But also note that the ordering of the vectors in <m>B</m> is important:
      changing the order changes the position of the coefficients in <m>C_B(\vv)</m>.
    </p>

    <p>
      The coefficient isomorphism is especially useful when we want to analyze a linear map computationally.
      Suppose we're given <m>T:V\to W</m> where <m>V, W</m> are finite-dimensional.
      Let us choose bases <m>B=\{\vv_1,\ldots, \vv_n\}</m> of <m>V</m>
      and <m>B' = \{\ww_1,\ldots, \ww_m\}</m> of <m>W</m>.
      The choice of these two bases determines scalars <m>a_{ij}, 1\leq i\leq n, 1\leq j\leq m</m>, such that
      <me>
        T(\vv_j) = a_{1j}\ww_1+a_{2j}\ww_2+\cdots + a_{mj}\ww_j,
      </me>
      for each <m>i=1,2,\ldots, n</m>. The resulting matrix <m>A=[a_{ij}]</m>
      defines a matrix transformation <m>T_A:\R^n\to \R^m</m> such that
      <me>
        T_A\circ C_B = C_{B'}\circ T
      </me>.
      The relationship among the four maps used here is best captured by the <q>commutative diagram</q>
      in <xref ref="fig_transformation_matrix"/>.
    </p>

    <figure xml:id="fig_transformation_matrix">
      <caption>Defining the matrix of a linear map with respect to choices of basis.</caption>
      <image xml:id="img_trans_matrix" width="35%">
        <latex-image>
          \begin{tikzpicture}
          \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
          {
          V \amp W \\
          \R^n \amp \R^m \\};
          \path[-stealth]
          (m-1-1) edge node [left] {$C_B$} (m-2-1)
                  edge node [above] {$T$} (m-1-2)
          (m-2-1) edge node [above] {$T_A$} (m-2-2)
          (m-1-2) edge node [right] {$C_{B'}$} (m-2-2);
          \end{tikzpicture}
        </latex-image>
      </image>
    </figure>

    <p>
      The matrix of a linear transformation is studied in more detail in <xref ref="sec-matrix-of-transformation"/>.
    </p>
  </subsection>

  <subsection xml:id="subsec-composition">
    <title>Composition and inverses</title>
    <p>
      Recall that for any function <m>f:A\to B</m>,
      if <m>f</m> is a bijection, then it has an <term>inverse</term>:
      a function <m>f^{-1}:B\to A</m> that <q>undoes</q> the action of <m>f</m>.
      That is, if <m>f(a)=b</m>, then <m>f^{-1}(b)=a</m>,
      or in other words, <m>f^{-1}(f(a))=a</m> <mdash/> the composition <m>f^{-1}\circ f</m>
      is equal to the identity function on <m>A</m>.
    </p>

    <p>
      The same is true for composition in the other order: <m>f\circ f^{-1}</m>
      is the identity function on <m>B</m>.
      One way of interpreting this is to observe that just as <m>f^{-1}</m> is the inverse of <m>f</m>,
      so is <m>f</m> the inverse of <m>f^{-1}</m>; that is, <m>(f^{-1})^{-1}=f</m>.
    </p>

    <p>
      Since linear transformations are a special type of function,
      the above is true for a linear transformation as well.
      But if we want to keep everything under the umbrella of linear algebra,
      there are two things we should check: that the composition of two linear transformations is another linear transformation,
      and that the inverse of a linear transformation is a linear transformation.
    </p>

    <exercise>
      <statement>
        <p>
          Show that the composition of two linear maps is again a linear map.
        </p>
      </statement>
      <solution>
        <p>
          Suppose we have linear maps <m>U\xrightarrow{T} V\xrightarrow{S} W</m>,
          and let <m>\uu_1,\uu_2\in U</m>. Then
          <md>
            <mrow>ST(\uu_1+\uu_2) \amp = S(T(\uu_1+\uu_2)) </mrow>
            <mrow> \amp = S(T(\uu_1)+T(\uu_2))</mrow>
            <mrow>  \amp = S(T(\uu_1))+S(T(\uu_2)) </mrow>
            <mrow>  \amp = ST(\uu_1)+ST(\uu_2)</mrow>
          </md>,
          and for any scalar <m>c</m>,
          <me>
            ST(c\uu_1) = S(T(c\uu_1))=S(cT(\uu_1)) = cS(T(\uu_1))=c(ST(\uu_1))
          </me>.

        </p>
      </solution>
    </exercise>

    <exercise>
      <statement>
        <p>
          Given transformations <m>S:V\to W</m> and <m>T:U\to V</m>, show that:
          <ol>
            <li>
              <p>
                <m>\ker T\subseteq \ker ST</m>
              </p>
            </li>
            <li>
              <p>
                <m>\im ST\subseteq \im S</m>
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <hint>
        <p>
          This is simpler than it looks!
          It's mostly a matter of chasing the definitions: see <xref ref="rem-using-ker-im"/>.
        </p>
      </hint>
    </exercise>

    <exercise>
      <statement>
        <p>
          Let <m>T:V\to W</m> be a bijective linear transformation.
          Show that <m>T^{-1}:W\to V</m> is a linear transformation.
        </p>
      </statement>
      <hint>
        <p>
          Since <m>T</m> is a bijection, every <m>\ww\in W</m> can be associated with some <m>\vv\in V</m>.
        </p>
      </hint>
      <solution>
        <p>
          Let <m>\ww_1,\ww_2\in W</m>.
          Then there exist <m>\vv_1,\vv_2\in V</m>  with <m>\ww_1=T(\vv_1), \ww_2=T(\vv_2)</m>.
          We then have
          <md>
            <mrow>T^{-1}(\ww_1+\ww_2) \amp = T^{-1}(T(\vv_1)+T(\vv_2)) </mrow>
            <mrow> \amp = T^{-1}(T(\vv_1+\vv_2))</mrow>
            <mrow>  \amp = \vv_1+\vv_2</mrow>
            <mrow>  \amp = T^{-1}(\ww_1)+T^{-1}(\ww_2)</mrow>
          </md>.
          For any scalar <m>c</m>, we similarly have
          <me>
            T^{-1}(c\ww_1) = T^{-1}(cT(\vv_1))=T^{-1}(T(c\vv_1)) = c\vv_1 = cT^{-1}(\ww_1)
          </me>.
        </p>
      </solution>
    </exercise>

    <remark xml:id="rem-composition-product">
      <statement>
        <p>
          With this connection between linear maps (in general) and matrices,
          it can be worthwhile to pause and consider invertibility in the context of matrices.
          Recall that an <m>n\times n</m> matrix <m>A</m> is <em>invertible</em> if there exists a matrix <m>A^{-1}</m>
          such that <m>AA^{-1}=I_n</m> and <m>A^{-1}A=I_n</m>.
        </p>

        <p>
          The same definition can be made for linear maps.
          We've defined what it means for a map <m>T:V\to W</m> to be invertible as a <em>function</em>.
          In particular, we relied on the fact that any bijection has an inverse.
        </p>

        <p>
          Let <m>A</m> be an <m>m\times n</m>
          matrix, and let <m>B</m> be an <m>n\times k</m> matrix. Then we have linear maps
          <me>
            \R^k \xrightarrow{T_B} \R^n\xrightarrow{T_A} \R^m
          </me>,
          and the composition <m>T_A\circ T_B:\R^k\to \R^m</m> satisfies
          <me>
            T_A\circ T_B(\xx) = T_A(T_B(\xx)) = T_A(B\xx)=A(B\xx)=(AB)\xx=T_{AB}(\xx)
          </me>.
          Note that the rules given in elementary linear algebra, for the relative sizes of matrices that can be multiplied,
          are simply a manifestation of the fact that to compose functions,
          the range of the first must be contained in the domain of the second.
        </p>
      </statement>
    </remark>

    <aside>
      <p>
        A note on notation. Given linear maps <m>U\xrightarrow{T} V\xrightarrow{S} W</m>,
        we typically write the composition <m>S\circ T:U\to W</m> as a <q>product</q> <m>ST</m>.
        The reason for this is again to mimic the case of matrices:
        as seen in <xref ref="rem-composition-product"/>, <m>T_A\circ T_B = T_{AB}</m>
        for matrix transformations.
      </p>
    </aside>

    <exercise xml:id="ex-inverse-identity">
      <statement>
        <p>
          Show that if <m>ST=1_V</m>, then <m>S</m> is surjective and <m>T</m> is injective.
          Conclude that if <m>ST=1_V</m> and <m>TS=1_w</m>, then <m>S</m> and <m>T</m> are both bijections.
        </p>
      </statement>
      <hint>
        <p>
          This is true even if the functions aren't linear.
          In fact, you've probably seen the proof in an earlier course!
        </p>
      </hint>
    </exercise>

    <p>
      <xref ref="thm-iso-dimension"/> also tells us why we can only consider invertibility for square matrices:
      we know that invertible linear maps are only defined between spaces of equal dimension.
      In analogy with matrices, some texts will define a linear map <m>T:V\to W</m>
      to be invertible if there exists a linear map <m>S:W\to V</m> such that
      <me>
        ST = 1_V \quad \text{ and } \quad TS = 1_W
      </me>.
      By <xref ref="ex-inverse-identity"/>, this implies that <m>S</m> and <m>T</m> are bijections,
      and therefore <m>S</m> and <m>T</m> are invertible, with <m>S=T^{-1}</m>.
    </p>

    <p>
      We end this section with a discussion of inverses and composition.
      If we have isomorphisms <m>S:V\to W</m> and <m>T:U\to V</m>,
      what can we say about the composition <m>ST</m>?
    </p>

    <exercise>
      <statement correct="no">
        <p>
          The inverse of the composition <m>ST</m> is <m>S^{-1}T^{-1}</m>.
        </p>
      </statement>
      <feedback>
        <p>
          The composition of <m>ST</m> and its inverse should be the identity.
          Is that the case here? (Remember that order of composition matters!)
        </p>
      </feedback>
    </exercise>

    <p>
      We know that the composition of two linear transformations is a linear transformation,
      and that the composition of two bijections is a bijection.
      It follows that the composition of two isomorphisms is an isomorphism!
    </p>

    <p>
      With this observation, one can show that the relation of isomorphism is an equivalence relation.
      Two finite-dimensional vector spaces belong to the same equivalence class if and only if they have the same dimension.
      Here, we see again the importance of dimension in linear algebra.
    </p>

    <remark>
      <p>
        If you got that last exercise incorrect, consider the following:
        given <m>S:V\to W</m> and <m>T:U\to V</m>, we have <m>ST:U\to W</m>.
        Since <m>ST</m> is an isomorphism, it has an inverse, which goes from <m>W</m> to <m>U</m>.
        This inverse can be expressed in terms of the inverses of <m>S</m> and <m>T</m>,
        but we're going backwards, so we have to apply them in the opposite order!
        <md>
          <mrow>U \amp \xrightarrow{T} V\xrightarrow{S} W \amp \text{ defines } \amp ST:U\to W</mrow>
          <mrow>U \amp \xleftarrow{T^{-1}} V \xleftarrow{S^{-1}} W \amp \text{ defines } \amp (ST)^{-1}:W\to U</mrow>
        </md>
      </p>
    </remark>
  </subsection>
</section>
