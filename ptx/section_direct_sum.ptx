<?xml version="1.0" encoding="UTF-8" ?>

<section xml:id="sec-direct-sum">
  <title>Direct Sums and Invariant Subspaces</title>
  <introduction>
    <p>
      Much of this section has been mentioned previously in the course (and these notes),
      but we will follow the organization of Nicholson's textbook,
      and reprise these concepts in more detail than previously.
    </p>
  </introduction>

  <subsection xml:id="subsec-invariant">
    <title>Invariant subspaces</title>
    <definition xml:id="def-invariant-subspace">
      <statement>
        <p>
          Given an operator <m>T:V\to V</m>, we say that a subspace <m>U\subseteq V</m>
          is <m>T</m>-<term>invariant</term> if <m>T(\uu)\in U</m> for all <m>\uu\in U</m>.
        </p>
      </statement>
    </definition>

    <p>
      In other words, a subspace <m>U</m> is <m>T</m>-invariant if <m>T</m> does not map any vectors in <m>U</m> outside of <m>U</m>.
      Notice that if we shrink the domain of <m>T</m> to <m>U</m>, then we get an operator from <m>U</m> to <m>U</m>,
      since the image <m>T(U)</m> is contained in <m>U</m>.
    </p>

    <p>
      Given a basis <m>B=\basis{u}{k}</m> of <m>U</m>,
      note that <m>U</m> is <m>T</m>-invariant if and only if <m>T(\uu_i)\in U</m> for each <m>i=1,2,\ldots, k</m>.
    </p>

    <p>
      For any operator <m>T:V\to V</m>, there are four subspaces that are always <m>T</m>-invariant:
      <me>
        \{\zer\}, V, \ker T, \text{ and } \im T
      </me>.
      Of course, some of these subspaces might be the same; for example,
      if <m>T</m> is invertible, then <m>\ker T = \{\zer\}</m> and <m>\im T = V</m>.
      (We will skip the proof that <m>\ker T</m> and <m>\im T</m> are <m>T</m>-invariant,
      since this has been assigned as homework!)
    </p>

    <definition xml:id="def-restriction">
      <statement>
        <p>
          Let <m>T:V\to V</m> be a linear operator, and let <m>U</m> be a <m>T</m>-invariant subspace.
          The <term>restriction</term> of <m>T</m> to <m>U</m>, denoted <m>T|_U</m>,
          is the operator <m>T|_U:U\to U</m> defined by <m>T|_U(\uu)=T(\uu)</m> for all <m>\uu\in U</m>.
        </p>
      </statement>
    </definition>

    <p>
      Notice that the restriction <m>T|_U</m> is defined by the same <q>rule</q> as <m>T</m>,
      but its domain is the subspace <m>U</m> instead of the entire vector space <m>V</m>.
    </p>

    <p>
      A lot can be learned by studying the restrictions of an operator to invariant subspaces.
      Indeed, the textbook by Axler does almost everything from this point of view.
      One reason to study invariant subspaces is that they allow us to put the matrix of <m>T</m> into simpler forms.
    </p>

    <theorem xml:id="thm-invariant-block-triangular">
      <statement>
        <p>
          Let <m>T:V\to V</m> be a linear operator, and let <m>U</m> be a <m>T</m>-invariant subspace.
          Let <m>B_U = \basis{u}{k}</m> be a basis of <m>U</m>, and extend this to a basis
          <me>
            B = \{\uu_1,\ldots, \uu_k,\ww_1,\ldots, \ww_{n-k}\}
          </me>
          of <m>V</m>. Then the matrix <m>M_B(T)</m> with respect to this basis has the block-triangular form
          <me>
            M_B(T) = \bbm M_{B_U}(T_U) \amp P\\0 \amp Q\ebm
          </me>
          for some <m>(n-k)\times (n-k)</m> matrix <m>Z</m>.
        </p>
      </statement>
    </theorem>

    <p>
      Reducing a matrix to block triangular form is useful,
      because it simplifies computations such as determinants and eigenvalues
      (and determinants and eigenvalues are computationally expensive).
      In particular, if a matrix <m>A</m> has the block form
      <me>
        A = \bbm A_{11} \amp A_{12} \amp \cdots \amp A_{1n}\\
                 0\amp A_{22} \amp \cdots \amp A_{2n}\\
                 \vdots \amp \vdots \amp \ddots \amp \vdots\\
                 0 \amp 0 \amp \cdots \amp A_{nn}\ebm
      </me>,
      where the diagonal blocks are square matrices,
      then <m>\det(A) = \det(A_{11})\det(A_{22})\cdots \det(A_{nn})</m>
      and <m>c_A(x) = c_{A_{11}}(x)c_{A_{22}}(x)\cdots C_{A_{nn}}(x)</m>.
    </p>
  </subsection>

  <subsection xml:id="subsec-eigenspace">
    <title>Eigenspaces</title>
    <p>
      An important source of invariant subspaces is eigenspaces.
      Recall that for any real number <m>\lambda</m>,
      and any operator <m>T:V\to V</m>, we define
      <me>
        E_\lambda(T) = \ker(T-\lambda 1_V) = \{\vv\in V \,|\, T(\vv) = \lambda\vv\}
      </me>.
      For most values of <m>\lambda</m>, we'll have <m>E_\lambda(T)=\{\zer\}</m>.
      The values of <m>\lambda</m> for which <m>E_\lambda(T)</m> is non-trivial are precisely the eigenvalues of <m>T</m>.
      Note that since similar matrices have the same characteristic polynomial any matrix representation <m>M_B(T)</m>
      will have the same eigenvalues. They do <em>not</em> generally have the same eigenspaces,
      but we do have the following.
    </p>

    <theorem xml:id="thm-eigenspace-invariant">
      <statement>
        <p>
          Let <m>T:V\to V</m> be a linear operator. For any scalar <m>\lambda</m>,
          the eigenspace <m>E_\lambda(T)</m> is <m>T</m>-invariant.
          Moreover, for any ordered basis <m>B</m> of <m>V</m>,
          the coefficient isomorphism <m>C_B:V\to \R^n</m> induces an isomorphism
          <me>
            C_B|_{E_\lambda(T)}:E_\lambda(T)\to E_{\lambda}(M_B(T))
          </me>.
        </p>
      </statement>
    </theorem>
  </subsection>

  <subsection xml:id="subsec-direct-sum">
    <title>Direct Sums</title>
    <p>
      Recall that for any subspaces <m>U,W</m> of a vector space <m>V</m>,
      the sets
      <md>
        <mrow>U+W \amp =\{\uu+\ww \,|\, \uu\in U \text{ and } \ww\in W\}</mrow>
        <mrow>U\cap W \amp = \{\vv \in V \,|\, \vv\in U \text{ and } \vv\in W\}</mrow>
      </md>
      are subspaces of <m>V</m>. Saying that <m>\vv\in U+W</m> means that <m>\vv</m>
      can be written as a sum of a vector in <m>U</m> and a vector in <m>W</m>.
      However, this sum may not be unique. If <m>\vv\in U\cap W</m>, <m>\uu\in U</m> and <m>\ww\in W</m>,
      then we can write <m>(\uu+\vv)+\ww = \uu + (\vv+\ww)</m>,
      giving two different representations of a vector as an element of <m>U+W</m>.
    </p>

    <p>
      We proved in <xref ref="thm-unique-sum"/> in <xref ref="sec-subspace-combine"/>
      that for any <m>\vv\in U+W</m>, there exist <em>unique</em> vectors
      <m>\uu\in U</m> and <m>\ww\in W</m> such that <m>\vv=\uu+\ww</m>,
      if and only if <m>U\cap W=\{\zer\}</m>.
    </p>

    <p>
      In <xref ref="def-direct-sum"/>, we said that a sum <m>U+W</m> where <m>U\cap W=\{\zer\}</m>
      is called a <term>direct sum</term>, written as <m>U\oplus W</m>.
    </p>

    <p>
      Typically we are interested in the case that the two subspaces sum to <m>V</m>.
      Recall from <xref ref="def-subspace-complement"/> that if <m>V = U\oplus W</m>,
      we say that <m>W</m> is a <term>complement</term> of <m>U</m>.
      We also say that <m>U\oplus W</m> is a direct sum decomposition of <m>V</m>.
      Of course, the orthogonal complement <m>U^\bot</m> of a subspace <m>U</m>
      is a complement in this sense, if <m>V</m> is equipped with an inner product.
      (Without an inner product we have no concept of <q>orthogonal</q>.)
      But even if we don't have an inner product, finding a complement is not too difficult,
      as the next example shows.
    </p>

    <example xml:id="eg-direct-sum-basis">
      <title>Finding a complement by extending a basis</title>

      <statement>
        <p>
          The easiest way to determine a direct sum decomposition (or equivalently, a complement)
          is through the use of a basis. Suppose <m>U</m> is a subspace of <m>V</m> with basis
          <m>\basis{e}{k}</m>, and extend this to a basis
          <me>
            B = \{\mathbf{e}_1,\ldots, \mathbf{e}_k,\mathbf{e}_{k+1},\ldots, \mathbf{e}_n\}
          </me>
          of <m>V</m>. Let <m>W = \spn\{\mathbf{e}_{k+1},\ldots, \mathbf{e}_n\}</m>.
          Then clearly <m>U+W=V</m>, and <m>U\cap W=\{\zer\}</m>,
          since if <m>\vv\in U\cap W</m>, then <m>\vv\in U</m> and <m>\vv\in W</m>, so we have
          <me>
            \vv = a_1\mathbf{e}_1+\cdots + a_k\mathbf{e_k} = b_1\mathbf{e}_{k+1}+\cdots+b_{n-k}e_{n}
          </me>,
          which gives
          <me>
            a_1\mathbf{e}_1+\cdots + a_k\mathbf{e}_k-b_1\mathbf{e}_{k+1}-\cdots - b_{n-k}\mathbf{e}_n=\zer
          </me>,
          so <m>a_1=\cdots b_{n-k}=0</m> by the linear independence of <m>B</m>, showing that <m>\vv=\zer</m>.
        </p>

        <p>
          Conversely, if <m>V=U\oplus W</m>, and we have bases <m>\basis{u}{k}</m> of <m>U</m>
          and <m>\basis{v}{l}</m> of <m>W</m>, then
          <me>
            B = \{\uu_1,\ldots, \uu_k,\ww_1,\ldots, \ww_l\}
          </me>
          is a basis for <m>V</m>. Indeed, <m>B</m> spans <m>V</m>,
          since every element of <m>V</m> can be written as <m>\vv=\uu+\ww</m> with <m>\uu\in U,\ww\in W</m>.
          Independence follows by reversing the argument above: if
          <me>
            a_1\uu_1+\cdots + a_k\uu_k+b_1\ww_1+\cdots b_l\ww_l=\zer
          </me>
          then <m>a_1\uu_1+\cdots + a_k\uu_k = -b_1\ww_1-\cdots -b_l\ww_l</m>,
          and equality is only possible if both sides belong to <m>U\cap W = \{\zer\}</m>.
          Since <m>\basis{u}{k}</m> is independent, the <m>a_i</m> have to be zero,
          and since <m>\basis{w}{l}</m> is independent, the <m>b_j</m> have to be zero.
        </p>
      </statement>
    </example>

    <p>
      The argument given in the second part of <xref ref="eg-direct-sum-basis"/> has an immediate,
      but important consequence.
    </p>

    <theorem xml:id="thm-direct-sum-dimension">
      <statement>
        <p>
          Suppose <m>V=U\oplus W</m>, where <m>\dim U = m</m> and <m>\dim W = n</m>.
          Then <m>V</m> is finite-dimensional, and <m>\dim V = m+n</m>.
        </p>
      </statement>
    </theorem>

    <example xml:id="eg-invariant-block">
      <statement>
        <p>
          Suppose <m>V=U\oplus W</m>, where <m>U</m> and <m>W</m> are <m>T</m>-invariant subspaces
          for some operator <m>T:V\to V</m>. Let <m>B_U=\basis{u}{m}</m> and let <m>B_W = \basis{w}{n}</m>
          be bases for <m>U</m> and <m>W</m>, respectively.
          Determine the matrix of <m>T</m> with respect to the basis <m>B=B_U\cup B_W</m> of <m>V</m>.
        </p>
      </statement>
      <solution>
        <p>
          Since we don't know the map <m>T</m> or anything about the bases <m>B_U,B_W</m>,
          we're looking for a fairly general statement here.
          Since <m>U</m> is <m>T</m>-invariant, we must have <m>T(\uu_i)\in U</m> for each <m>i=1,\ldots, m</m>.
          Similarly, <m>T(\ww_j)\in W</m> for each <m>j=1,\ldots, n</m>. This means that we have
          <md>
            <mrow>T(\uu_1) \amp = a_{11}\uu_1 + \cdots + a_{m1}\uu_m + 0\ww_1+\cdots + 0\ww_n</mrow>
            <mrow>  \amp \vdots </mrow>
            <mrow>T(\uu_m) \amp = a_{1m}\uu_1 + \cdots + a_{mm}\uu_m+0\ww_1+\cdots + 0\ww_n</mrow>
            <mrow>T(\ww_1) \amp = 0\uu_1 + \cdots + 0\uu_m+b_{11}\ww_1 + \cdots + b_{n1}\ww_n </mrow>
            <mrow> \amp \vdots </mrow>
            <mrow>T(\ww_n) \amp = 0\uu_1 + \cdots + 0\uu_m+b_{1n}\ww_1 + \cdots + b_{nn}\ww_n</mrow>
          </md>
          for some scalars <m>a_{ij},b_{ij}</m>. If we set <m>A = [a_{ij}]_{m\times m}</m>
          and <m>B = [b_{ij}]_{n\times n}</m>, then we have
          <me>
            M_B(T) = \bbm A \amp 0\\0\amp B\ebm
          </me>.
          Moreover, we can also see that <m>A = M_{B_U}(T|_U)</m>, and <m>B = M_{B_W}(T|_W)</m>.
        </p>
      </solution>
    </example>

  </subsection>

</section>
