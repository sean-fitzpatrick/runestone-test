<?xml version="1.0" encoding="UTF-8" ?>

<worksheet xml:id="worksheet-svd">
  <title>Worksheet: Singular Value Decomposition</title>
  <page>
    <introduction>
      <p>
        For this worksheet, the reader is directed to Section 8.6 of
        <em>Linear Algebra with Applications</em>, by Keith Nicholson,
        and, of course, to <xref ref="section-matrix-factor"/>.
        (See also <url href="https://www.juanklopper.com/wp-content/uploads/2015/03/III_05_Singular_value_decomposition.html" visual="www.juanklopper.com/wp-content/uploads/2015/03/III_05_Singular_value_decomposition.html">notebook by Dr. Juan H Klopper</url>.)
      </p>

      <p>
        In <xref ref="section-matrix-factor"/> we saw that the <c>svd</c> algorithm in the mpmath library does things a little bit differently than Nicholson.
        If we start with a square matrix <m>A</m>, the results are the same,
        but if <m>A</m> is not square, the decomposition <m>A = P\Sigma_A Q^T</m> looks a little different.
        In particular, if <m>A</m> is <m>m\times n</m>,
        the matrix <m>\Sigma_A</m> defined in Nicholson will also be <m>m\times n</m>,
        but it will contain some rows or columns of zeros that are added to get the desired size.
        The matrix <m>Q</m> is an orthogonal <m>n\times n</m> matrix whose columns are an orthonormal basis of eigenvectors for <m>A^TA</m>.
        The matrix <m>P</m> is an orthogonal <m>m\times m</m> matrix whose columns are an orthonormal basis of <m>\R^m</m>.
        (The first <m>r</m> columns of <m>P</m> are given by <m>A\vecq_i</m>, where <m>\vecq_i</m> is the eigenvector of <m>A^TA</m> corresponding to the positive singular value <m>\sigma_i</m>.)
      </p>

      <p>
        The <c>svd</c> algorithm provided by mpmath replaces <m>\Sigma_A</m> by the <m>m\times m</m> diagonal matrix of singular values.
        The matrix <m>Q</m> is replaced by the <m>m\times n</m> matrix whose columns are the first <m>m</m> eigenvectors of <m>A^TA</m>.
        (Note that the rank of <m>A^TA</m> is equal to the rank of <m>A</m>,
        which is equal to the number of nonzero eigenvectors of <m>A^TA</m> (counted with multiplicity).)
        So we will have <m>m\geq r</m>, where <m>r</m> is the number of nonzero singular values.
      </p>

      <p>
        The product <m>\Sigma_A Q^T</m> will be the same in both cases, and the matrix <m>P</m> is the same as well.
      </p>
    </introduction>

    <p>
      This time, rather than using the mpmath algorithm,
      we will work through the process as outlined in Nicholson step-by-step.
      First, we will work through (again) Example 8.6.1 in Nicholson.
      Let <m>A = \bbm 1\amp 0\amp 1\\-1\amp 1\amp 0\ebm</m>.
      First, we get the singular values:
    </p>

    <sage>
      <input>
        from sympy import Matrix,init_printing
        init_printing()
        A = Matrix([[1,0,1],[-1,1,0]])
        L0=A.singular_values()
        L0
      </input>
    </sage>

    <p>
      Next, we get the eigenvalues and eigenvectors of <m>A^TA</m>:
    </p>

    <sage>
      <input>
        B = (A.T)*A
        L1=B.eigenvects()
        L1
      </input>
    </sage>

    <p>
      Now we need to normalize the eigenvectors, in the correct order.
      Note that the eigenvectors were listed in <em>increasing</em> order of eigenvalue,
      so we need to reverse the order. Note that <c>L1</c> is a list of lists.
      The eigenvector is the third entry (index 2) in the list (eigenvalue, multiplicity, eigenvector).
      We also need to turn list elements into matrices.
      So, for example the second eigenvector is <c>Matrix(L1[1][2])</c>.
    </p>

    <sage>
      <input>
        R1=Matrix(L1[2][2])
        R2=Matrix(L1[1][2])
        R3=Matrix(L1[0][2])
        Q1 = (1/R1.norm())*R1
        Q2 = (1/R2.norm())*R2
        Q3 = (1/R3.norm())*R3
        Q1,Q2,Q3
      </input>
    </sage>
  </page>

  <page>
    <p>
      Next, we can assemble these vectors into a matrix, and confirm that it's orthogonal.
    </p>

    <sage>
      <input>
        from sympy import BlockMatrix
        Q = Matrix(BlockMatrix([Q1,Q2,Q3]))
        Q,Q*Q.T
      </input>
    </sage>

    <p>
      We've made the matrix <m>Q</m>! Next, we construct <m>\Sigma_A</m>.
      This we will do by hand.
    </p>

    <sage>
      <input>
        SigA = Matrix([[L0[0],0,0],[0,L0[1],0]])
        SigA
      </input>
    </sage>

    <p>
      Alternatively, you could do <c>SigA = diag(L0[0],L0[1]).row_join(Matrix([0,0]))</c>.
      Finally, we need to make the matrix <m>P</m>.
      First, we find the vectors <m>A\vecq_1, A\vecq_2</m> and normalize.
      (Note that <m>A\vecq_3=\zer</m>, so this vector is unneeded, as expected.)
    </p>

    <sage>
      <input>
        S1 = A*Q1
        S2 = A*Q2
        P1 = (1/S1.norm())*S1
        P2 = (1/S2.norm())*S2
        P = Matrix(BlockMatrix([P1,P2]))
        P
      </input>
    </sage>

    <p>
      Note that the matrix <m>P</m> is already the correct size, because <m>\rank(A)=2\dim(\R^2)</m>.
      In general, for an <m>m\times n</m> matrix <m>A</m>, if <m>\rank(A)=r\lt m</m>,
      we would have to extend the set <m>\{\vecp_1,\ldots, \vecp_r\}</m> to a basis for <m>\R^m</m>.
      Finally, we check that our matrices work as advertised.
    </p>

    <sage>
      <input>
        P*SigA*(Q.T)
      </input>
    </sage>

    <p>
      For convenience, here is all of the above code, with all print commands (except the last one) removed.
      This can be run as a single code cell.
    </p>

    <listing xml:id="listing-svd">
      <caption>SymPy code for a singular value decomposition example</caption>
      <program>
        <input>
          from sympy import Matrix,BlockMatrix,init_printing
          init_printing()
          A = Matrix([[1,0,1],[-1,1,0]])
          B=(A.T)*A
          L0=A.singular_values()
          L1=B.eigenvects()
          R1=Matrix(L1[2][2])
          R2=Matrix(L1[1][2])
          R3=Matrix(L1[0][2])
          Q1 = (1/R1.norm())*R1
          Q2 = (1/R2.norm())*R2
          Q3 = (1/R3.norm())*R3
          Q = Matrix(BlockMatrix([Q1,Q2,Q3]))
          SigA = diag(L0[0],L0[1]).row_join(Matrix([0,0]))
          S1 = A*Q1
          S2 = A*Q2
          P1 = (1/S1.norm())*S1
          P2 = (1/S2.norm())*S2
          P = Matrix(BlockMatrix([P1,P2]))
          P,SigA,Q,P*SigA*Q.T
        </input>
      </program>
    </listing>
  </page>

  <page>
    <exercise workspace="7in">
      <statement>
        <p>
          Do Exercise 8.6.9 in Nicholson: compute the SVD for the matrices
          <me>
            \bbm 1\amp -1\\1\amp 0\\0\amp 1\ebm \quad \quad \bbm 1\amp 1\amp 1\\-1\amp 0\amp -2 \\1\amp 2\amp 0\ebm
          </me>.
           Note that for these matrices, you will need to do some additional work to extend the <m>\vecp_i</m>
           vectors to an orthonormal basis. You can adapt the code above,
           but you will have to think about how to implement additional code to construct any extra vectors you need.
        </p>
      </statement>
    </exercise>

    <sage>

    </sage>

  </page>

  <page>
    <exercise workspace="3.5in">
      <statement>
        <p>
          Either by reading Nicholson or by searching online (or both),
          come up with a couple of answers to the question:
          <q>Why are people interested in the singular value decomposition?</q>
        </p>
      </statement>
    </exercise>

    <sage language="html">

    </sage>

    <exercise workspace="3.5in">
      <statement>
        <p>
          (Optional) If you are interested, learn how to compute the <init>SVD</init>
          using tools from the NumPy and SciPy libraries instead.
        </p>
      </statement>
    </exercise>

    <sage language="html">

    </sage>
  </page>
</worksheet>
